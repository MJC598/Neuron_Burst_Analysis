epoch,training loss,validation loss
0,106030.8994140625,24492.825286865234
1,94183.8871459961,24476.846160888672
2,88564.97891235352,20122.294799804688
3,63180.67272949219,14055.282516479492
4,50378.046813964844,12258.535705566406
5,46722.867919921875,11777.653030395508
6,45109.269607543945,11635.925155639648
7,44185.430419921875,11375.767547607422
8,43513.90948486328,11185.730270385742
9,43037.50450134277,11001.238021850586
10,42710.11730957031,10949.719818115234
11,42372.254653930664,10849.745071411133
12,42139.78437805176,10821.776062011719
13,41804.722106933594,10740.924819946289
14,41640.926330566406,10682.278747558594
15,41413.682189941406,10730.920059204102
16,41211.18785095215,10629.68392944336
17,41086.61347961426,10538.383010864258
18,40808.76819610596,10553.389053344727
19,40729.839111328125,10490.96322631836
20,40553.0138092041,10448.08967590332
21,40430.075103759766,10461.82406616211
22,40257.74018859863,10409.091125488281
23,40122.2445602417,10443.195281982422
24,40017.745376586914,10343.858108520508
25,39931.73359680176,10324.221672058105
26,39805.89343261719,10349.654106140137
27,39760.41728210449,10296.133506774902
28,39629.336853027344,10351.406372070312
29,39560.24481964111,10266.853271484375
30,39471.6662979126,10229.312385559082
31,39432.30878448486,10229.893966674805
32,39364.35787200928,10224.93147277832
33,39301.12871551514,10196.1371383667
34,39257.73630523682,10192.172370910645
35,39198.24806213379,10200.712051391602
36,39178.15700531006,10279.809646606445
37,39146.507247924805,10167.144943237305
38,39093.771560668945,10185.967338562012
39,39054.45607757568,10148.928283691406
40,39030.92224884033,10137.415229797363
41,38982.82167053223,10138.470260620117
42,38972.66320800781,10121.363174438477
43,38932.77908325195,10131.642738342285
44,38923.61599731445,10117.129821777344
45,38887.182678222656,10136.516792297363
46,38893.55995941162,10115.069686889648
47,38864.52468109131,10117.690971374512
48,38857.70871734619,10101.61595916748
49,38823.63366699219,10101.637519836426
