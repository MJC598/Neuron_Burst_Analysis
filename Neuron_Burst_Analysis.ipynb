{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import scipy.io\n",
    "import random\n",
    "import pandas as pds\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Explanations\n",
    "\n",
    "These are 3 regression RNN-based models. In order to change it to a classifier the \n",
    "nn.Linear layers must have their second parameter changed to match the number of \n",
    "expected outputs.\n",
    "\n",
    "* Expected Input Shape: (batch_size, time_sequence, features)\n",
    "\n",
    "* Input_Size - number of features\n",
    "* Hidden_Size - number of connections between the hidden layers\n",
    "* Batch_Size - How many samples you want to push through the network before executing backprop\n",
    "    (this is a hyperparameter that can change how fast or slow a model converges)\n",
    "* Batch_First - Should always be set to True to keep input shape the same\n",
    "* Dropout - Only really does anything with more than 1 layer on the LSTM, RNN, GRU. Useful to help generalize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineRNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size,hidden_size,num_layers,batch_first,dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h_n  = self.rnn1(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size,hidden_size,num_layers,batch_first,dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "        self.c0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size,hidden_size,num_layers,batch_first,dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.h0.shape)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_mat(file_path, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    duration = []\n",
    "    amp = []\n",
    "    pre_pn = []\n",
    "    pre_itn = []\n",
    "    pre_aff = []\n",
    "    pre_point_exc = []\n",
    "    pre_point_inh = []\n",
    "\n",
    "\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        duration.append(data['info_collect'][i][0])\n",
    "        amp.append(data['info_collect'][i][1])\n",
    "        pre_pn.append(data['info_collect'][i][2])\n",
    "        pre_itn.append(data['info_collect'][i][3])\n",
    "        pre_aff.append(data['info_collect'][i][4])\n",
    "        pre_point_exc.append(data['info_collect'][i][5])\n",
    "        pre_point_inh.append(data['info_collect'][i][6])\n",
    "\n",
    "    full_data = np.concatenate((pre_pn, pre_itn, pre_aff, pre_point_exc, pre_point_inh), axis=2)\n",
    "    full_labels = np.concatenate((amp, duration), axis=2)\n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 5446\n",
    "    k = 4981\n",
    "    full = np.arange(data_samples)\n",
    "    training_indices = np.random.choice(full, size=k, replace=False)\n",
    "    validation_indices = np.delete(full,training_indices)\n",
    "    training_data = full_data[training_indices,:,:]\n",
    "    training_labels = full_labels[training_indices,:,:]\n",
    "    validation_data = full_data[validation_indices,:,:]\n",
    "    validation_labels = full_labels[validation_indices,:,:]\n",
    "    \n",
    "    print(training_data.shape)\n",
    "    print(training_labels.shape)\n",
    "    print(validation_data.shape)\n",
    "    print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method\n",
    "* Model - Model initialized based on classes above\n",
    "* Save_Filepath - Where you want to save the model to. Should end with a .pt or .pth extension. This is how you are able to load the model later for testing, etc.\n",
    "* training_loader - dataloader iterable with training dataset samples\n",
    "* validation_loader - dataloader iterable with validation dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader):\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "\n",
    "    \"\"\"\n",
    "    This is your optimizer. It can be changed but Adam is generally used. \n",
    "    Learning rate (alpha in gradient descent) is set to 0.001 but again \n",
    "    can easily be adjusted if you are getting issues\n",
    "\n",
    "    Loss function is set to Mean Squared Error. If you switch to a classifier \n",
    "    I'd recommend switching the loss function to nn.CrossEntropyLoss(), but this \n",
    "    is also something that can be changed if you feel a better loss function would work\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in range(20):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 10000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):  \n",
    "                # This permutation is done so it fits into the model. \n",
    "                # I reversed the features and sequence length with my EEG data\n",
    "                # So I had to fix it here, it will come back later with another permute\n",
    "#                 x = x.permute(0, 2, 1)\n",
    "                output = model(x)\n",
    "                #Computing loss  \n",
    "                print(output.shape)\n",
    "                print(y.shape)                            \n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y))  \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                                      \n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "        print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "    print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    loss_df.to_csv('losses/losses_lstm_csv.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scoring\n",
    "* Model - same model as sent to train_model\n",
    "* testing_dataloader - whichever dataloader you want to R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_eval(model, testing_dataloader):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    for i, (x, y) in enumerate(testing_dataloader):      \n",
    "        # Same permute issue we had in training. Basically switching from (batch_size, features, time) \n",
    "        # to (batch_size, time, features) \n",
    "#         x = x.permute(0, 2, 1)\n",
    "        output = model(x) \n",
    "        output_list.append(np.squeeze(np.transpose(output.detach().cpu().numpy())))\n",
    "        labels_list.append(y.detach().cpu().numpy())\n",
    "    output_list = np.hstack(output_list)\n",
    "    labels_list = np.hstack(labels_list)\n",
    "    print(output_list.shape)\n",
    "    print(np.squeeze(labels_list).shape)\n",
    "    print(r2_score(np.squeeze(labels_list), output_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4981, 50, 5)\n",
      "(4981, 1, 2)\n",
      "(465, 50, 5)\n",
      "(465, 1, 2)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 50, 25), got [1, 1, 25]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f5c0321aecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'baselineLSTM.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-77c9d049f126>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, save_filepath, training_loader, validation_loader)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# So I had to fix it here, it will come back later with another permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#                 x = x.permute(0, 2, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m#Computing loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-58ec41a884c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# take last cell output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0m\u001b[1;32m    534\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    535\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 50, 25), got [1, 1, 25]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = 'data/new_bursts.mat'\n",
    "    input_size = 5\n",
    "    hidden_size = 25\n",
    "    output_size = 2\n",
    "    batch_size = 1\n",
    "    num_layers = 1\n",
    "    batch_first = True\n",
    "    dropout = 0.0\n",
    "    model = baselineLSTM(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "    # model = baselineGRU(input_size,hidden_size,batch_size,batch_first,0)\n",
    "    # model = baselineRNN(input_size,hidden_size,batch_size,batch_first)\n",
    "    training_dataset, validation_dataset = get_data_from_mat(DATA_PATH) #retrieve data function\n",
    "    \n",
    "    # Turn datasets into iterable dataloaders\n",
    "    training_loader = DataLoader(dataset=training_dataset,batch_size=batch_size,shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    PATH = 'models/baselineLSTM.pth'\n",
    "    train_model(model,PATH,training_loader,validation_loader)\n",
    "    model = torch.load(PATH)\n",
    "    model.eval()\n",
    "    r2_score_eval(model, training_loader)\n",
    "    r2_score_eval(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
