epoch,training loss,validation loss
0,126053.84383392334,26838.061264038086
1,101381.55253601074,26276.47612762451
2,100162.86785888672,26086.296043395996
3,99540.67951965332,25933.760887145996
4,99085.60514068604,25795.190521240234
5,98747.6224822998,25727.942306518555
6,98526.30844116211,25823.124557495117
7,98338.25730895996,25767.38624572754
8,98174.71448516846,25714.82748413086
9,98039.39405059814,25714.186332702637
10,97948.98244476318,25719.81276702881
11,97838.14125061035,25710.21949005127
12,97742.81684112549,25695.212631225586
13,97660.77103424072,25653.032760620117
14,97594.1261062622,25612.43489074707
15,97541.08744049072,25559.641456604004
16,97486.10977172852,25515.845916748047
17,97421.34007263184,25487.6323928833
18,97372.50036621094,25471.08071899414
19,97328.51330566406,25449.495086669922
20,97286.51454162598,25436.497276306152
21,97249.15727996826,25423.093322753906
22,97215.04565429688,25413.891235351562
23,97184.37114715576,25406.815887451172
24,97158.89031982422,25401.955451965332
25,97135.50141906738,25398.096870422363
26,97115.70248413086,25396.42625427246
27,97097.0156173706,25395.696899414062
28,97077.92486572266,25395.747833251953
29,97061.92710113525,25395.68207550049
30,97046.62926483154,25393.697746276855
31,97032.50508117676,25390.983757019043
32,97020.20739746094,25389.71127319336
33,97009.49932098389,25387.84747314453
34,96999.60627746582,25386.20278930664
35,96990.6644744873,25385.356315612793
36,96982.9570388794,25384.48323059082
37,96975.84879302979,25383.763465881348
38,96969.14212799072,25383.281967163086
39,96963.27701568604,25383.0860748291
40,96957.88368988037,25383.179664611816
41,96952.9386062622,25382.92236328125
42,96948.46759033203,25382.849082946777
43,96944.30702972412,25382.82679748535
44,96940.50442504883,25382.880882263184
45,96937.06147003174,25382.886192321777
46,96933.92768859863,25383.00595855713
47,96931.17063140869,25383.11228942871
48,96928.63459777832,25383.23081970215
49,96926.4169845581,25383.24447631836
