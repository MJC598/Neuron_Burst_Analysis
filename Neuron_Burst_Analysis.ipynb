{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import scipy.io\n",
    "import random\n",
    "import pandas as pds\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FILE = 'losses/losses_lstm_csv.csv'\n",
    "PATH = 'models/baselineLSTM.pth'\n",
    "DATA_PATH = 'data/new_bursts.mat'\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    LOSS_FILE = COLAB_PRE + LOSS_FILE\n",
    "    PATH = COLAB_PRE + PATH\n",
    "    DATA_PATH = COLAB_PRE + DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Explanations\n",
    "\n",
    "These are 3 regression RNN-based models. In order to change it to a classifier the \n",
    "nn.Linear layers must have their second parameter changed to match the number of \n",
    "expected outputs.\n",
    "\n",
    "* Expected Input Shape: (batch_size, time_sequence, features)\n",
    "\n",
    "* Input_Size - number of features\n",
    "* Hidden_Size - number of connections between the hidden layers\n",
    "* Batch_Size - How many samples you want to push through the network before executing backprop\n",
    "    (this is a hyperparameter that can change how fast or slow a model converges)\n",
    "* Batch_First - Should always be set to True to keep input shape the same\n",
    "* Dropout - Only really does anything with more than 1 layer on the LSTM, RNN, GRU. Useful to help generalize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineRNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h_n  = self.rnn1(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "        self.c0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.h0.shape)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_mat(file_path, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    duration = []\n",
    "    amp = []\n",
    "    pre_pn = []\n",
    "    pre_itn = []\n",
    "    pre_aff = []\n",
    "    pre_point_exc = []\n",
    "    pre_point_inh = []\n",
    "\n",
    "\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        duration.append(data['info_collect'][i][0])\n",
    "        amp.append(data['info_collect'][i][1])\n",
    "        pre_pn.append(data['info_collect'][i][2])\n",
    "        pre_itn.append(data['info_collect'][i][3])\n",
    "        pre_aff.append(data['info_collect'][i][4])\n",
    "        pre_point_exc.append(data['info_collect'][i][5])\n",
    "        pre_point_inh.append(data['info_collect'][i][6])\n",
    "        \n",
    "    \n",
    "\n",
    "    full_data = np.concatenate((pre_pn, pre_itn, pre_aff, pre_point_exc, pre_point_inh), axis=2)\n",
    "    full_labels = np.concatenate((amp, duration), axis=2)\n",
    "    \n",
    "    x = full_labels[:,:,0]\n",
    "    normalized_amp = (x-min(x))/(max(x)-min(x))\n",
    "    full_labels[:,:,0] = normalized_amp\n",
    "    \n",
    "    x = full_labels[:,:,1]\n",
    "    normalized_duration = (x-min(x))/(max(x)-min(x))\n",
    "    full_labels[:,:,1] = normalized_duration\n",
    "    \n",
    "#     print(full_labels)\n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 5440 #5446\n",
    "    k = 4928\n",
    "    full = np.arange(data_samples)\n",
    "    training_indices = np.random.choice(full, size=k, replace=False)\n",
    "    validation_indices = np.delete(full,training_indices)\n",
    "    training_data = full_data[training_indices,:,:]\n",
    "    training_labels = full_labels[training_indices,:,:]\n",
    "    validation_data = full_data[validation_indices,:,:]\n",
    "    validation_labels = full_labels[validation_indices,:,:]\n",
    "    \n",
    "#     print(training_data.shape)\n",
    "#     print(training_labels.shape)\n",
    "#     print(validation_data.shape)\n",
    "#     print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method\n",
    "* Model - Model initialized based on classes above\n",
    "* Save_Filepath - Where you want to save the model to. Should end with a .pt or .pth extension. This is how you are able to load the model later for testing, etc.\n",
    "* training_loader - dataloader iterable with training dataset samples\n",
    "* validation_loader - dataloader iterable with validation dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs):\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "\n",
    "    \"\"\"\n",
    "    This is your optimizer. It can be changed but Adam is generally used. \n",
    "    Learning rate (alpha in gradient descent) is set to 0.001 but again \n",
    "    can easily be adjusted if you are getting issues\n",
    "\n",
    "    Loss function is set to Mean Squared Error. If you switch to a classifier \n",
    "    I'd recommend switching the loss function to nn.CrossEntropyLoss(), but this \n",
    "    is also something that can be changed if you feel a better loss function would work\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss()\n",
    "    decay_rate = 0.90 #decay the lr each step to 90% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):  \n",
    "                output = model(x)                       \n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y))  \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                                      \n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%10 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    loss_df.to_csv(LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scoring\n",
    "* Model - same model as sent to train_model\n",
    "* testing_dataloader - whichever dataloader you want to R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_eval(model, testing_dataloader):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    for i, (x, y) in enumerate(testing_dataloader):      \n",
    "        # Same permute issue we had in training. Basically switching from (batch_size, features, time) \n",
    "        # to (batch_size, time, features) \n",
    "#         x = x.permute(0, 2, 1)\n",
    "        output = model(x) \n",
    "        output_list.append(np.transpose(output.detach().cpu().numpy()))\n",
    "        labels_list.append(np.transpose(y.detach().cpu().numpy()))\n",
    "    output_list = np.transpose(np.hstack(output_list))\n",
    "    labels_list = np.transpose(np.hstack(labels_list)).reshape((-1,2))\n",
    "#     print(output_list.shape)\n",
    "#     print(np.squeeze(labels_list).shape)\n",
    "    print(r2_score(np.squeeze(labels_list), output_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df02dc9c3cb14f0abbebd6eb07349b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    16] train loss: 3.235299 val loss: 0.302831\n",
      "[11,    16] train loss: 2.734354 val loss: 0.286450\n",
      "[21,    16] train loss: 2.714505 val loss: 0.286276\n",
      "[31,    16] train loss: 2.698217 val loss: 0.286173\n",
      "[41,    16] train loss: 2.687283 val loss: 0.286669\n",
      "[51,    16] train loss: 2.682069 val loss: 0.286974\n",
      "[61,    16] train loss: 2.679996 val loss: 0.287127\n",
      "[71,    16] train loss: 2.679252 val loss: 0.287199\n",
      "[81,    16] train loss: 2.679031 val loss: 0.287204\n",
      "[91,    16] train loss: 2.678935 val loss: 0.287213\n",
      "-0.01616581858679933\n",
      "-0.015260584370253127\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 50\n",
    "output_size = 2\n",
    "batch_size = 32\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "dropout = 0.0\n",
    "epochs = 100\n",
    "model = baselineLSTM(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "# model = baselineGRU(input_size,hidden_size,batch_size,batch_first,0)\n",
    "# model = baselineRNN(input_size,hidden_size,batch_size,batch_first)\n",
    "training_dataset, validation_dataset = get_data_from_mat(DATA_PATH) #retrieve data function\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "training_loader = DataLoader(dataset=training_dataset,batch_size=batch_size,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size)\n",
    "\n",
    "training_loss, validation_loss = train_model(model,PATH,training_loader,validation_loader,epochs)\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "r2_score_eval(model, training_loader)\n",
    "r2_score_eval(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), training_loss)\n",
    "# plt.plot(range(epochs), validation_loss)\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
