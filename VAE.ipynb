{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def generate(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    \"\"\"\n",
    "    KL Divergence and Reconstruction derived from:\n",
    "    https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "    \"\"\"\n",
    "    def kl_div(self, sample, mean, std):\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mean), torch.ones_like(std)).to(self.device)\n",
    "        q = torch.distributions.Normal(mean, std).to(self.device)\n",
    "\n",
    "        log_qzx = q.log_prob(sample)\n",
    "        log_pz = p.log_prob(sample)\n",
    "\n",
    "        kl = (log_qzx - log_pz)\n",
    "        return kl.sum(-1)\n",
    "    \n",
    "    def reconstruction(self, x_hat, x, dims=(1,2,3)):\n",
    "        scale = torch.exp(nn.Parameter(torch.Tensor([0.0])).to(self.device)).to(self.device)\n",
    "        dist = torch.distributions.Normal(x_hat, scale).to(self.device)\n",
    "        return dist.log_prob(x).sum(dim=dims)\n",
    "        \n",
    "    def ELBOLoss(self, x, sample, mean, std, x_hat):\n",
    "        return (self.kl_div(sample, mean, std) - self.reconstruction(x_hat, x)).mean()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_VAE(VAE):\n",
    "    def __init__(self):\n",
    "        super(LSTM_VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "                        nn.LSTM(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout),\n",
    "                        nn.Linear(in_features=in_features, out_features=out_features), #needs to encode from LSTM Dims to Latent Dims\n",
    "                        nn.ReLU()\n",
    "                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_VAE(VAE):\n",
    "    def __init__(self):\n",
    "        super(GRU_VAE, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
