{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import scipy.io\n",
    "import random\n",
    "import pandas as pds\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FILE = 'losses/newdata_losses_lstm_csv.csv'\n",
    "PATH = 'models/newdata_baselineLSTM.pth'\n",
    "DATA_PATH = 'data/info_collect_for_NN_network_cycle.mat'\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    LOSS_FILE = COLAB_PRE + LOSS_FILE\n",
    "    PATH = COLAB_PRE + PATH\n",
    "    DATA_PATH = COLAB_PRE + DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Explanations\n",
    "\n",
    "These are 3 regression RNN-based models. In order to change it to a classifier the \n",
    "nn.Linear layers must have their second parameter changed to match the number of \n",
    "expected outputs.\n",
    "\n",
    "* Expected Input Shape: (batch_size, time_sequence, features)\n",
    "\n",
    "* Input_Size - number of features\n",
    "* Hidden_Size - number of connections between the hidden layers\n",
    "* Batch_Size - How many samples you want to push through the network before executing backprop\n",
    "    (this is a hyperparameter that can change how fast or slow a model converges)\n",
    "* Batch_First - Should always be set to True to keep input shape the same\n",
    "* Dropout - Only really does anything with more than 1 layer on the LSTM, RNN, GRU. Useful to help generalize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineRNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h_n  = self.rnn1(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "        self.c0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.h0.shape)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_mat(file_path, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    duration = []\n",
    "    amp = []\n",
    "    pre_pn = []\n",
    "    pre_itn = []\n",
    "    pre_aff = []\n",
    "    pre_point_exc = []\n",
    "    pre_point_inh = []\n",
    "\n",
    "\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        duration.append(data['info_collect'][i][0])\n",
    "        amp.append(data['info_collect'][i][1])\n",
    "        pre_pn.append(data['info_collect'][i][2])\n",
    "        pre_itn.append(data['info_collect'][i][3])\n",
    "        pre_aff.append(data['info_collect'][i][4])\n",
    "        pre_point_exc.append(data['info_collect'][i][5])\n",
    "        pre_point_inh.append(data['info_collect'][i][6])\n",
    "        \n",
    "    \n",
    "\n",
    "    full_data = np.concatenate((pre_pn, pre_itn, pre_aff, pre_point_exc, pre_point_inh), axis=2)\n",
    "    full_labels = np.concatenate((amp, duration), axis=2)\n",
    "    \n",
    "    x = full_labels[:,:,0]\n",
    "    normalized_amp = (x-min(x))/(max(x)-min(x))\n",
    "    full_labels[:,:,0] = normalized_amp\n",
    "    \n",
    "    x = full_labels[:,:,1]\n",
    "    normalized_duration = (x-min(x))/(max(x)-min(x))\n",
    "    full_labels[:,:,1] = normalized_duration\n",
    "    \n",
    "    for i in range(5):\n",
    "        x = full_data[:,:,i]\n",
    "        full_data[:,:,i] = (x - np.min(x))/(np.max(x)-np.min(x))\n",
    "#     full_data[:,:,1] = (full_data[:,:,1] - np.min(full_data[:,:,1]))/(np.max(full_data[:,:,1])-np.min(full_data[:,:,1]))\n",
    "#     full_data[:,:,2] = (full_data[:,:,2] - np.min(full_data[:,:,2]))/(np.max(full_data[:,:,2])-np.min(full_data[:,:,2]))\n",
    "#     full_data[:,:,3] = (full_data[:,:,3] - np.min(full_data[:,:,3]))/(np.max(full_data[:,:,3])-np.min(full_data[:,:,3]))\n",
    "#     full_data[:,:,4] = (full_data[:,:,4] - np.min(full_data[:,:,4]))/(np.max(full_data[:,:,4])-np.min(full_data[:,:,4]))\n",
    "#     print(full_labels)\n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 5440 #5446\n",
    "    k = 4928\n",
    "    full = np.arange(data_samples)\n",
    "    training_indices = np.random.choice(full, size=k, replace=False)\n",
    "    validation_indices = np.delete(full,training_indices)\n",
    "    training_data = full_data[training_indices,:,:]\n",
    "    training_labels = full_labels[training_indices,:,:]\n",
    "    validation_data = full_data[validation_indices,:,:]\n",
    "    validation_labels = full_labels[validation_indices,:,:]\n",
    "    \n",
    "#     print(training_data.shape)\n",
    "#     print(training_labels.shape)\n",
    "#     print(validation_data.shape)\n",
    "#     print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method\n",
    "* Model - Model initialized based on classes above\n",
    "* Save_Filepath - Where you want to save the model to. Should end with a .pt or .pth extension. This is how you are able to load the model later for testing, etc.\n",
    "* training_loader - dataloader iterable with training dataset samples\n",
    "* validation_loader - dataloader iterable with validation dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs):\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "\n",
    "    \"\"\"\n",
    "    This is your optimizer. It can be changed but Adam is generally used. \n",
    "    Learning rate (alpha in gradient descent) is set to 0.001 but again \n",
    "    can easily be adjusted if you are getting issues\n",
    "\n",
    "    Loss function is set to Mean Squared Error. If you switch to a classifier \n",
    "    I'd recommend switching the loss function to nn.CrossEntropyLoss(), but this \n",
    "    is also something that can be changed if you feel a better loss function would work\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss()\n",
    "    decay_rate = 0.90 #decay the lr each step to 90% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):  \n",
    "                output = model(x)                       \n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y))  \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                                      \n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%10 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    loss_df.to_csv(LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scoring\n",
    "* Model - same model as sent to train_model\n",
    "* testing_dataloader - whichever dataloader you want to R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_eval(model, testing_dataloader):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    for i, (x, y) in enumerate(testing_dataloader):      \n",
    "        # Same permute issue we had in training. Basically switching from (batch_size, features, time) \n",
    "        # to (batch_size, time, features) \n",
    "#         x = x.permute(0, 2, 1)\n",
    "        output = model(x) \n",
    "        output_list.append(np.transpose(output.detach().cpu().numpy()))\n",
    "        labels_list.append(np.transpose(y.detach().cpu().numpy()))\n",
    "    output_list = np.transpose(np.hstack(output_list))\n",
    "    labels_list = np.transpose(np.hstack(labels_list)).reshape((-1,2))\n",
    "#     print(output_list.shape)\n",
    "#     print(np.squeeze(labels_list).shape)\n",
    "    print(r2_score(np.squeeze(labels_list), output_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccafb2afe25d46b08c53af8c272ee8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    16] train loss: 2.957794 val loss: 0.294244\n",
      "[11,    16] train loss: 1.507064 val loss: 0.162167\n",
      "[21,    16] train loss: 1.462484 val loss: 0.158995\n",
      "[31,    16] train loss: 1.425259 val loss: 0.157319\n",
      "[41,    16] train loss: 1.411102 val loss: 0.156487\n",
      "[51,    16] train loss: 1.404313 val loss: 0.155981\n",
      "[61,    16] train loss: 1.398676 val loss: 0.156098\n",
      "[71,    16] train loss: 1.399172 val loss: 0.156037\n",
      "[81,    16] train loss: 1.399096 val loss: 0.156049\n",
      "[91,    16] train loss: 1.399049 val loss: 0.156046\n",
      "-0.4306046070491386\n",
      "-0.4343379076436863\n"
     ]
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 50\n",
    "output_size = 2\n",
    "batch_size = 32\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "dropout = 0.0\n",
    "epochs = 100\n",
    "model = baselineLSTM(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "# model = baselineGRU(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "# model = baselineRNN(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "training_dataset, validation_dataset = get_data_from_mat(DATA_PATH) #retrieve data function\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "training_loader = DataLoader(dataset=training_dataset,batch_size=batch_size,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size)\n",
    "\n",
    "training_loss, validation_loss = train_model(model,PATH,training_loader,validation_loader,epochs)\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "r2_score_eval(model, training_loader)\n",
    "r2_score_eval(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlJElEQVR4nO3deZxddX3/8df7rjOTmUlCMmQPQQggoIhGcRfFKm6416pV6w9/lD78VbA8+nNpa2vbh12sa3EpFYtaxFZFpdQFhCDlp4IJRkISliCQhAQyIQmTSTLLnfv5/XHOJDeTmckk5OQmc97Px+M+uPfc7z33c+aE877f8z2LIgIzM8uvQrMLMDOz5nIQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzCYpSX8g6bZm12FHPweBHTMkPSTp5c2u41BIOldSXVLviMfzml2bWanZBZjlyMaImN/sIsxGco/AjnmSqpI+K2lj+vispGr63kxJ10vaLmmrpP+RVEjf+5CkRyTtkHSvpPNGmfc5kh6VVGyY9kZJd6XPnyNpmaQeSY9J+vQhLsMtkv5O0h3pvH4g6biG9y+QtCpdjlskPbXhvQWSrpXULelxSZePmPc/Sdom6UFJrzqU+mxycxDYZPBnwHOBZwBnAc8B/jx97zJgA9AFzAI+CoSkU4H/Azw7IjqAVwIPjZxxRNwO7ARe1jD5HcA30+efAz4XEZ3AScB/PonleDfwv4A5QA34PICkU4BrgEvT5fgh8F+SKmlAXQ88DCwC5gHfapjnOcC9wEzgH4ErJelJ1GiTkIPAJoN3An8dEZsjohv4OPCu9L1Bkg3rCRExGBH/E8kFtoaAKnC6pHJEPBQRD4wx/2uAtwNI6gBenU4bnv/JkmZGRG9E/HKcOuemv+gbH1Ma3v9GRNwdETuBvwB+N93Qvw3474i4MSIGgX8CWoHnk4TeXOBPI2JnRPRFROMA8cMR8a8RMQR8Lf1bzBr3r2m54yCwyWAuyS/iYQ+n0wA+CawFbpD0W0kfBoiItSS/sP8K2CzpW5LmMrpvAm9Kdze9CbgzIoa/70LgFOAeSb+S9Npx6twYEdNGPHY2vL9+xDKUSX7J77N8EVFP284DFpBs7GtjfOejDZ/blT5tH6dGyyEHgU0GG4ETGl4vTKcRETsi4rKIeApwAfAnw2MBEfHNiHhh+tkA/mG0mUfEapIN8avYd7cQEXF/RLwdOD79/HdG/Mo/GAtGLMMgsGXk8qW7dhYAj5AEwkJJPvDDDpmDwI41ZUktDY8SyW6aP5fUJWkm8DHg3wEkvVbSyenG8wmSXUJ1SadKeln6K78P2A3Ux/nebwKXAC8Gvj08UdLvS+pKf6VvTyePN5/x/L6k0yW1AX8NfCfdpfOfwGsknSepTDLu0Q/8HLgD2AT8vaQp6d/kBYf4/ZZTDgI71vyQZKM9/Pgr4G+BZcBdwErgznQawGLgp0Av8AvgixGxlGR84O9JfnE/SvKL/iPjfO81wEuAmyNiS8P084FVknpJBo5/LyJ2jzGPuaOcR/Dmhve/AVyV1tMCfAAgIu4Ffh/457Te1wGvi4iBNCheB5wMrCMZGH/bOMthth/5xjRmzSfpFuDfI+Irza7F8sc9AjOznHMQmJnlnHcNmZnlnHsEZmY5d8wdezxz5sxYtGhRs8swMzumLF++fEtEdI323jEXBIsWLWLZsmXNLsPM7Jgi6eGx3sts11B6Yssdkn6TXjXx46O0qUr6D0lrJd0uaVFW9ZiZ2eiyHCPoB14WEWeRXBXyfEnPHdHmQmBbRJwMfIYxTvE3M7PsZBYEkehNX5bTx8hDlF5PckVEgO8A5/kSuWZmR1amRw1JKkpaAWwGbkyv7d5oHukVF9OrJz4BzBhlPhelN/9Y1t3dnWXJZma5k2kQRMRQRDwDmA88R9KZhzifKyJiSUQs6eoaddDbzMwO0RE5jyAitgNLSS7Q1egR0kvvpleRnAo8fiRqMjOzRJZHDXVJmpY+bwV+B7hnRLPrgPekz99CcmVHn+psZnYEZdkjmAMsTW/y/SuSMYLrJf21pAvSNlcCMyStBf4E+HBWxdz76A4+dcO9PN7bn9VXmJkdkzI7oSwi7gLOHmX6xxqe9wFvzaqGRg909/LPN6/lNU+fw4z26pH4SjOzY0JurjVUKSaL2j94qDePMjObnHITBNVysqgDQw4CM7NGuQmC4R7BQM1BYGbWKD9BUHIQmJmNJndB0O8gMDPbR26CoLonCIaaXImZ2dElR0FQBLxryMxspNwEwZ4xAh81ZGa2j/wEgY8aMjMbVX6CwEcNmZmNKndB4KOGzMz2lZsgKBVEQe4RmJmNlJsgkESlVPBgsZnZCLkJAkgGjN0jMDPbV76CoFT0GIGZ2Qi5CoJqqeAzi83MRsjyVpULJC2VtFrSKkmXjNJmqqT/kvSbtM17s6oHkiDwriEzs31ldocyoAZcFhF3SuoAlku6MSJWN7R5P7A6Il4nqQu4V9LVETGQRUEVB4GZ2X4y6xFExKaIuDN9vgNYA8wb2QzokCSgHdhKEiCZ8FFDZmb7OyJjBJIWkdy/+PYRb10OPBXYCKwELomI/bbUki6StEzSsu7u7kOuw0cNmZntL/MgkNQOfBe4NCJ6Rrz9SmAFMBd4BnC5pM6R84iIKyJiSUQs6erqOuRaquWCjxoyMxsh0yCQVCYJgasj4tpRmrwXuDYSa4EHgdOyqsc9AjOz/WV51JCAK4E1EfHpMZqtA85L288CTgV+m1VNHiw2M9tflkcNvQB4F7BS0op02keBhQAR8WXgb4CrJK0EBHwoIrZkVVClVPRgsZnZCJkFQUTcRrJxH6/NRuAVWdUwUqVYoH/QJ5SZmTXK15nFZR8+amY2Uq6CoFL0UUNmZiPlKgh8iQkzs/3lKgiGzyyOiGaXYmZ21MhXEBQLRMDgkIPAzGxYroKgWk5vYO8BYzOzPXIVBJViGgQeJzAz2yNfQVAqAg4CM7NGOQsC9wjMzEbKZRD4dpVmZnvlKgiqe4LAPQIzs2G5CoI9u4Z81JCZ2R65CoKqjxoyM9tProLAg8VmZvvLZRB4jMDMbK9cBUHV5xGYme0ny1tVLpC0VNJqSaskXTJGu3MlrUjb/CyreqBxsNiHj5qZDcvyVpU14LKIuFNSB7Bc0o0RsXq4gaRpwBeB8yNinaTjM6zHYwRmZqPIrEcQEZsi4s70+Q5gDTBvRLN3ANdGxLq03eas6gFfa8jMbDRHZIxA0iLgbOD2EW+dAkyXdIuk5ZLenWUdHiw2M9tflruGAJDUDnwXuDQiekb5/mcB5wGtwC8k/TIi7hsxj4uAiwAWLlx4yLX4zGIzs/1l2iOQVCYJgasj4tpRmmwAfhIROyNiC3ArcNbIRhFxRUQsiYglXV1dh1yPdw2Zme0vy6OGBFwJrImIT4/R7AfACyWVJLUB55CMJWSiUBDlonyJCTOzBlnuGnoB8C5gpaQV6bSPAgsBIuLLEbFG0o+Bu4A68JWIuDvDmqgUC/QPOgjMzIZlFgQRcRugCbT7JPDJrOoYKbmBvc8jMDMblqsziyE5u9hjBGZme+UuCCqlgoPAzKxBPoPAg8VmZnvkLwg8WGxmto/cBUG17B6BmVmj3AVBpVjwmcVmZg3yFwQeLDYz20fugqDqIDAz20fugqBSKtBf8wllZmbDchcE1VLRg8VmZg1yFwSVoncNmZk1yl8QeIzAzGwfDgIzs5zLZRD4PAIzs71yFwTVUoFaPajXo9mlmJkdFXIXBMM3sPeRQ2ZmifwFQdE3sDcza5TlPYsXSFoqabWkVZIuGaftsyXVJL0lq3qGVUu+gb2ZWaMs71lcAy6LiDsldQDLJd0YEasbG0kqAv8A3JBhLXsM7xry2cVmZonMegQRsSki7kyf7wDWAPNGafrHwHeBzVnV0qhaKgLuEZiZDTsiYwSSFgFnA7ePmD4PeCPwpQN8/iJJyyQt6+7uflK1eLDYzGxfmQeBpHaSX/yXRkTPiLc/C3woIsbdKkfEFRGxJCKWdHV1Pal6hgeL3SMwM0tkOUaApDJJCFwdEdeO0mQJ8C1JADOBV0uqRcT3s6pp7xiBg8DMDDIMAiVb9yuBNRHx6dHaRMSJDe2vAq7PMgSgYdeQg8DMDMi2R/AC4F3ASkkr0mkfBRYCRMSXM/zuMfnwUTOzfWUWBBFxG6CDaP8HWdXSyLuGzMz2lbszi6s+asjMbB+5C4JKMTmPoH/QJ5SZmUEeg8A9AjOzfeQuCDxYbGa2r9wFgQ8fNTPbl4PAzCznchcEpYKQfPiomdmw3AWBJCrFggeLzcxSuQsCSAaMvWvIzCyRyyColIreNWRmlsplELhHYGa2Vy6DoFIq+FaVZmapfAZB0T0CM7NhuQyCatlHDZmZDZtQEEiaIqmQPj9F0gXp3ceOSe4RmJntNdEewa1AS3qz+RtIbjhzVVZFZS0ZI3AQmJnBxINAEbELeBPwxYh4K3DGuB+QFkhaKmm1pFWSLhmlzTsl3SVppaSfSzrr4Bfh4FV81JCZ2R4TvUOZJD0PeCdwYTqteIDP1IDLIuJOSR3Ackk3RsTqhjYPAi+JiG2SXgVcAZxzEPUfEh8+ama210SD4FLgI8D3ImKVpKcAS8f7QERsAjalz3dIWgPMA1Y3tPl5w0d+CcyfeOmHrlIqerDYzCw1oSCIiJ8BPwNIB423RMQHJvolkhYBZwO3j9PsQuBHY3z+IuAigIULF070a8fkwWIzs70metTQNyV1SpoC3A2slvSnE/xsO/Bd4NKI6BmjzUtJguBDo70fEVdExJKIWNLV1TWRrx2XTygzM9trooPFp6cb8TeQ/Go/keTIoXGlh5h+F7g6Iq4do83Tga8Ar4+IxydYz5NS9VFDZmZ7TDQIyulG/Q3AdRExCMR4H5Ak4EpgTUR8eow2C4FrgXdFxH0TrvpJ8mCxmdleEx0s/hfgIeA3wK2STgBG3c3T4AUkvYaVklak0z4KLASIiC8DHwNmAF9McoNaRCw5iPoPSaWUnFkcEaTfa2aWWxMdLP488PmGSQ+n+/XH+8xtwLhb2Yh4H/C+idRwOFWKBSKgVg/KRQeBmeXbRAeLp0r6tKRl6eNTwJSMa8vM8H2LPU5gZjbxMYKvAjuA300fPcC/ZVVU1nwDezOzvSY6RnBSRLy54fXHG/b7H3OqpeSkaAeBmdnEewS7Jb1w+IWkFwC7sykpe+4RmJntNdEewcXA1yVNTV9vA96TTUnZ2xMEQz6pzMxsokcN/QY4S1Jn+rpH0qXAXRnWlplKMQmCvkH3CMzMDuoOZRHR03CZiD/JoJ4jorqnR+AgMDN7MreqPGYPwG8pJ4PFuwe8a8jM7MkEwbiXmDiadXVUANjS29/kSszMmm/cMQJJOxh9gy+gNZOKjoCu9hYAunc4CMzMxg2CiOg4UoUcSZ2tJSqlgoPAzIwnt2vomCWJrvaqg8DMjJwGAUBXR5VujxGYmeU8CNwjMDNzEJiZ5V1+g6C9ytZdAwz6pDIzy7nMgkDSAklLJa2WtErSJaO0kaTPS1or6S5Jz8yqnpG6OqpEwNadA0fqK83MjkpZ9ghqwGURcTrwXOD9kk4f0eZVwOL0cRHwpQzr2UdXRxXwuQRmZpkFQURsiog70+c7gDXAvBHNXg98PRK/BKZJmpNVTY0cBGZmiSMyRiBpEXA2cPuIt+YB6xteb2D/sMhEV7uDwMwMjkAQSGoHvgtc2nDl0oOdx0XD90vu7u4+LHXt6RH4XAIzy7lMg0BSmSQEro6Ia0dp8giwoOH1/HTaPiLiiohYEhFLurq6DkttLeUinS0lNvf0HZb5mZkdq7I8akjAlcCaiPj0GM2uA96dHj30XOCJiNiUVU0j+exiM7OJ36ryULwAeBewsuFG9x8FFgJExJeBHwKvBtYCu4D3ZljPfnxSmZlZhkEQEbdxgJvXREQA78+qhgPp6mhh5Ybtzfp6M7OjQm7PLAZ8BVIzM/IeBB1Vdg4MsbO/1uxSzMyaJvdBAL5lpZnlm4MAn1RmZvmW7yDw2cVmZvkOguM7fXaxmVmug2B6W4ViQe4RmFmu5ToIigUxY0rFQWBmuZbrIACfXWxm5iDoqLLZQWBmOeYg8NnFZpZzDoKOKlt6+6nXo9mlmJk1hYOgo0qtHmzfPdjsUszMmsJB4LOLzSznHATp2cWbd/hOZWaWT7kPgpOOb0eC5Q9va3YpZmZNkfsgmNleZckJ0/nx3Y82uxQzs6bI8p7FX5W0WdLdY7w/VdJ/SfqNpFWSjuhtKhudf+Yc7nl0Bw9t2dmsEszMmibLHsFVwPnjvP9+YHVEnAWcC3xKUiXDesb0yjNmAfCTVe4VmFn+ZBYEEXErsHW8JkCHJAHtadum3Cps/vQ2njZvKj/y7iEzy6FmjhFcDjwV2AisBC6JiPpoDSVdJGmZpGXd3d2ZFHP+mbNZsX47m57Yncn8zcyOVs0MglcCK4C5wDOAyyV1jtYwIq6IiCURsaSrqyubYs6YDcANqx7LZP5mZkerZgbBe4FrI7EWeBA4rVnFnHx8Oycf3+6jh8wsd5oZBOuA8wAkzQJOBX7bxHo4/4zZ3P7g42zdOdDMMszMjqgsDx+9BvgFcKqkDZIulHSxpIvTJn8DPF/SSuAm4EMRsSWreibi/DNnUw/40d2bmlmGmdkRVcpqxhHx9gO8vxF4RVbffyjOmNvJqbM6+NYd63nnOSc0uxwzsyMi92cWN5LEO85ZyMpHnuCuDdubXY6Z2RHhIBjhDWfPo6Vc4Ju3r2t2KWZmR4SDYISprWUuOGsu1/1mIzv6fI8CM5v8HASjeMc5J7BrYIjvr9jY7FLMzDLnIBjFWfOncvqcTr55+zoifAtLM5vcHASjGB40XrOph1+v397scszMMuUgGMMbzp5HZ0uJf/zxPe4VmNmk5iAYQ3u1xEde/VR++dutfHv5hmaXY2aWGQfBON62ZAFLTpjOJ364hsd7fXN7M5ucHATjKBTE373paezsr/G3/72m2eWYmWXCQXAAi2d1cPFLTuJ7v36E2+5v6qWQzMwy4SCYgPe/9GQWHtfG3/1oDfW6B47NbHJxEExAS7nIJectZtXGHt/X2MwmHQfBBL3h7Hmc1DWFz/z0PobcKzCzScRBMEHFgrj05adw32O9XH+XLz1hZpOHg+AgvOZpczhtdgef++n91IbqzS7HzOywyPIOZV+VtFnS3eO0OVfSCkmrJP0sq1oOl0JBfPB3TuG3W3byvV8/0uxyzMwOiyx7BFcB54/1pqRpwBeBCyLiDOCtGdZy2Lzi9FmcOa+TL97ygMcKzGxSyCwIIuJWYOs4Td4BXBsR69L2m7Oq5XCSxPvPPZkHt+zkhyt9b2MzO/Y1c4zgFGC6pFskLZf07rEaSrpI0jJJy7q7u49giaN75RmzOalrCl9YutYXpDOzY14zg6AEPAt4DfBK4C8knTJaw4i4IiKWRMSSrq6uI1njqAoF8Ufnnsw9j+5g6b3HREfGzGxMzQyCDcBPImJnRGwBbgXOamI9B+X1z5jLvGmtXH5z0iu4/7Ed/OE3lvGJH/qaRGZ2bCk18bt/AFwuqQRUgHOAzzSxnoNSLhb4w5c8hY/9YBV/+I3l3HTPZobqgQRve/YCTupqb3aJZmYTkuXho9cAvwBOlbRB0oWSLpZ0MUBErAF+DNwF3AF8JSLGPNT0aPS7SxYws73KTfds5p3nLOSGD76YSrHAv/zsgWaXZmY2YZn1CCLi7RNo80ngk1nVkLWWcpHvXPw8JDhhxhQg6Q1cc8c6Pvg7pzBnamuTKzQzOzCfWfwkLZo5ZU8IAPzvFz2FesC/3vpgE6syM5s4B8FhtuC4Nl5/1lyuuWMdW3cONLscM7MDchBk4OJzT2L34BBX/T/3Cszs6OcgyMApszp41Zmz+cItD/CtO9Y1uxwzs3E5CDLyybeexQtPnsmHr13JJ39yj+9sZmZHrWaeRzCptVdLXPmeJfzFD1bxhaUP8KsHt3HanA662qvMndbKqbM7WDyrnWqp2OxSzSznHAQZKhULfOKNZ3JS1xS+9av1/GDFRp7YPbj3/YI4qaudxbPaOWVWB2fM7eRFi7uolNxRM7MjR8faRdOWLFkSy5Yta3YZh6y/NsT6rbu559Ee1mzq4Z5NO7hv8w7Wb90NwIwpFd7yrPm87qy59NeG2Li9j227BnjR4i5OnDnlAHM3MxudpOURsWTU9xwER4ed/TXueGgr37pjHT9ds3nUex08Z9FxvHXJfC54xlzvUjKzg+IgOMZs7unjtrVbmD6lwtyprbSUC/z3yk18e9kGHtyyk7lTW/jj8xbzlmfNp1z0biQzOzAHwSQREdy2dgufuuE+Vqzfzgkz2njd0+fyrEXTeebC6fT217hnUw/3PdZLS7nAiTOncFJXO/OntyKp2eWbWRONFwQeLD6GSOJFi7t44ckzufmezXzplgf40s8eYGjp+GH+tHlT+eRbn85pszuPUKVmdixxj+AYt2ugxop12/n1+u10tpZ56uwOTpndQf9gnd9297J6Uw+X37yWnr5BPvCyxVx87knenWSWQ941lHOP9/bzl9et4vq7NtFRLXHyrHYWH9/OabM7OWvBNM6Y20lL2YPPZpOZg8AAuPmex1h6TzdrN/dy/+ZetvT2A8n5DItndXBS1xSe0tXO1NYy67fuYv3WXfTVhnj+STN52WnHc9rsjn3GGjbv6OMHv97I8oe38Z7nL+J5J81o1qKZ2QE4CGxUm3v6WLF+OyvWb2fNph4e6N7Jhm27qAdMqRRZmF5ee82mHgBmtleYPbWFme1VhurBzx94nKF60NFSore/xvteeCKXveJUWspFNvf0sWpjD4tmTmHRjDYPVps1WVOCQNJXgdcCmyPizHHaPZvkTma/FxHfOdB8HQTZ6hscYtfAENPbyns23o/19HHLvZtZ9tA2unv7ebx3gL7BIV5++ize/Mz5zJ3Wwid+uIZ//+U6TpjRRj1izwlyAHOmtvDcp8xgSrXIzv4hdvbXqJaLTGstM62tzIwpFY7vbGFWZ5WTutqZ1lZp1uKbTVrNCoIXA73A18cKAklF4EagD/iqg+DYtvTezXzup/czZ2oLzzphOmfOm8oD3b38/IHHuePBrQzVgynVIlMqJfprdbbvGuCJ3YM0njsnwelzOnneU2ZwyqwOyiVRLhYoFURByaNcKtBaLtJaLlIqiv5aPQ2wGlt6B9jS20/fwBCnz+3kmQunc3xnCwBD9aC3v0bjv3khCgUoFQq0VjxOYpNX03YNSVoEXD9OEFwKDALPTts5CHKmXg+27RrgsZ5+HtvRx8oNT/CLBx5n+bptDNTqhzxfCYb/ac9sr9JfG0pDYOzPzGyvcNrsTk6Z1cHAUHJ5j43bd1OrB9VSgZZykRlTKiw4ro0F01uZ2lamNhR7Aqa7t5/uHf1EsOccjjnTWiikPauIYHAoGByqMzhURxLDO8yG6kGtHtQj2dU2tbVMZ0uZ3YND7OgbZEdfjb7BOv21IQZqdarlIh0tJTpbSkxtrXDclApTW8sUC9rzPQNDSUD2DQ6xo6/G470DPL6zn4FanY6WMp2tJTpbyrRWknBurRSplgpUSwVq9WDj9t08/PguNu/op61SZEq1RHu1xLS2MtPbku8brr0eQblYoFjYfxfgQK3OE7sHeWL3IMWCaK+W6GgpMTBU5/HeAbbu7AdEV3uVro4qrZUi9XowWE/Wf6VYGHPX4vCyBpG+hv5a8nfqH0z+zgNDdQZrwVBEesZ+0NXewpxpLXuOoIsIevpqVIr7/iAYqNXZvKOP3QNDe/5dldM2UyolWsrFPcs8/G95S+8AW3cO0Fcbon8w+dy8aW0sPK6NqW3lPd83/ANIwFAEj/X0sXF7H4/19FGPoCBRKojO1jIz2pN1fFxbhdIhHvV3VJ5HIGke8EbgpSRBYDlUKIgZ7VVmtFc5nU5eeurxfOC8xfQNDrGlt5/ang1nsrGpR/J690Cd3YND1IbqVMsFWkpF2qolZkypMLO9SqEAdz/Sw6/XbeO+x3bQVinR2Vqms6W053/cCAiS/4EHhuo8tGUn9z62g2/e8TCt5SJzp7Uyf3oblZLoG0w2qg9u2cmt93fTN7h/SFWKBbo6qkQE3/v1I0f4L5lspArSqJcnOZR5HcpvxEK6oRz+fMBBB/rI7y4IqqUi1XKBopSEHbB7IOkFHuriFgtidmcLA0N1tu0coJbOqKVc4Li2Cv21Oo9P4C6Dw+EwVI8D/u1bygXqAYND9UP6+77vhSfy5689/eA/eADNPKHss8CHIqJ+oIFESRcBFwEsXLgw+8qs6VrKReZPb3tS83jWCdN51gnTD1NFe0UE3b397OofolhINkxtlSJTW/eOq+waqPHglp1s7unf57OVUmGfX87DPfJiQZQKyQZ0R1+NJ3YP0NNXozX95d/RUqa1nGwMK8UC/bUhevpq7OirsX3XANt2DrB11yBD9TqlQoFyUVTSXkxLqUh7SxKSM9qrVIoFetJeRk/fYLpBTTaq/bX6nl+x86e3sXBGG7M7W+iv1entH6Snr8YTuwbZtmuAnt21tPbkZMehehLS+2z4Be2VElPbykxtLVOPoLevtufX9/Av3QC6dyQ9qt0DQ5SKyS7BiKC/Vmf3wBADQ/U9vY8IaKuUaKsUaSnv7TEoDY1KqUC1WKBaTv7eyd88CcogOVBiw7bdPLJtN5VSIfm1PaXC4FCwdWc/W3cOUikVmN3ZwuypVaZUS3s23AO1OrsGh9jVn/TSavXkh0qxQNqraWH6lHR9lYrUI3hk+27Wb93FYz19FAsFKkVRLOwNzIKgqyO5RP3sqS0UC0p6RUPBE7sH2boz6TmdNiebk0KbtmtI0oOwp2c8E9gFXBQR3x9vnt41ZGZ28I7KXUMRceLwc0lXkQTG95tVj5lZXmUWBJKuAc4FZkraAPwlUAaIiC9n9b1mZnZwMguCiHj7QbT9g6zqMDOz8fnqY2ZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnPH3GWoJXUDDx/ix2cCWw5jOceKPC53HpcZ8rnceVxmOPjlPiEiukZ745gLgidD0rKxzqybzPK43HlcZsjncudxmeHwLrd3DZmZ5ZyDwMws5/IWBFc0u4AmyeNy53GZIZ/LncdlhsO43LkaIzAzs/3lrUdgZmYjOAjMzHIuN0Eg6XxJ90paK+nDza4nC5IWSFoqabWkVZIuSacfJ+lGSfen/z38t+06CkgqSvq1pOvT1ydKuj1d5/8hqdLsGg8nSdMkfUfSPZLWSHpeHta1pA+m/77vlnSNpJbJuK4lfVXSZkl3N0wbdf0q8fl0+e+S9MyD+a5cBIGkIvAF4FXA6cDbJR3+G382Xw24LCJOB54LvD9dzg8DN0XEYuCm9PVkdAmwpuH1PwCfiYiTgW3AhU2pKjufA34cEacBZ5Es+6Re1+m9zj8ALEnvfFgEfo/Jua6vAs4fMW2s9fsqYHH6uAj40sF8US6CAHgOsDYifhsRA8C3gNc3uabDLiI2RcSd6fMdJBuGeSTL+rW02deANzSlwAxJmg+8BvhK+lrAy4DvpE0m1XJLmgq8GLgSICIGImI7OVjXJPdRaZVUAtqATUzCdR0RtwJbR0wea/2+Hvh6JH4JTJM0Z6LflZcgmAesb3i9IZ02aaX3iz4buB2YFRGb0rceBWY1q64MfRb4v8DwndNnANsjopa+nmzr/ESgG/i3dHfYVyRNYZKv64h4BPgnYB1JADwBLGdyr+tGY63fJ7WNy0sQ5IqkduC7wKUR0dP4XiTHC0+qY4YlvRbYHBHLm13LEVQCngl8KSLOBnYyYjfQJF3X00l+/Z4IzAWmsP/uk1w4nOs3L0HwCLCg4fX8dNqkI6lMEgJXR8S16eTHhruJ6X83N6u+jLwAuEDSQyS7/V5Gsv98Wrr7ACbfOt8AbIiI29PX3yEJhsm+rl8OPBgR3RExCFxLsv4n87puNNb6fVLbuLwEwa+AxemRBRWSwaXrmlzTYZfuF78SWBMRn2546zrgPenz9wA/ONK1ZSkiPhIR8yNiEcm6vTki3gksBd6SNptUyx0RjwLrJZ2aTjoPWM0kX9cku4SeK6kt/fc+vNyTdl2PMNb6vQ54d3r00HOBJxp2IR1YROTiAbwauA94APizZteT0TK+kKSreBewIn28mmR/+U3A/cBPgeOaXWuGf4NzgevT508B7gDWAt8Gqs2u7zAv6zOAZen6/j4wPQ/rGvg4cA9wN/ANoDoZ1zVwDck4yCBJD/DCsdYvIJIjIx8AVpIcVTXh7/IlJszMci4vu4bMzGwMDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwG0HSkKQVDY/DduE2SYsaryZpdjQoHbiJWe7sjohnNLsIsyPFPQKzCZL0kKR/lLRS0h2STk6nL5J0c3od+JskLUynz5L0PUm/SR/PT2dVlPSv6TX1b5DU2rSFMsNBYDaa1hG7ht7W8N4TEfE04HKSK54C/DPwtYh4OnA18Pl0+ueBn0XEWSTXAVqVTl8MfCEizgC2A2/OdGnMDsBnFpuNIKk3ItpHmf4Q8LKI+G16cb9HI2KGpC3AnIgYTKdvioiZkrqB+RHR3zCPRcCNkdxYBEkfAsoR8bdHYNHMRuUegdnBiTGeH4z+hudDeKzOmsxBYHZw3tbw31+kz39OctVTgHcC/5M+vwn4I9hzP+WpR6pIs4PhXyJm+2uVtKLh9Y8jYvgQ0umS7iL5Vf/2dNofk9wp7E9J7hr23nT6JcAVki4k+eX/RyRXkzQ7qniMwGyC0jGCJRGxpdm1mB1O3jVkZpZz7hGYmeWcewRmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZz/x/y26YlaMDw6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), training_loss)\n",
    "# plt.plot(range(epochs), validation_loss)\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
