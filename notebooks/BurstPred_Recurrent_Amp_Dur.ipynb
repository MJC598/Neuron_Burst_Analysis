{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import scipy.io\n",
    "import random\n",
    "import pandas as pds\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Explanations\n",
    "\n",
    "These are 3 regression RNN-based models. In order to change it to a classifier the \n",
    "nn.Linear layers must have their second parameter changed to match the number of \n",
    "expected outputs.\n",
    "\n",
    "* Expected Input Shape: (batch_size, time_sequence, features)\n",
    "\n",
    "* Input_Size - number of features\n",
    "* Hidden_Size - number of connections between the hidden layers\n",
    "* Batch_Size - How many samples you want to push through the network before executing backprop\n",
    "    (this is a hyperparameter that can change how fast or slow a model converges)\n",
    "* Batch_First - Should always be set to True to keep input shape the same\n",
    "* Dropout - Only really does anything with more than 1 layer on the LSTM, RNN, GRU. Useful to help generalize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineRNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h_n  = self.rnn1(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        self.c0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.h0.shape)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class conv1DLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(conv1DLSTM, self).__init__()\n",
    "        self.c1 = nn.Conv1d(input_size, hidden_size, 3)\n",
    "        self.p1 = nn.AvgPool1d(3)\n",
    "        self.c2 = nn.Conv1d(hidden_size, hidden_size, 2)\n",
    "        self.p2 = nn.AvgPool1d(2)\n",
    "        self.c3 = nn.Conv1d(hidden_size, hidden_size, 1)\n",
    "#         self.p1 = nn.AvgPool1d(2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.rnn = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        self.c0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #switch (batch, sequence, feature) to (batch, feature, sequence)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.c1(x)\n",
    "        x = self.p1(x)\n",
    "#         x = self.tanh(x)\n",
    "#         x = self.c2(x)\n",
    "#         x = self.p2(x)\n",
    "#         x = self.dropout(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        #switch backwards\n",
    "        x = x.transpose(1,2)\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class conv1DGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(conv1DGRU, self).__init__()\n",
    "        self.c1 = nn.Conv1d(input_size, hidden_size, 5)\n",
    "        self.p1 = nn.AvgPool1d(5)\n",
    "        self.c2 = nn.Conv1d(hidden_size, hidden_size, 3)\n",
    "        self.p2 = nn.AvgPool1d(3)\n",
    "        self.c3 = nn.Conv1d(hidden_size, hidden_size, 1)\n",
    "#         self.p1 = nn.AvgPool1d(2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.rnn = nn.GRU(input_size=hidden_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #switch (batch, sequence, feature) to (batch, feature, sequence)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.c1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.c2(x)\n",
    "#         x = self.p2(x)\n",
    "#         x = self.c3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        #switch backwards\n",
    "        x = x.transpose(1,2)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 80\n",
    "FRONT_TIME = -25\n",
    "BACK_TIME = 5\n",
    "T_START = 50+FRONT_TIME\n",
    "T_END = 50+BACK_TIME\n",
    "MODEL = conv1DLSTM\n",
    "OUTPUT = 'full'\n",
    "LOSS_FILE = ('losses/bursts/losses_' + str(MODEL) + \n",
    "             '_' + OUTPUT + '_' + str(FRONT_TIME) + \n",
    "             '_' + str(T_END) + '_fullin.csv')\n",
    "PATH = ('models/bursts/' + str(MODEL) + '_' + OUTPUT + \n",
    "        '_' + str(FRONT_TIME) + '_' + str(T_END) + \n",
    "        '_fullin.pth')\n",
    "DATA_PATH = 'data/info_collect_for_NN_network_burst_separatePNITN.mat'\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    LOSS_FILE = COLAB_PRE + LOSS_FILE\n",
    "    PATH = COLAB_PRE + PATH\n",
    "    DATA_PATH = COLAB_PRE + DATA_PATH\n",
    "\n",
    "# Specific Model Parameters\n",
    "input_size = 8\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "batch_size = 32\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "dropout = 0.0\n",
    "epochs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['burst duration(ms)'], dtype='<U18')\n",
      " array(['burst amp (uV)'], dtype='<U14')\n",
      " array(['pre-burst PN FR in 50ms window'], dtype='<U30')\n",
      " array(['pre-burst ITN FR in 50ms window'], dtype='<U31')\n",
      " array(['pre-burst AFF Rate to PN in 50ms window'], dtype='<U39')\n",
      " array(['pre-burst AFF Rate to ITN in 50ms window'], dtype='<U40')\n",
      " array(['pre-burst exc noise conduct (nS) of PN in 50ms window'],\n",
      "      dtype='<U53')\n",
      " array(['pre-burst inh noise conduct (nS) of PN in 50ms window'],\n",
      "      dtype='<U53')\n",
      " array(['pre-burst exc noise conduct (nS) of ITN in 50ms window'],\n",
      "      dtype='<U54')\n",
      " array(['pre-burst inh noise conduct (nS) of ITN in 50ms window'],\n",
      "      dtype='<U54')\n",
      " array(['within-burst PN FR'], dtype='<U18')\n",
      " array(['ITN FR within-burst '], dtype='<U20')\n",
      " array(['AFF Rate to PN within-burst'], dtype='<U27')\n",
      " array(['AFF Rate to ITN within-burst'], dtype='<U28')\n",
      " array(['exc noise conduct (nS) of PN within-burst'], dtype='<U41')\n",
      " array(['inh noise conduct (nS) of PN within-burst'], dtype='<U41')\n",
      " array(['exc noise conduct (nS) of ITN within-burst'], dtype='<U42')\n",
      " array(['inh noise conduct (nS) of ITN within-burst'], dtype='<U42')]\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498, 50, 1)\n",
      "(5498,)\n",
      "(5498,)\n",
      "(5498,)\n",
      "(5498,)\n",
      "(5498,)\n",
      "(5498,)\n",
      "(5498,)\n",
      "(5498,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1e52ce872a94>:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_fr_pn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_fr_itn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_aff_pn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_aff_itn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_exc_pn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_inh_pn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:66: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_exc_itn).shape)\n",
      "<ipython-input-14-1e52ce872a94>:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(wb_inh_itn).shape)\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1e52ce872a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mget_data_from_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-1e52ce872a94>\u001b[0m in \u001b[0;36mget_data_from_mat\u001b[0;34m(file_path, output_index, type)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                 pb_exc_pn, pb_inh_pn, pb_exc_itn, pb_inh_itn), axis=2)\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     rear_data = np.concatenate((wb_fr_pn, wb_fr_itn, wb_aff_pn, wb_aff_itn,\n\u001b[0m\u001b[1;32m     75\u001b[0m                                 wb_exc_pn, wb_inh_pn, wb_exc_itn, wb_inh_itn), axis=2)\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "def get_data_from_mat(file_path, output_index=None, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    duration = []\n",
    "    amp = []\n",
    "    pb_fr_pn = []\n",
    "    pb_fr_itn = []\n",
    "    pb_aff_pn = []\n",
    "    pb_aff_itn = []\n",
    "    pb_exc_pn = []\n",
    "    pb_inh_pn = []\n",
    "    pb_exc_itn = []\n",
    "    pb_inh_itn = []\n",
    "    wb_fr_pn = []\n",
    "    wb_fr_itn = []\n",
    "    wb_aff_pn = []\n",
    "    wb_aff_itn = []\n",
    "    wb_exc_pn = []\n",
    "    wb_inh_pn = []\n",
    "    wb_exc_itn = []\n",
    "    wb_inh_itn = []\n",
    "\n",
    "    print(data['info_collect'][0])\n",
    "#     print(data['info_collect'].shape)\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        arr = data['info_collect'][i]\n",
    "        \n",
    "#         print(arr.shape)\n",
    "        \n",
    "        duration.append(arr[0])\n",
    "        amp.append(arr[1])\n",
    "        \n",
    "        pb_fr_pn.append(arr[2])\n",
    "        pb_fr_itn.append(arr[3])\n",
    "        pb_aff_pn.append(arr[4])\n",
    "        pb_aff_itn.append(arr[5])\n",
    "        pb_exc_pn.append(arr[6])\n",
    "        pb_inh_pn.append(arr[7])\n",
    "        pb_exc_itn.append(arr[8])\n",
    "        pb_inh_itn.append(arr[9])\n",
    "        \n",
    "#         print(arr[10].shape)\n",
    "        wb_fr_pn.append(arr[10])\n",
    "        wb_fr_itn.append(arr[11])\n",
    "        wb_aff_pn.append(arr[12])\n",
    "        wb_aff_itn.append(arr[13])\n",
    "        wb_exc_pn.append(arr[14])\n",
    "        wb_inh_pn.append(arr[15])\n",
    "        wb_exc_itn.append(arr[16])\n",
    "        wb_inh_itn.append(arr[17])\n",
    "    \n",
    "    print(np.array(pb_fr_pn).shape)\n",
    "    print(np.array(pb_fr_itn).shape)\n",
    "    print(np.array(pb_aff_pn).shape)\n",
    "    print(np.array(pb_aff_itn).shape)\n",
    "    print(np.array(pb_exc_pn).shape)\n",
    "    print(np.array(pb_inh_pn).shape)\n",
    "    print(np.array(pb_exc_itn).shape)\n",
    "    print(np.array(pb_inh_itn).shape)\n",
    "    \n",
    "    print(np.array(wb_fr_pn).shape)\n",
    "    print(np.array(wb_fr_itn).shape)\n",
    "    print(np.array(wb_aff_pn).shape)\n",
    "    print(np.array(wb_aff_itn).shape)\n",
    "    print(np.array(wb_exc_pn).shape)\n",
    "    print(np.array(wb_inh_pn).shape)\n",
    "    print(np.array(wb_exc_itn).shape)\n",
    "    print(np.array(wb_inh_itn).shape)\n",
    "    \n",
    "    full_labels = np.concatenate((duration, amp), axis=2)\n",
    "    \n",
    "    front_data = np.concatenate((pb_fr_pn, pb_fr_itn, pb_aff_pn, pb_aff_itn, \n",
    "                                pb_exc_pn, pb_inh_pn, pb_exc_itn, pb_inh_itn), axis=2)\n",
    "    \n",
    "    rear_data = np.concatenate((wb_fr_pn, wb_fr_itn, wb_aff_pn, wb_aff_itn,\n",
    "                                wb_exc_pn, wb_inh_pn, wb_exc_itn, wb_inh_itn), axis=2)\n",
    "    \n",
    "    for j in range(2):\n",
    "        x = full_labels[:,:,j]\n",
    "        full_labels[:,:,j] = (x - np.min(x))/(np.max(x)-np.min(x))\n",
    "    \n",
    "    for i in range(full_data.shape[0]):\n",
    "        for j in range(input_size):\n",
    "            x = full_data[i,:,j]\n",
    "            full_data[i,:,j] = (x - np.min(x))/(np.max(x)-np.min(x))\n",
    "    \n",
    "    print(full_data.shape)\n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 24832 #24848\n",
    "    k = 19808\n",
    "    full = np.arange(data_samples)\n",
    "    training_indices = np.random.choice(full, size=k, replace=False)\n",
    "    validation_indices = np.delete(full,training_indices)\n",
    "    \n",
    "    training_data = full_data[training_indices,T_START:T_END,:] \n",
    "    validation_data = full_data[validation_indices,T_START:T_END,:]\n",
    "    \n",
    "    if output_index is None:\n",
    "        training_labels = full_labels[training_indices,:,:] \n",
    "    else:\n",
    "        training_labels = full_labels[training_indices,:,output_index]\n",
    "    \n",
    "    if output_index is None:\n",
    "        validation_labels = full_labels[validation_indices,:,:]\n",
    "    else:\n",
    "        validation_labels = full_labels[validation_indices,:,output_index]\n",
    "    \n",
    "#     print(training_data.shape)\n",
    "#     print(training_labels.shape)\n",
    "#     print(validation_data.shape)\n",
    "#     print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset\n",
    "get_data_from_mat(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method\n",
    "* Model - Model initialized based on classes above\n",
    "* Save_Filepath - Where you want to save the model to. Should end with a .pt or .pth extension. This is how you are able to load the model later for testing, etc.\n",
    "* training_loader - dataloader iterable with training dataset samples\n",
    "* validation_loader - dataloader iterable with validation dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,count):\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "\n",
    "    \"\"\"\n",
    "    This is your optimizer. It can be changed but Adam is generally used. \n",
    "    Learning rate (alpha in gradient descent) is set to 0.001 but again \n",
    "    can easily be adjusted if you are getting issues\n",
    "\n",
    "    Loss function is set to Mean Squared Error. If you switch to a classifier \n",
    "    I'd recommend switching the loss function to nn.CrossEntropyLoss(), but this \n",
    "    is also something that can be changed if you feel a better loss function would work\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = 0.93 #decay the lr each step to 93% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):  \n",
    "                output = model(x)                       \n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y))  \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                                      \n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%10 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    lf = ('losses/losses_' + str(MODEL) + '_' \n",
    "          + OUTPUT + '_' + str(FRONT_TIME) + '_' \n",
    "          + str(T_END) + str(count) + '_fullin.csv')\n",
    "    loss_df.to_csv(lf, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scoring\n",
    "* Model - same model as sent to train_model\n",
    "* testing_dataloader - whichever dataloader you want to R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_eval(model, testing_dataloader):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    for i, (x, y) in enumerate(testing_dataloader):\n",
    "        output = model(x) \n",
    "        output_list.append(np.transpose(output.detach().cpu().numpy()))\n",
    "        labels_list.append(np.transpose(y.detach().cpu().numpy()))\n",
    "    output_list = np.transpose(np.hstack(output_list))\n",
    "    labels_list = np.transpose(np.hstack(labels_list)).reshape((-1,output_size))\n",
    "#     print(output_list.shape)\n",
    "#     print(labels_list.shape)\n",
    "    print(r2_score(np.squeeze(labels_list), output_list))\n",
    "    return output_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MODEL(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "model2 = MODEL(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "model3 = MODEL(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "\n",
    "a1_training_dataset, a1_validation_dataset = get_data_from_mat(DATA_PATH, 0) #retrieve amps1\n",
    "a2_training_dataset, a2_validation_dataset = get_data_from_mat(DATA_PATH, 1) #retrieve amps2\n",
    "d_training_dataset, d_validation_dataset = get_data_from_mat(DATA_PATH, 2) #retrieve duration\n",
    "# b_training_dataset, b_validation_dataset = get_data_from_mat(DATA_PATH, 3) #retrieve burst\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "a1_training_loader = DataLoader(dataset=a1_training_dataset,batch_size=batch_size,shuffle=True)\n",
    "a1_validation_loader = DataLoader(dataset=a1_validation_dataset,batch_size=batch_size)\n",
    "\n",
    "a2_training_loader = DataLoader(dataset=a2_training_dataset,batch_size=batch_size,shuffle=True)\n",
    "a2_validation_loader = DataLoader(dataset=a2_validation_dataset,batch_size=batch_size)\n",
    "\n",
    "d_training_loader = DataLoader(dataset=d_training_dataset,batch_size=batch_size,shuffle=True)\n",
    "d_validation_loader = DataLoader(dataset=d_validation_dataset,batch_size=batch_size)\n",
    "\n",
    "\n",
    "p1 = 'models/' + str(MODEL) + '_' + OUTPUT + '_' + str(FRONT_TIME) + '_' + str(T_END) + str(0) + '_fullin.pth'\n",
    "a1_training_loss, a1_validation_loss = train_model(model1,p1,a1_training_loader,a1_validation_loader,epochs,0)\n",
    "p2 = 'models/' + str(MODEL) + '_' + OUTPUT + '_' + str(FRONT_TIME) + '_' + str(T_END) + str(1) + '_fullin.pth'\n",
    "a2_training_loss, a2_validation_loss = train_model(model2,p2,a2_training_loader,a2_validation_loader,epochs,1)\n",
    "p3 = 'models/' + str(MODEL) + '_' + OUTPUT + '_' + str(FRONT_TIME) + '_' + str(T_END) + str(2) + '_fullin.pth'\n",
    "d_training_loss, d_validation_loss = train_model(model3,p3,d_training_loader,d_validation_loader,epochs,2)\n",
    "\n",
    "\n",
    "model1 = torch.load(p1)\n",
    "model1.eval()\n",
    "a1_t_output_list, a1_t_labels_list = r2_score_eval(model1, a1_training_loader)\n",
    "a1_v_output_list, a1_v_labels_list = r2_score_eval(model1, a1_validation_loader)\n",
    "\n",
    "model2 = torch.load(p2)\n",
    "model2.eval()\n",
    "a2_t_output_list, a2_t_labels_list = r2_score_eval(model2, a2_training_loader)\n",
    "a2_v_output_list, a2_v_labels_list = r2_score_eval(model2, a2_validation_loader)\n",
    "\n",
    "model3 = torch.load(p3)\n",
    "model3.eval()\n",
    "d_t_output_list, d_t_labels_list = r2_score_eval(model3, d_training_loader)\n",
    "d_v_output_list, d_v_labels_list = r2_score_eval(model3, d_validation_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=2)\n",
    "fig.tight_layout()\n",
    "ax[0,0].plot(range(epochs), a1_training_loss)\n",
    "ax[0,0].set_title('Validation Loss')\n",
    "ax[0,0].set_ylabel('Loss')\n",
    "ax[0,0].set_xlabel('Epoch')\n",
    "\n",
    "ax[0,1].plot(range(epochs), a1_validation_loss)\n",
    "ax[0,1].set_title('Training Loss')\n",
    "ax[0,1].set_ylabel('Loss')\n",
    "ax[0,1].set_xlabel('Epoch')\n",
    "\n",
    "\n",
    "ax[1,0].plot(np.arange(a1_v_labels_list.shape[0]), a1_v_labels_list[:,0], color='blue')\n",
    "ax[1,0].plot(np.arange(a1_v_labels_list.shape[0]), a1_v_output_list[:,0], color='red')\n",
    "ax[1,0].set_title('Validation Amps1 per Sample')\n",
    "ax[1,0].set_ylabel('Amp1')\n",
    "ax[1,0].set_xlabel('Sample')\n",
    "\n",
    "ax[1,1].plot(np.arange(a1_t_labels_list.shape[0]), a1_t_labels_list[:,0], color='blue')\n",
    "ax[1,1].plot(np.arange(a1_t_labels_list.shape[0]), a1_t_output_list[:,0], color='red')\n",
    "ax[1,1].set_title('Training Amps1 per Sample')\n",
    "ax[1,1].set_ylabel('Amp1')\n",
    "ax[1,1].set_xlabel('Sample')\n",
    "\n",
    "ax[2,0].plot(np.arange(a2_v_labels_list.shape[0]), a2_v_labels_list[:,0], color='blue')\n",
    "ax[2,0].plot(np.arange(a2_v_labels_list.shape[0]), a2_v_output_list[:,0], color='red')\n",
    "ax[2,0].set_title('Validation Amps2 per Sample')\n",
    "ax[2,0].set_ylabel('Amp2')\n",
    "ax[2,0].set_xlabel('Sample')\n",
    "\n",
    "ax[2,1].plot(np.arange(a2_t_labels_list.shape[0]), a2_t_labels_list[:,0], color='blue')\n",
    "ax[2,1].plot(np.arange(a2_t_labels_list.shape[0]), a2_t_output_list[:,0], color='red')\n",
    "ax[2,1].set_title('Training Amps2 per Sample')\n",
    "ax[2,1].set_ylabel('Amp2')\n",
    "ax[2,1].set_xlabel('Sample')\n",
    "\n",
    "ax[3,0].plot(np.arange(d_v_labels_list.shape[0]), d_v_labels_list[:,0], color='blue')\n",
    "ax[3,0].plot(np.arange(d_v_labels_list.shape[0]), d_v_output_list[:,0], color='red')\n",
    "ax[3,0].set_title('Validation Duration per Sample')\n",
    "ax[3,0].set_ylabel('Duration')\n",
    "ax[3,0].set_xlabel('Sample')\n",
    "\n",
    "ax[3,1].plot(np.arange(d_t_labels_list.shape[0]), d_t_labels_list[:,0], color='blue')\n",
    "ax[3,1].plot(np.arange(d_t_labels_list.shape[0]), d_t_output_list[:,0], color='red')\n",
    "ax[3,1].set_title('Training Duration per Sample')\n",
    "ax[3,1].set_ylabel('Duration')\n",
    "ax[3,1].set_xlabel('Sample')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
