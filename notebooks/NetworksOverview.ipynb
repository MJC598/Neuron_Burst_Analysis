{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Auto-Encoders\n",
    "<img src=\"images/AE.png\" alt=\"Vanilla Auto-Encoder\" width=\"400\"/>\n",
    "#### What is it?\n",
    "* An unsupervised method to learn data encodings. This can allow for de-noising giving strong data representations (i.e. the paper we examined)\n",
    "    - https://en.wikipedia.org/wiki/Autoencoder\n",
    "\n",
    "#### Papers Using Vanillia Auto-Encoders in a Time Series:\n",
    "* Worth noting, most of the papers I found use the AE as a dimensional reduction technique and fed into another network to do the predictive analysis. I included one of them and another paper that uses an LSTM as an encoder scheme (much more like the VAE papers were doing)\n",
    "* https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180944\n",
    "* https://www.nature.com/articles/s41598-019-55320-6\n",
    "\n",
    "## Variational Auto-Encoders\n",
    "<img src=\"images/VAE.png\" alt=\"Variational Auto-Encoder\" width=\"400\"/>\n",
    "#### What is it?\n",
    "* Generative models of the AE that keep their latent space continuous. This is done by keeping track of the $\\text{mean: } \\mu \\text{ and standard deviations: } \\sigma$\n",
    "     - https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
    "\n",
    "#### Papers Using Variational Auto-Encoders in a Time Series:\n",
    "* The initial VAE paper: https://arxiv.org/pdf/1312.6114.pdf\n",
    "* Recurrent VAE: https://arxiv.org/pdf/1412.6581.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaAutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, MinSize, D_out):\n",
    "        super(VanillaAutoEncoder, self).__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(D_in, D_in, 1)\n",
    "            nn.Linear(D_in, D_in),\n",
    "            nn.Conv2d(D_in, D_in,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(D_in/2, D_in/2,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(D_in/4, D_in/4,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(D_in/8, D_in/8,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(D_in/16, D_in/16,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2)\n",
    "            nn.Linear(D_in/8, D_in/8),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "            nn.Linear(D_in/4, D_in/4),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "            nn.Linear(D_in/2, D_in/2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "            nn.Linear(D_in, D_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(D_in, D_in, 1),\n",
    "            nn.Linear(D_in, D_out)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #if we wanted we could break the sequential steps and add skip connections\n",
    "        c = self.encoder(x) \n",
    "        d = self.decoder(c)\n",
    "        return d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
