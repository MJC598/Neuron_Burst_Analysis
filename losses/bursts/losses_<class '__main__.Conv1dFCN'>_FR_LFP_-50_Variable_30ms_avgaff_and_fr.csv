epoch,training loss,validation loss
0,126053.37761688232,26849.116165161133
1,101384.62390899658,26269.345504760742
2,100159.12125396729,26083.262985229492
3,99526.05878448486,25911.5498046875
4,99064.8519821167,25753.518661499023
5,98731.62678527832,25719.828002929688
6,98503.14789581299,25822.105949401855
7,98322.02657318115,25758.074325561523
8,98164.37286376953,25713.73021697998
9,98037.6503829956,25730.203170776367
10,97937.43361663818,25723.40288543701
11,97845.4681930542,25714.1346282959
12,97738.7945098877,25692.65640258789
13,97653.23701477051,25646.73769378662
14,97589.08996582031,25610.26050567627
15,97529.59789276123,25565.25528717041
16,97480.28054046631,25521.497009277344
17,97421.65952301025,25494.265159606934
18,97374.82372283936,25472.97560119629
19,97330.95039367676,25455.848602294922
20,97287.72947692871,25437.98162841797
21,97251.90797424316,25426.354026794434
22,97220.81380462646,25418.879104614258
23,97192.8018875122,25412.483642578125
24,97167.15957641602,25407.82152557373
25,97145.21413421631,25403.5611038208
26,97125.7094116211,25400.89170074463
27,97107.76051330566,25401.012718200684
28,97090.25175476074,25401.83522796631
29,97073.50960540771,25400.649520874023
30,97058.5892791748,25398.90142059326
31,97044.92405700684,25395.7820892334
32,97031.97412109375,25394.44497680664
33,97021.25215911865,25392.679122924805
34,97011.22618103027,25391.596099853516
35,97003.01100158691,25390.298141479492
36,96995.33515930176,25389.681434631348
37,96988.22552490234,25388.90821838379
38,96981.7364578247,25388.55886077881
39,96975.8925857544,25388.10231781006
40,96970.70971679688,25387.735008239746
41,96965.791015625,25387.584213256836
42,96961.2123260498,25387.52069091797
43,96957.16953277588,25387.402824401855
44,96953.37243652344,25387.392112731934
45,96950.05234527588,25387.535438537598
46,96946.96705627441,25387.55532836914
47,96944.13259124756,25387.53116607666
48,96941.68710327148,25387.61538696289
49,96939.44830322266,25387.650115966797
