{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import scipy.io\n",
    "import random\n",
    "import pandas as pds\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Explanations\n",
    "\n",
    "These are 3 regression RNN-based models. In order to change it to a classifier the \n",
    "nn.Linear layers must have their second parameter changed to match the number of \n",
    "expected outputs.\n",
    "\n",
    "* Expected Input Shape: (batch_size, time_sequence, features)\n",
    "\n",
    "* Input_Size - number of features\n",
    "* Hidden_Size - number of connections between the hidden layers\n",
    "* Batch_Size - How many samples you want to push through the network before executing backprop\n",
    "    (this is a hyperparameter that can change how fast or slow a model converges)\n",
    "* Batch_First - Should always be set to True to keep input shape the same\n",
    "* Dropout - Only really does anything with more than 1 layer on the LSTM, RNN, GRU. Useful to help generalize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineRNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size,hidden_size,num_layers,batch_first,dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h_n  = self.rnn1(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "        self.c0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0):\n",
    "        super(baselineGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size,hidden_size,num_layers,batch_first,dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.h0.shape)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_mat(file_path, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    duration = []\n",
    "    amp = []\n",
    "    pre_pn = []\n",
    "    pre_itn = []\n",
    "    pre_aff = []\n",
    "    pre_point_exc = []\n",
    "    pre_point_inh = []\n",
    "\n",
    "\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        duration.append(data['info_collect'][i][0])\n",
    "        amp.append(data['info_collect'][i][1])\n",
    "        pre_pn.append(data['info_collect'][i][2])\n",
    "        pre_itn.append(data['info_collect'][i][3])\n",
    "        pre_aff.append(data['info_collect'][i][4])\n",
    "        pre_point_exc.append(data['info_collect'][i][5])\n",
    "        pre_point_inh.append(data['info_collect'][i][6])\n",
    "        \n",
    "    \n",
    "\n",
    "    full_data = np.concatenate((pre_pn, pre_itn, pre_aff, pre_point_exc, pre_point_inh), axis=2)\n",
    "    full_labels = np.concatenate((amp, duration), axis=2)\n",
    "    \n",
    "    x = full_labels[:,:,1]\n",
    "    normalized_duration = (x-min(x))/(max(x)-min(x))\n",
    "    full_labels[:,:,1] = normalized_duration\n",
    "    \n",
    "#     print(full_labels)\n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 5446\n",
    "    k = 4981\n",
    "    full = np.arange(data_samples)\n",
    "    training_indices = np.random.choice(full, size=k, replace=False)\n",
    "    validation_indices = np.delete(full,training_indices)\n",
    "    training_data = full_data[training_indices,:,:]\n",
    "    training_labels = full_labels[training_indices,:,:]\n",
    "    validation_data = full_data[validation_indices,:,:]\n",
    "    validation_labels = full_labels[validation_indices,:,:]\n",
    "    \n",
    "#     print(training_data.shape)\n",
    "#     print(training_labels.shape)\n",
    "#     print(validation_data.shape)\n",
    "#     print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method\n",
    "* Model - Model initialized based on classes above\n",
    "* Save_Filepath - Where you want to save the model to. Should end with a .pt or .pth extension. This is how you are able to load the model later for testing, etc.\n",
    "* training_loader - dataloader iterable with training dataset samples\n",
    "* validation_loader - dataloader iterable with validation dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader):\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "\n",
    "    \"\"\"\n",
    "    This is your optimizer. It can be changed but Adam is generally used. \n",
    "    Learning rate (alpha in gradient descent) is set to 0.001 but again \n",
    "    can easily be adjusted if you are getting issues\n",
    "\n",
    "    Loss function is set to Mean Squared Error. If you switch to a classifier \n",
    "    I'd recommend switching the loss function to nn.CrossEntropyLoss(), but this \n",
    "    is also something that can be changed if you feel a better loss function would work\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(20), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):  \n",
    "                # This permutation is done so it fits into the model. \n",
    "                # I reversed the features and sequence length with my EEG data\n",
    "                # So I had to fix it here, it will come back later with another permute\n",
    "#                 x = x.permute(0, 2, 1)\n",
    "#                 print(x.size())\n",
    "                output = model(x)\n",
    "                #Computing loss  \n",
    "#                 print(output.shape)\n",
    "#                 print(y.shape)                            \n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y))  \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                                      \n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "    print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    loss_df.to_csv('losses/losses_lstm_csv.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scoring\n",
    "* Model - same model as sent to train_model\n",
    "* testing_dataloader - whichever dataloader you want to R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_eval(model, testing_dataloader):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    for i, (x, y) in enumerate(testing_dataloader):      \n",
    "        # Same permute issue we had in training. Basically switching from (batch_size, features, time) \n",
    "        # to (batch_size, time, features) \n",
    "#         x = x.permute(0, 2, 1)\n",
    "        output = model(x) \n",
    "        output_list.append(np.transpose(output.detach().cpu().numpy()))\n",
    "        labels_list.append(y.detach().cpu().numpy())\n",
    "    output_list = np.transpose(np.hstack(output_list))\n",
    "    labels_list = np.hstack(labels_list)\n",
    "    print(output_list.shape)\n",
    "    print(np.squeeze(labels_list).shape)\n",
    "    print(r2_score(np.squeeze(labels_list), output_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f49e15e6f04b5ea7f7e1a7a391d49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   465] train loss: 43.162705 val loss: 3.963112\n",
      "[2,   465] train loss: 40.410835 val loss: 3.562354\n",
      "[3,   465] train loss: 39.698448 val loss: 3.929242\n",
      "[4,   465] train loss: 38.926575 val loss: 3.777551\n",
      "[5,   465] train loss: 38.464431 val loss: 3.942404\n",
      "[6,   465] train loss: 38.781069 val loss: 3.642235\n",
      "[7,   465] train loss: 38.724165 val loss: 3.603128\n",
      "[8,   465] train loss: 39.154027 val loss: 3.592591\n",
      "[9,   465] train loss: 38.983537 val loss: 3.635056\n",
      "[10,   465] train loss: 37.712845 val loss: 3.679714\n",
      "[11,   465] train loss: 37.472054 val loss: 3.715784\n",
      "[12,   465] train loss: 37.528957 val loss: 3.586131\n",
      "[13,   465] train loss: 37.592270 val loss: 3.563864\n",
      "[14,   465] train loss: 37.398121 val loss: 4.090488\n",
      "[15,   465] train loss: 37.411133 val loss: 3.582369\n",
      "[16,   465] train loss: 37.665743 val loss: 3.694834\n",
      "[17,   465] train loss: 37.528119 val loss: 3.629892\n",
      "[18,   465] train loss: 37.527829 val loss: 3.566667\n",
      "[19,   465] train loss: 37.467417 val loss: 4.054190\n",
      "[20,   465] train loss: 37.717058 val loss: 3.781122\n",
      "880.9937717914581\n",
      "(4981, 2)\n",
      "(4981, 2)\n",
      "-0.40551889679218955\n",
      "(465, 2)\n",
      "(465, 2)\n",
      "-0.49000181384990316\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = 'data/new_bursts.mat'\n",
    "    input_size = 5\n",
    "    hidden_size = 25\n",
    "    output_size = 2\n",
    "    batch_size = 1\n",
    "    num_layers = 1\n",
    "    batch_first = True\n",
    "    dropout = 0.0\n",
    "    model = baselineLSTM(input_size,hidden_size,output_size,batch_size,num_layers,batch_first,dropout)\n",
    "    # model = baselineGRU(input_size,hidden_size,batch_size,batch_first,0)\n",
    "    # model = baselineRNN(input_size,hidden_size,batch_size,batch_first)\n",
    "    training_dataset, validation_dataset = get_data_from_mat(DATA_PATH) #retrieve data function\n",
    "    \n",
    "    # Turn datasets into iterable dataloaders\n",
    "    training_loader = DataLoader(dataset=training_dataset,batch_size=batch_size,shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    PATH = 'models/baselineLSTM.pth'\n",
    "    train_model(model,PATH,training_loader,validation_loader)\n",
    "    model = torch.load(PATH)\n",
    "    model.eval()\n",
    "    r2_score_eval(model, training_loader)\n",
    "    r2_score_eval(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
