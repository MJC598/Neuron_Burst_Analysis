{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score\n",
    "import copy\n",
    "\n",
    "s = 67\n",
    "\n",
    "rs = RandomState(MT19937(SeedSequence(s)))\n",
    "rng = default_rng(seed=s)\n",
    "torch.manual_seed(s)\n",
    "\n",
    "plt.rcParams.update({'font.size': 32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fully Connected Network](graphs/fullyConnectedNetwork.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, in_size, h_size, out_size):\n",
    "        super(FCN,self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, h_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "#         self.fc2 = nn.Linear(h_size, 40)\n",
    "#         self.fc3 = nn.Linear(40, 15)\n",
    "        self.fc2 = nn.Linear(h_size, out_size)\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "#         x = self.tanh(self.fc2(x))\n",
    "#         x = self.tanh(self.fc3(x))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formerly Conv1dFCN\n",
    "class LFPNet1C(nn.Module):\n",
    "    def __init__(self, in_size, h_size, out_size):\n",
    "        super(LFPNet1C, self).__init__()\n",
    "        self.cn1 = nn.Conv1d(1, 1, kernel_size=5,padding=2)\n",
    "        self.cn2 = nn.Conv1d(1, 1, kernel_size=3,padding=1)\n",
    "        self.bn = nn.BatchNorm1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(int(in_size), h_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(h_size, out_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = self.relu(self.cn1(x))\n",
    "        x = self.relu(self.cn2(x))\n",
    "        x += residual\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFPNetMC(nn.Module):\n",
    "    def __init__(self, in_size, h_size, out_size):\n",
    "        super(LFPNetMC, self).__init__()\n",
    "        self.cn1 = nn.Conv1d(2, 2, kernel_size=5,padding=2)\n",
    "        self.cn2 = nn.Conv1d(2, 2, kernel_size=3,padding=1)\n",
    "        self.cn3 = nn.Conv1d(2, 1, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(int(in_size), h_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(h_size, out_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = self.relu(self.cn1(x))\n",
    "        x = self.relu(self.cn2(x))\n",
    "        x += residual\n",
    "        x = self.relu(self.cn3(x))\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Model Parameters\n",
    "INPUT_FEATURES = 1\n",
    "PREVIOUS_TIME = 50\n",
    "LOOK_AHEAD = 20\n",
    "INPUT_SIZE = INPUT_FEATURES * PREVIOUS_TIME\n",
    "HIDDEN_SIZE = 95\n",
    "OUTPUT_SIZE = 1\n",
    "BATCH_SIZE = 512\n",
    "BATCH_FIRST = True\n",
    "DROPOUT = 0.0\n",
    "EPOCHS = 20\n",
    "\n",
    "MODEL = LFPNetMC\n",
    "MODEL_NAME = 'LFPNetMC'\n",
    "OUTPUT = 'FilteredLFP'\n",
    "INPUT = 'ITNFRRawLFP'\n",
    "LOSS_FILE = ('losses/bursts/losses_' + str(MODEL_NAME) + \n",
    "             '_' + INPUT + str(PREVIOUS_TIME) + '_' + OUTPUT + str(LOOK_AHEAD) + '.csv')\n",
    "PATH = ('models/LFPNet/' + str(MODEL_NAME) + '_' + INPUT + str(PREVIOUS_TIME) +\n",
    "        '_' + OUTPUT + str(LOOK_AHEAD) + '.pt')\n",
    "# PATH = 'models/<class \\'__main__.FCN\\'>_FR_LFP_-50_90_full.pth'\n",
    "\n",
    "DATA_PATH = 'data/bursts/burst_separatePNITNv3.mat'\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    LOSS_FILE = COLAB_PRE + LOSS_FILE\n",
    "    PATH = COLAB_PRE + PATH\n",
    "    DATA_PATH = COLAB_PRE + DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Data From Matlab File\n",
    "\n",
    "Read in Matlab info and Average over sliding window of size 3 PN Afferent and PN Firing Rate. \n",
    "Variables:\n",
    "$$\n",
    "x = \\text{PN Firing Rate}\\\\\n",
    "y = \\text{ITN Firing Rate}\\\\\n",
    "z = \\text{Local Field Potential}\\\\\n",
    "e_{1} = \\text{PN Afferent}\\\\\n",
    "e_{2} = \\text{ITN Afferent}\\\\\n",
    "e_{3} = \\text{PN Excitatory Point Conductance}\\\\\n",
    "e_{4} = \\text{ITN Excitatory Point Conductance}\\\\\n",
    "e_{5} = \\text{PN Inhibitory Point Conductance}\\\\\n",
    "e_{6} = \\text{ITN Inhibitory Point Conductance}\\\\\n",
    "t = \\text{Timestep}\\\\\n",
    "m = \\text{Number of Previous Timesteps}\\\\\n",
    "N = \\text{Number of Samples}\\\\\n",
    "n = \\text{Individual Sample}\\\\\n",
    "f = \\text{Number of Features}\\\\\n",
    "\\omega(n) = \\text{Time Sequence of Sample n}\\\\\n",
    "$$\n",
    "\n",
    "Sliding Window:\n",
    "$$\n",
    "x_{t} = \n",
    "\\begin{cases} \n",
    "    \\frac{1}{3}\\sum_{i=0}^{2}x_{t+i} & \\text{if } t \\geq 2\\\\\n",
    "    x_{t} & otherwise\\\\\n",
    "\\end{cases}\\\\\n",
    "e_{1,t} = \n",
    "\\begin{cases}\n",
    "    \\frac{1}{3}\\sum_{i=0}^{2}e_{1,t+i}& \\text{if } t \\geq 2\\\\\n",
    "    e_{1,t} & otherwise\\\\\n",
    "\\end{cases}\\\\\n",
    "$$\n",
    "\n",
    "Data $\\text{size} = (N \\times 1) , n = (\\omega(n) \\times f)$:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{t} & y_{t} & e_{1, t} & e_{2, t} & e_{3, t} & e_{4, t} & e_{5, t} & e_{6, t} & z_{t}\\\\\n",
    "x_{t+1} & y_{t+1} & e_{1, t+1} & e_{2, t+1} & e_{3, t+1} & e_{4, t+1} & e_{5, t+1} & e_{6, t+1} & z_{t+1}\\\\\n",
    "...\\\\\n",
    "x_{t+\\omega(n)} & y_{t+\\omega(n)} & e_{1, t+\\omega(n)} & e_{2, t+\\omega(n)} & e_{3, t+\\omega(n)} & e_{4, t+\\omega(n)} & e_{5, t+\\omega(n)} & e_{6, t+\\omega(n)} & z_{t+\\omega(n)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Label $\\text{size} = (N \\times 1) , n = (\\omega(n) \\times 3)$:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{t} & y_{t} & z_{t}\\\\\n",
    "x_{t+1} & y_{t+1} & z_{t+1}\\\\\n",
    "...\\\\\n",
    "x_{t+\\omega(n)} & y_{t+\\omega(n)} & z_{t+\\omega(n)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Building Inputs $\\text{size} = ((\\sum_{n=0}^{N}\\omega(n) - m - 1) \\times (f*m))$:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{0,t} & y_{0,t} & e_{1,0,t} & e_{2,0,t} & e_{3,0,t} & e_{4,0,t} & e_{5,0,t} & e_{6,0,t} & z_{0,t} & x_{0,t+1} & ... & z_{0,t+m}\\\\\n",
    "x_{0,t+1} & y_{0,t+1} & e_{1,0,t+1} & e_{2,0,t+1} & e_{3,0,t+1} & e_{4,0,t+1} & e_{5,0,t+1} & e_{6,0,t+1} & z_{0,t+1} & x_{0,t+2} & ... & z_{0,t+m+1}\\\\\n",
    "...\\\\\n",
    "x_{0,\\omega(0)-m-1} & y_{0,\\omega(0)-m-1} & e_{1,0,\\omega(0)-m-1} & e_{2,0,\\omega(0)-m-1} & e_{3,0,\\omega(0)-m-1} & e_{4,0,\\omega(0)-m-1} & e_{5,0,\\omega(0)-m-1} & e_{6,0,\\omega(0)-m-1} & z_{0,\\omega(0)-m-1} & x_{0,\\omega(0)-m} & ... & z_{0,\\omega(0)-1}\\\\\n",
    "...\\\\\n",
    "x_{N,t} & y_{N,t} & e_{1,N,t} & e_{2,N,t} & e_{3,N,t} & e_{4,N,t} & e_{5,N,t} & e_{6,N,t} & z_{N,t} & x_{N,t+1} & ... & z_{N,t+m}\\\\\n",
    "x_{N,t+1} & y_{N,t+1} & e_{1,N,t+1} & e_{2,N,t+1} & e_{3,N,t+1} & e_{4,N,t+1} & e_{5,N,t+1} & e_{6,N,t+1} & z_{N,t+1} & x_{N,t+2} & ... & z_{N,t+m+1}\\\\\n",
    "...\\\\\n",
    "x_{N,\\omega(N)-m-1} & y_{N,\\omega(N)-m-1} & e_{1,N,\\omega(N)-m-1} & e_{2,N,\\omega(N)-m-1} & e_{3,N,\\omega(N)-m-1} & e_{4,N,\\omega(N)-m-1} & e_{5,N,\\omega(N)-m-1} & e_{6,N,\\omega(N)-m-1} & z_{N,\\omega(N)-m-1} & x_{N,\\omega(N)-m} & ... & z_{N,\\omega(N)-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Building Labels $\\text{size} = ((\\sum_{n=0}^{N}\\omega(n) - m - 1) \\times 3)$:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{0,t+m+1} & y_{0,t+m+1} & z_{0,t+m+1}\\\\\n",
    "x_{0,t+m+2} & y_{0,t+m+2} & z_{0,t+m+2}\\\\\n",
    "...\\\\\n",
    "x_{0,\\omega(0)} & y_{0,\\omega(0)} & z_{0,\\omega(0)}\\\\\n",
    "...\\\\\n",
    "x_{N,t+m+1} & y_{N,t+m+1} & z_{N,t+m+1}\\\\\n",
    "x_{N,t+m+2} & y_{N,t+m+2} & z_{N,t+m+2}\\\\\n",
    "...\\\\\n",
    "x_{N,\\omega(N)} & y_{N,\\omega(N)} & z_{N,\\omega(N)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filteredLFP():\n",
    "    lfp_input_file = 'data/raw_data/LFP_filt.txt'\n",
    "    lfp_labels_file = 'data/raw_data/LFP_filt.txt'\n",
    "    fir_file = 'data/raw_data/FR_PN_ITN.txt'\n",
    "    aff_file = 'data/raw_data/AFF_PN_ITN.txt'\n",
    "    with open(lfp_input_file) as f:\n",
    "        lfp_in = f.read().splitlines()\n",
    "    lfp_in = np.array([float(x) for x in lfp_in]).reshape((-1, 1))\n",
    "    \n",
    "    with open(lfp_labels_file) as f:\n",
    "        lfp_out = f.read().splitlines()\n",
    "    lfp_out = np.array([float(x) for x in lfp_out]).reshape((-1, 1))\n",
    "#     print(lfp_out)\n",
    "        \n",
    "#     with open(fir_file) as f:\n",
    "#         fr = f.read().splitlines()\n",
    "#     fr = np.array([(float(x.split(',')[0]), float(x.split(',')[1])) for x in fr])\n",
    "        \n",
    "    with open(aff_file) as f:\n",
    "        aff = f.read().splitlines()\n",
    "    aff = np.array([(float(x.split(',')[0]), float(x.split(',')[1])) for x in aff])\n",
    "        \n",
    "#     full_data = np.hstack((lfp_in, aff))\n",
    "    full_data = lfp_in\n",
    "    full_labels = lfp_out\n",
    "    \n",
    "    training_samples = 900000\n",
    "    indices = rng.integers(low=0, high=full_labels.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD), size=training_samples)\n",
    "    validation_samples = 100000\n",
    "    v_indices = rng.integers(low=0, high=full_labels.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD), size=validation_samples)\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    validation_data = []\n",
    "    validation_labels = []\n",
    "    f_data = []\n",
    "    f_labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        training_data.append(full_data[idx:idx+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        training_labels.append(full_labels[idx+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    training_data = np.stack(training_data, axis=0)\n",
    "    training_labels = np.stack(training_labels, axis=0)\n",
    "    \n",
    "    for idx in v_indices:\n",
    "        validation_data.append(full_data[idx:idx+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        validation_labels.append(full_labels[idx+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    validation_data = np.stack(validation_data, axis=0)\n",
    "    validation_labels = np.stack(validation_labels, axis=0)\n",
    "    \n",
    "    for i in range(full_data.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD)):\n",
    "        f_data.append(full_data[i:i+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        f_labels.append(full_labels[i+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    f_data = np.stack(f_data, axis=0)\n",
    "    f_labels = np.stack(f_labels, axis=0)\n",
    "    \n",
    "    print('Training Data: {}'.format(training_data.shape))\n",
    "    print('Training Labels: {}'.format(training_labels.shape))\n",
    "    print('Validation Data: {}'.format(validation_data.shape))\n",
    "    print('Validation Labels: {}'.format(validation_labels.shape))\n",
    "    print('Full Data: {}'.format(f_data.shape))\n",
    "    print('Full Labels: {}'.format(f_labels.shape))\n",
    "    \n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "    f_dataset = TensorDataset(torch.Tensor(f_data), torch.Tensor(f_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset, f_dataset\n",
    "# get_filteredLFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawLFP():\n",
    "    lfp_input_file = 'data/raw_data/LFP_elec_combine.txt'\n",
    "    lfp_labels_file = 'data/raw_data/LFP_filt.txt'\n",
    "    fir_file = 'data/raw_data/FR_PN_ITN.txt'\n",
    "    aff_file = 'data/raw_data/AFF_PN_ITN.txt'\n",
    "    with open(lfp_input_file) as f:\n",
    "        lfp_in = f.read().splitlines()\n",
    "    lfp_in = np.array([float(x) for x in lfp_in]).reshape((-1, 1))\n",
    "    \n",
    "    with open(lfp_labels_file) as f:\n",
    "        lfp_out = f.read().splitlines()\n",
    "    lfp_out = np.array([float(x) for x in lfp_out]).reshape((-1, 1))\n",
    "#     print(lfp_out)\n",
    "        \n",
    "    with open(fir_file) as f:\n",
    "        fr = f.read().splitlines()\n",
    "#     fr = np.array([(float(x.split(',')[0]), float(x.split(',')[1])) for x in fr])\n",
    "    fr = np.array([float(x.split(',')[0]) for x in fr]).reshape((-1,1))\n",
    "        \n",
    "    with open(aff_file) as f:\n",
    "        aff = f.read().splitlines()\n",
    "    aff = np.array([(float(x.split(',')[0]), float(x.split(',')[1])) for x in aff])\n",
    "        \n",
    "#     full_data = np.hstack((lfp_in, fr))\n",
    "#     print(full_data.shape)\n",
    "    full_data = lfp_in\n",
    "    full_labels = lfp_out\n",
    "    \n",
    "    training_samples = 900000\n",
    "    indices = rng.integers(low=0, high=full_labels.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD), size=training_samples)\n",
    "    validation_samples = 100000\n",
    "    v_indices = rng.integers(low=0, high=full_labels.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD), size=validation_samples)\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    validation_data = []\n",
    "    validation_labels = []\n",
    "    f_data = []\n",
    "    f_labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        training_data.append(full_data[idx:idx+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        training_labels.append(full_labels[idx+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    training_data = np.stack(training_data, axis=0)\n",
    "    training_labels = np.stack(training_labels, axis=0)\n",
    "    \n",
    "    for idx in v_indices:\n",
    "        validation_data.append(full_data[idx:idx+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        validation_labels.append(full_labels[idx+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    validation_data = np.stack(validation_data, axis=0)\n",
    "    validation_labels = np.stack(validation_labels, axis=0)\n",
    "    \n",
    "    for i in range(full_data.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD)):\n",
    "        f_data.append(full_data[i:i+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        f_labels.append(full_labels[i+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    f_data = np.stack(f_data, axis=0)\n",
    "    f_labels = np.stack(f_labels, axis=0)\n",
    "    \n",
    "    print('Training Data: {}'.format(training_data.shape))\n",
    "    print('Training Labels: {}'.format(training_labels.shape))\n",
    "    print('Validation Data: {}'.format(validation_data.shape))\n",
    "    print('Validation Labels: {}'.format(validation_labels.shape))\n",
    "    print('Full Data: {}'.format(f_data.shape))\n",
    "    print('Full Labels: {}'.format(f_labels.shape))\n",
    "    \n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "    f_dataset = TensorDataset(torch.Tensor(f_data), torch.Tensor(f_labels))\n",
    "\n",
    "    return training_dataset, validation_dataset, f_dataset\n",
    "# get_rawLFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WN(time_s=300000, channels=1):\n",
    "    lfp_input_file = 'data/raw_data/LFP_elec_combine.txt'\n",
    "    with open(lfp_input_file) as f:\n",
    "        lfp_in = f.read().splitlines()\n",
    "    lfp_in = np.array([float(x) for x in lfp_in]).reshape((-1, 1))\n",
    "    \n",
    "#     print(np.std(lfp_in))\n",
    "    \n",
    "    noise = rng.normal(0, np.std(lfp_in), (time_s, channels))\n",
    "    \n",
    "    oscBand = np.array([0.08,0.14])\n",
    "    b, a = signal.butter(4,oscBand,btype='bandpass')\n",
    "    noise_filt = signal.lfilter(b, a, noise, axis=0)\n",
    "    \n",
    "#     print(noise_filt)\n",
    "    \n",
    "    full_data = noise\n",
    "    full_labels = noise_filt\n",
    "    f_data = []\n",
    "    f_labels = []\n",
    "    for i in range(full_data.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD)):\n",
    "        f_data.append(full_data[i:i+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        f_labels.append(full_labels[i+PREVIOUS_TIME+LOOK_AHEAD,0].reshape((-1,OUTPUT_SIZE)))\n",
    "    f_data = np.stack(f_data, axis=0)\n",
    "    f_labels = np.stack(f_labels, axis=0)\n",
    "    print('Noise Data: {}'.format(f_data.shape))\n",
    "    print('Noise Labels: {}'.format(f_labels.shape))\n",
    "    noise_dataset = TensorDataset(torch.Tensor(f_data), torch.Tensor(f_labels))\n",
    "    return noise_dataset\n",
    "# get_WN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_sin(time_s=3000, channels=1):\n",
    "    A = .06 #Randomly chosen to be close to LFP magnitude\n",
    "    data = []\n",
    "    t_li = np.arange(0,time_s,0.001)\n",
    "    for t in t_li:\n",
    "        data.append(A*np.sin(t*(314.1516))) #A*np.sin((50*2*np.pi*t))\n",
    "    data = np.array(data).reshape((-1,1))\n",
    "    \n",
    "#     oscBand = np.array([0.08,0.14])\n",
    "#     b, a = signal.butter(4,oscBand,btype='bandpass')\n",
    "#     data_filt = signal.lfilter(b, a, data, axis=0)\n",
    "    \n",
    "    full_data = data\n",
    "    full_labels = data\n",
    "    f_data = []\n",
    "    f_labels = []\n",
    "#     print(full_data)\n",
    "    for i in range(full_data.shape[0]-(PREVIOUS_TIME+LOOK_AHEAD)):\n",
    "        f_data.append(full_data[i:i+PREVIOUS_TIME,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES)))\n",
    "        f_labels.append(full_labels[i+PREVIOUS_TIME+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "    f_data = np.stack(f_data, axis=0)\n",
    "    f_labels = np.stack(f_labels, axis=0)\n",
    "    print('Sin Data: {}'.format(f_data.shape))\n",
    "    print('Sin Labels: {}'.format(f_labels.shape))\n",
    "    sin_dataset = TensorDataset(torch.Tensor(f_data), torch.Tensor(f_labels))\n",
    "    return sin_dataset\n",
    "# get_sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RAW_FILE = 'data/raw_data/LFP_elec_combine.txt'\n",
    "# DEFAULT_RAW_FILE = 'data/raw_data/LFP_filt.txt'\n",
    "DEFAULT_LABELS_FILE = 'data/raw_data/LFP_filt.txt'\n",
    "\n",
    "from scipy import stats, signal\n",
    "\n",
    "def get_burstLFP(in_file=DEFAULT_RAW_FILE, out_file=DEFAULT_LABELS_FILE):\n",
    "    \n",
    "    fir_file = fir_file = 'data/raw_data/FR_PN_ITN.txt'\n",
    "    \n",
    "    with open(in_file) as f:\n",
    "        lfp_in = f.read().splitlines()\n",
    "    lfp_in = np.array([float(x) for x in lfp_in]).reshape((-1, 1))\n",
    "    \n",
    "    with open(out_file) as f:\n",
    "        lfp_out = f.read().splitlines()\n",
    "    lfp_out = np.array([float(x) for x in lfp_out]).reshape((-1, 1))\n",
    "    \n",
    "    with open(fir_file) as f:\n",
    "        fr = f.read().splitlines()\n",
    "#     fr = np.array([(float(x.split(',')[0]), float(x.split(',')[1])) for x in fr])\n",
    "    fr = np.array([float(x.split(',')[0]) for x in fr]).reshape((-1,1))\n",
    "    \n",
    "    hilb = np.abs(signal.hilbert(lfp_in)) #hilbert transform of raw data\n",
    "\n",
    "    z_score = stats.zscore(hilb)\n",
    "    thresh = np.mean(np.squeeze(hilb)) + 2*np.std(np.squeeze(hilb)) # 2*z_score\n",
    "#     print(np.mean(np.squeeze(hilb)))\n",
    "#     print(2*np.std(np.squeeze(hilb)))\n",
    "#     print(thresh)\n",
    "\n",
    "#     print(hilb.shape)\n",
    "    indices = np.nonzero(np.squeeze(hilb)>thresh)[0]\n",
    "#     print(indices.shape)\n",
    "\n",
    "    idx_count = PREVIOUS_TIME + LOOK_AHEAD + 1\n",
    "\n",
    "    burst_indices = []\n",
    "    temp_idx = []\n",
    "    #start at the second index and compare to first one\n",
    "    for i, idx in enumerate(indices[1:], 0):\n",
    "        #if the index is not next start a new sample\n",
    "        if idx - indices[i] != 1 and temp_idx:\n",
    "            if len(temp_idx) < idx_count:\n",
    "                padding = idx_count - len(temp_idx)\n",
    "                pad = np.arange(temp_idx[0]-padding+1,temp_idx[0],1)\n",
    "                temp_idx[:0] = pad\n",
    "                if temp_idx[0] < 0:\n",
    "                    temp_idx = []\n",
    "                    continue\n",
    "#                 print(len(temp_idx))\n",
    "#             print(np.array(temp_idx).reshape((-1,1)).shape)\n",
    "            burst_indices.append(np.array(temp_idx).reshape((-1,1)))\n",
    "            temp_idx = []\n",
    "            temp_idx.append(idx)\n",
    "        #otherwise add the index to the sample\n",
    "        else:\n",
    "            temp_idx.append(idx)\n",
    "\n",
    "    # print(np.squeeze(indices))\n",
    "    burst_in = []\n",
    "    burst_out = []\n",
    "    filt_out = []\n",
    "    \n",
    "    oscBand = np.array([0.08,0.14])\n",
    "    b, a = signal.butter(4,oscBand,btype='bandpass')\n",
    "    \n",
    "    for sample in burst_indices:\n",
    "#         print(sample.shape)\n",
    "        if sample.shape[0] >= 10:\n",
    "            inp = np.take(lfp_in, np.squeeze(sample[:PREVIOUS_TIME])).reshape((-1,1))\n",
    "            fr_i = np.take(fr, np.squeeze(sample[:PREVIOUS_TIME])).reshape((-1,1))\n",
    "            inp = np.concatenate((inp, fr_i), axis=1)\n",
    "#             print(inp.shape)\n",
    "            filt = np.take(lfp_out, np.squeeze(sample[-1])).reshape((-1,1))\n",
    "            lab = np.take(lfp_in, np.squeeze(sample[PREVIOUS_TIME:,:])).reshape((-1,1))\n",
    "            lab = signal.lfilter(b, a, lab.reshape((-1,OUTPUT_SIZE)), axis=0)\n",
    "#             print(lab.shape)\n",
    "            burst_in.append(inp)\n",
    "            burst_out.append(lab[-1,:])\n",
    "            filt_out.append(filt)\n",
    "#     print(np.stack(burst_out).reshape((-1, 1, 1)))\n",
    "    burst_in = np.transpose(np.stack(burst_in), (0,2,1))\n",
    "    burst_out = np.transpose(np.stack(burst_out).reshape((-1,1,1)), (0,2,1))\n",
    "    filt_out = np.transpose(np.stack(filt_out), (0,2,1))\n",
    "    print('Burst Data: {}'.format(burst_in.shape))\n",
    "    print('Burst Labels: {}'.format(burst_out.shape))\n",
    "    print('Filter Labels: {}'.format(filt_out.shape))\n",
    "    burst_dataset = TensorDataset(torch.Tensor(burst_in), torch.Tensor(burst_out))\n",
    "    filt_dataset = TensorDataset(torch.Tensor(burst_in), torch.Tensor(filt_out))\n",
    "    return burst_dataset, filt_dataset\n",
    "\n",
    "# get_burstLFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end1D(training_samples=900000, validation_samples=100000):\n",
    "    lfp_input_file = 'data/raw_data/LFP_elec_combine.txt'\n",
    "    lfp_filt_file = 'data/raw_data/LFP_filt.txt'\n",
    "    fir_file = fir_file = 'data/raw_data/FR_PN_ITN.txt'\n",
    "    \n",
    "    with open(lfp_input_file) as f:\n",
    "        lfp_in = f.read().splitlines()\n",
    "    lfp_in = np.array([float(x) for x in lfp_in]).reshape((-1, 1))\n",
    "    \n",
    "    with open(lfp_filt_file) as f:\n",
    "        lfp_filt = f.read().splitlines()\n",
    "    lfp_filt = np.array([float(x) for x in lfp_filt]).reshape((-1, 1))\n",
    "    \n",
    "    with open(fir_file) as f:\n",
    "        fr = f.read().splitlines()\n",
    "#     fr = np.array([(float(x.split(',')[0]), float(x.split(',')[1])) for x in fr])\n",
    "    fr = np.array([float(x.split(',')[0]) for x in fr]).reshape((-1,1))\n",
    "    \n",
    "    oscBand = np.array([0.08,0.14])\n",
    "    b, a = signal.butter(4,oscBand,btype='bandpass')\n",
    "\n",
    "\n",
    "    t_indices = rng.integers(low=PREVIOUS_TIME, high=lfp_in.shape[0]-LOOK_AHEAD, size=training_samples)\n",
    "    v_indices = rng.integers(low=PREVIOUS_TIME, high=lfp_in.shape[0]-LOOK_AHEAD, size=validation_samples)\n",
    "\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    training_filt = []\n",
    "    validation_data = []\n",
    "    validation_labels = []\n",
    "    validation_filt = []\n",
    "    f_data = []\n",
    "    f_labels = []\n",
    "    f_filt = []\n",
    "    \n",
    "    for idx in t_indices:\n",
    "        tlfp = lfp_in[idx-PREVIOUS_TIME:idx,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES))\n",
    "        t_fr = fr[idx-PREVIOUS_TIME:idx,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES))\n",
    "        training_data.append(np.concatenate((tlfp, t_fr), axis=0))\n",
    "        filter_1d = signal.lfilter(b, a, lfp_in[idx:idx+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)), axis=0)\n",
    "        training_filt.append(lfp_filt[idx+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "        training_labels.append(filter_1d[-1,:])\n",
    "    training_data = np.stack(training_data, axis=0)\n",
    "    training_labels = np.stack(training_labels, axis=0)\n",
    "    training_filt = np.stack(training_filt, axis=0).reshape((-1,1))\n",
    "    \n",
    "    for idx in v_indices:\n",
    "        vlfp = lfp_in[idx-PREVIOUS_TIME:idx,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES))\n",
    "        v_fr = fr[idx-PREVIOUS_TIME:idx,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES))\n",
    "        validation_data.append(np.concatenate((vlfp, v_fr), axis=0))\n",
    "        filter_1d = signal.lfilter(b, a, lfp_in[idx:idx+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)), axis=0)\n",
    "        validation_filt.append(lfp_filt[idx+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "        validation_labels.append(filter_1d[-1,:])\n",
    "    validation_data = np.stack(validation_data, axis=0)\n",
    "    validation_labels = np.stack(validation_labels, axis=0)\n",
    "    validation_filt = np.stack(validation_filt, axis=0).reshape((-1,1))\n",
    "    \n",
    "#     for i in range(PREVIOUS_TIME, lfp_in.shape[0]-LOOK_AHEAD, 1):\n",
    "#         flfp = lfp_in[i-PREVIOUS_TIME:i,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES))\n",
    "#         f_fr = fr[i-PREVIOUS_TIME:i,:].reshape((-1,PREVIOUS_TIME*INPUT_FEATURES))\n",
    "#         f_data.append(np.concatenate((flfp, f_fr), axis=1))\n",
    "#         filter_1d = signal.lfilter(b, a, lfp_in[i:i+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)), axis=0)\n",
    "#         f_filt.append(lfp_filt[i+LOOK_AHEAD,:].reshape((-1,OUTPUT_SIZE)))\n",
    "#         f_labels.append(filter_1d[-1,:])\n",
    "#     f_data = np.stack(f_data, axis=0)\n",
    "#     f_labels = np.stack(f_labels, axis=0)\n",
    "#     f_filt = np.stack(f_filt, axis=0).reshape((-1,1))\n",
    "    \n",
    "    print('Training Data: {}'.format(training_data.shape))\n",
    "    print('Training Labels: {}'.format(training_labels.shape))\n",
    "    print('Training Filter: {}'.format(training_filt.shape))\n",
    "    print('Validation Data: {}'.format(validation_data.shape))\n",
    "    print('Validation Labels: {}'.format(validation_labels.shape))\n",
    "    print('Validation Filter: {}'.format(validation_filt.shape))\n",
    "#     print('Full Data: {}'.format(f_data.shape))\n",
    "#     print('Full Labels: {}'.format(f_labels.shape))\n",
    "#     print('Full Filter: {}'.format(f_filt.shape))\n",
    "#     print(f_filt)\n",
    "    \n",
    "    training_dataset = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_labels))\n",
    "    training_filt = TensorDataset(torch.Tensor(training_data), torch.Tensor(training_filt))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_labels))\n",
    "    validation_filt = TensorDataset(torch.Tensor(validation_data), torch.Tensor(validation_filt))\n",
    "#     f_dataset = TensorDataset(torch.Tensor(f_data), torch.Tensor(f_labels))\n",
    "#     f_filt = TensorDataset(torch.Tensor(f_data), torch.Tensor(f_filt))\n",
    "\n",
    "    return training_dataset, validation_dataset, training_filt, validation_filt#f_dataset, training_filt, validation_filt, f_filt\n",
    "# get_end1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,device):\n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "    \n",
    "#     feedback_arr = torch.zeros(BATCH_SIZE, 90)\n",
    "    \n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = 0.95 #decay the lr each step to 98% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of EPOCHS trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                x = x.to(device)\n",
    "                output = model(x)\n",
    "                y = y.to(device)\n",
    "#                 if i%100000 == 0 and epoch%5 == 0:\n",
    "#                     print(output)\n",
    "#                     print(y)\n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y)) \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "#                     if i%100000 == 0 and epoch%5 == 0:\n",
    "#                         print(model.cn1.weight.grad)\n",
    "#                         print(model.cn2.weight.grad)\n",
    "#                         print(model.fc1.weight.grad)\n",
    "#                         print(model.fc2.weight.grad)\n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if train_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = train_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    \n",
    "    loss_df.to_csv(LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (900000, 2, 50)\n",
      "Training Labels: (900000, 1)\n",
      "Training Filter: (900000, 1)\n",
      "Validation Data: (100000, 2, 50)\n",
      "Validation Labels: (100000, 1)\n",
      "Validation Filter: (100000, 1)\n",
      "Noise Data: (299930, 2, 50)\n",
      "Noise Labels: (299930, 1, 1)\n",
      "Burst Data: (55198, 2, 50)\n",
      "Burst Labels: (55198, 1, 1)\n",
      "Filter Labels: (55198, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# f_tr, f_va, f_data = get_filteredLFP()\n",
    "# f_tr, f_va, f_data = get_rawLFP()\n",
    "f_tr, f_va, t_filt, v_filt = get_end1D()#f_data, t_filt, v_filt, f_filt = get_end1D()\n",
    "noise = get_WN(channels=2)\n",
    "# sin = get_sin()\n",
    "burst, fburst = get_burstLFP()\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "train_loader = DataLoader(dataset=f_tr,batch_size=BATCH_SIZE)\n",
    "tfilt_loader = DataLoader(dataset=t_filt,batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=f_va,batch_size=BATCH_SIZE)\n",
    "vfilt_loader = DataLoader(dataset=v_filt,batch_size=BATCH_SIZE)\n",
    "# full_loader = DataLoader(dataset=f_data,batch_size=BATCH_SIZE)\n",
    "# ffull_loader = DataLoader(dataset=f_filt,batch_size=BATCH_SIZE)\n",
    "noise_loader = DataLoader(dataset=noise,batch_size=BATCH_SIZE)\n",
    "# sine_loader = DataLoader(dataset=sin,batch_size=BATCH_SIZE)\n",
    "burst_loader = DataLoader(dataset=burst,batch_size=BATCH_SIZE)\n",
    "fburst_loader = DataLoader(dataset=fburst,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22887862ff2a4efbac5589aff6225135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   196] train loss: 0.104166 val loss: 0.006472\n",
      "[6,   196] train loss: 0.037616 val loss: 0.004166\n",
      "[11,   196] train loss: 0.036788 val loss: 0.004078\n",
      "[16,   196] train loss: 0.036461 val loss: 0.004086\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model1 = MODEL(INPUT_SIZE,HIDDEN_SIZE,OUTPUT_SIZE)\n",
    "model_initial = copy.deepcopy(model1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "\n",
    "pnfr_training_loss, pnfr_validation_loss = train_model(model1,PATH,train_loader,\n",
    "                                                       val_loader,EPOCHS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_eval(model, testing_dataloader, filt=None, k=None):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    temp_list = []\n",
    "    if filt is not None:\n",
    "        filt = iter(filt)\n",
    "    for i, (x, y) in enumerate(testing_dataloader):\n",
    "        output = model(x)         \n",
    "        if filt is None:\n",
    "            output_list.append(output.detach().cpu().numpy())\n",
    "            labels_list.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            xf, yf = next(filt)\n",
    "            yf = yf.detach().cpu().numpy().reshape((-1,1))\n",
    "#             print(yf)\n",
    "            y = y.detach().cpu().numpy().reshape((-1,1))\n",
    "            output = output.detach().cpu().numpy().reshape((-1,1))\n",
    "#             print((yf-y).shape)\n",
    "            pred = yf-y+output\n",
    "#             print(pred.shape)\n",
    "            output_list.append(pred)\n",
    "            labels_list.append(yf)\n",
    "        if k != None and i == k-1:\n",
    "            break\n",
    "#     print(\"Output list size: {}\".format(len(output_list)))\n",
    "#     print(output_list[0].shape)\n",
    "    output_list = np.squeeze(np.concatenate(output_list, axis=0))\n",
    "#     print(output_list.shape)\n",
    "    labels_list = np.squeeze(np.concatenate(labels_list, axis=0))\n",
    "#     print(labels_list.shape)\n",
    "#     print(output_list.shape)\n",
    "#     print(labels_list.shape)\n",
    "    print(r2_score(labels_list, output_list))\n",
    "    return output_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomState(MT19937(SeedSequence(123456789)))### Mask\n",
    "Variables:\n",
    "$$\n",
    "x = \\text{PN Firing Rate}\\\\\n",
    "y = \\text{ITN Firing Rate}\\\\\n",
    "z = \\text{Local Field Potential}\\\\\n",
    "t = \\text{Timestep}\\\\\n",
    "m = \\text{Number of Previous Timesteps}\\\\\n",
    "e_{x} = \\text{External Input}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Output Array:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{n,t+m} & y_{n,t+m} & z_{n,t+m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Feedback Array: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{n,t-1} & y_{n,t-1} & z_{n,t-1} & x_{n,t} & ... & z_{n,t+m-1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Replace index 0, 1, 2 with Output Array and Roll\n",
    "\n",
    "Rolled Feedback Array: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{n,t} & y_{n,t} & z_{n,t} & x_{n,t+1} & ... & z_{n,t+m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Input Array:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{n,t} & y_{n,t} & e_{1,n,t} & e_{2,n,t} & e_{3,n,t} & e_{4,n,t} & e_{5,n,t} & e_{6,n,t} & z_{n,t} & x_{n,t+1} & ... & z_{n,t+m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "New Output:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{n,t+m+1} & y_{n,t+m+1} & z_{n,t+m+1}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772238651890114\n",
      "0.977000965633649\n",
      "0.9804787399051351\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(PATH)\n",
    "model1.eval()\n",
    "\n",
    "start = 40\n",
    "k = 10000\n",
    "end= (start + k) if k != None else None\n",
    "\n",
    "model1.to('cpu')\n",
    "\n",
    "t_pred, t_real = r2_eval(model1, train_loader, filt=tfilt_loader ,k=end)\n",
    "v_pred, v_real = r2_eval(model1, val_loader, filt=vfilt_loader, k=end)\n",
    "# f_pred, f_real = r2_eval(model1, full_loader,filt=ffull_loader, k=end)\n",
    "# print(f_real)\n",
    "# t_pred, t_real = r2_eval(model1, train_loader, filt=None ,k=end)\n",
    "# v_pred, v_real = r2_eval(model1, val_loader, filt=None, k=end)\n",
    "# f_pred, f_real = r2_eval(model1, full_loader,filt=None, k=end)\n",
    "# n_pred, n_real = r2_eval(model1, noise_loader, end)\n",
    "# s_pred, s_real = r2_eval(model1, sine_loader, end)\n",
    "b_pred, b_real = r2_eval(model1, burst_loader, filt=fburst_loader,k=end)\n",
    "# for i in range(len(s_pred)):\n",
    "#     print(\"output: {} label: {}\".format(s_pred[i], s_real[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.000021\n",
      "Val MSE: 0.000021\n",
      "Burst MSE: 0.000018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Train MSE: {:f}\".format(mean_squared_error(t_real, t_pred)))\n",
    "print(\"Val MSE: {:f}\".format(mean_squared_error(v_real, v_pred)))\n",
    "# print(\"Full MSE: {:f}\".format(mean_squared_error(f_real, f_pred)))\n",
    "print(\"Burst MSE: {:f}\".format(mean_squared_error(b_real, b_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b7599a1d474106a53c35a18f1d7d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, -5.47777777777778, 'Epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(nrows=1, ncols=2)\n",
    "fig1.tight_layout()\n",
    "ax1[0].plot(range(EPOCHS), pnfr_training_loss)\n",
    "ax1[0].set_title('Training Loss')\n",
    "ax1[0].set_ylabel('Loss')\n",
    "ax1[0].set_xlabel('Epoch')\n",
    "\n",
    "ax1[1].plot(range(EPOCHS), pnfr_validation_loss)\n",
    "ax1[1].set_title('Validation Loss')\n",
    "ax1[1].set_ylabel('Loss')\n",
    "ax1[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c36321c17d47ada2e16567ecadaab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].plot(np.arange(start-OUTPUT_SIZE,end), v_real[start-OUTPUT_SIZE:end], color='blue',label='Labels')\n",
    "# ax[2,0].plot(np.arange(start-10,end), v_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "ax[0].scatter(np.arange(start-OUTPUT_SIZE,end), v_pred[start-1:end], color='slateblue',label='Training t+10')\n",
    "# ax[0].scatter(np.arange(start-OUTPUT_SIZE,end), v_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[0].scatter(np.arange(start-OUTPUT_SIZE,end), v_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[0].scatter(np.arange(start-OUTPUT_SIZE,end), v_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[0].scatter(np.arange(start-OUTPUT_SIZE,end), v_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[0].plot(np.arange(start-10,end), v_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "\n",
    "ax[0].set_title('Validation LFP')\n",
    "ax[0].set_ylabel('LFP')\n",
    "ax[0].set_xlabel('Time')\n",
    "# ax[2,0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(start-OUTPUT_SIZE,end), t_real[start-OUTPUT_SIZE:end], color='blue',label='Labels')\n",
    "# a[2,1].plot(np.arange(start-10,end), t_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "ax[1].scatter(np.arange(start-OUTPUT_SIZE,end), t_pred[start-1:end], color='slateblue',label='Training t+10')\n",
    "# ax[1].scatter(np.arange(start-OUTPUT_SIZE,end), t_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[1].scatter(np.arange(start-OUTPUT_SIZE,end), t_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[1].scatter(np.arange(start-OUTPUT_SIZE,end), t_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[1].scatter(np.arange(start-OUTPUT_SIZE,end), t_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[1].plot(np.arange(start-10,end), t_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "ax[1].set_title('Training LFP')\n",
    "ax[1].set_ylabel('LFP')\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].legend()\n",
    "\n",
    "# import plotly.tools as tls\n",
    "# plotly_fig = tls.mpl_to_plotly(fig)\n",
    "# plotly_fig.write_html(\"testfile.html\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd592dcd65346d3b36ff2c044c4689b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-73420cd843a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training t+10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Full LFP vs Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_real' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.plot(np.arange(start,end), f_real[start:end], color='blue',label='Labels')\n",
    "ax.scatter(np.arange(start,end), f_pred[start:end], color='red',label='Training t+10')\n",
    "ax.set_title('Full LFP vs Time')\n",
    "ax.set_ylabel('Signal')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# ax[1].plot(np.arange(start,end), n_real[start:end], color='blue',label='Labels')\n",
    "# ax[1].scatter(np.arange(start,end), n_pred[start:end], color='red',label='Training t+10')\n",
    "# ax[1].set_title('Noise')\n",
    "# ax[1].set_ylabel('Signal')\n",
    "# ax[1].set_xlabel('Time')z\n",
    "\n",
    "# ax.plot(np.arange(start,end), s_real[start:end], color='blue',label='Labels')\n",
    "# ax.scatter(np.arange(start,end), s_pred[start:end], color='red',label='Training t+10')\n",
    "# ax.set_title('Sine')\n",
    "# ax.set_ylabel('LFP')\n",
    "# ax.set_xlabel('Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "import copy\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "\n",
    "STEPS = 100\n",
    "# model_initial = MODEL(INPUT_SIZE,HIDDEN_SIZE,OUTPUT_SIZE)\n",
    "model_final = copy.deepcopy(model1)\n",
    "\n",
    "\n",
    "# data that the evaluator will use when evaluating loss\n",
    "x, y = iter(noise_loader).__next__()\n",
    "metric = loss_landscapes.metrics.Loss(nn.MSELoss(), x, y)\n",
    "\n",
    "\n",
    "loss_data_fin = loss_landscapes.random_plane(model_final, metric, 10000, STEPS, normalization='model', deepcopy_model=True)\n",
    "# plt.contour(loss_data_fin, levels=50)\n",
    "# plt.title('Loss Contours around Trained Model')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "X = np.array([[j for j in range(STEPS)] for i in range(STEPS)])\n",
    "Y = np.array([[i for _ in range(STEPS)] for i in range(STEPS)])\n",
    "ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface Plot of Loss Landscape')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
