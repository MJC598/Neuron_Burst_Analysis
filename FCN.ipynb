{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fec10bd3af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCN,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 85\n",
    "FRONT_TIME = -50\n",
    "BACK_TIME = 40\n",
    "T_START = 50+FRONT_TIME\n",
    "T_END = 50+BACK_TIME\n",
    "MODEL = FCN\n",
    "OUTPUT = 'FR_LFP'\n",
    "LOSS_FILE = ('losses/bursts/losses_' + str(MODEL) + \n",
    "             '_' + OUTPUT + '_' + str(FRONT_TIME) + \n",
    "             '_' + str(T_END) + '_10ms_lag_full.csv')\n",
    "PATH = ('models/bursts/' + str(MODEL) + '_' + OUTPUT + \n",
    "        '_' + str(FRONT_TIME) + '_' + str(T_END) + \n",
    "        '_10ms_lag_full.pth')\n",
    "DATA_PATH = 'data/bursts/burst_separatePNITNv2.mat'\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    LOSS_FILE = COLAB_PRE + LOSS_FILE\n",
    "    PATH = COLAB_PRE + PATH\n",
    "    DATA_PATH = COLAB_PRE + DATA_PATH\n",
    "\n",
    "# Specific Model Parameters\n",
    "input_features = 9\n",
    "previous_time = 10\n",
    "input_size = input_features * previous_time\n",
    "hidden_size = 95\n",
    "output_size = 3\n",
    "batch_size = 32\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "dropout = 0.0\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_arr(l, max_dur=0):\n",
    "    temp = []\n",
    "    for ar in l:\n",
    "        npad = ((0,max_dur-ar.shape[0]), (0,0))\n",
    "        arr = np.pad(ar, pad_width=npad, mode='constant', constant_values=-1)\n",
    "        temp.append(arr)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truncated_data_from_mat(file_path, output_index=None, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "    duration = []\n",
    "    amp = []\n",
    "    pb_fr_pn = []\n",
    "    pb_fr_itn = []\n",
    "    pb_aff_pn = []\n",
    "    pb_aff_itn = []\n",
    "    pb_exc_pn = []\n",
    "    pb_inh_pn = []\n",
    "    pb_exc_itn = []\n",
    "    pb_inh_itn = []\n",
    "    pb_lfp = []\n",
    "    wb_fr_pn = []\n",
    "    wb_fr_itn = []\n",
    "    wb_aff_pn = []\n",
    "    wb_aff_itn = []\n",
    "    wb_exc_pn = []\n",
    "    wb_inh_pn = []\n",
    "    wb_exc_itn = []\n",
    "    wb_inh_itn = []\n",
    "    wb_lfp = []\n",
    "\n",
    "#     print(data['info_collect'][0])\n",
    "#     print(data['info_collect'].shape[0])\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        arr = data['info_collect'][i]\n",
    "        \n",
    "#         print(arr.shape)\n",
    "        \n",
    "        duration.append(arr[0])\n",
    "        amp.append(arr[1])\n",
    "        \n",
    "        pb_fr_pn.append(arr[2])\n",
    "        pb_fr_itn.append(arr[3])\n",
    "        pb_aff_pn.append(arr[4])\n",
    "        pb_aff_itn.append(arr[5])\n",
    "        pb_exc_pn.append(arr[6])\n",
    "        pb_inh_pn.append(arr[7])\n",
    "        pb_exc_itn.append(arr[8])\n",
    "        pb_inh_itn.append(arr[9])\n",
    "        pb_lfp.append(arr[10])\n",
    "        \n",
    "        wb_fr_pn.append(arr[11])\n",
    "        wb_fr_itn.append(arr[12])\n",
    "        wb_aff_pn.append(arr[13])\n",
    "        wb_aff_itn.append(arr[14])\n",
    "        wb_exc_pn.append(arr[15])\n",
    "        wb_inh_pn.append(arr[16])\n",
    "        wb_exc_itn.append(arr[17])\n",
    "        wb_inh_itn.append(arr[18])\n",
    "        wb_lfp.append(arr[19])\n",
    "\n",
    "    min_dur = 1000\n",
    "    max_dur = 0\n",
    "    for i, ar in enumerate(wb_fr_pn):\n",
    "        if ar.shape[0] < min_dur:\n",
    "            min_dur = ar.shape[0]\n",
    "            mi = i\n",
    "        if ar.shape[0] > max_dur:\n",
    "            max_dur = ar.shape[0]\n",
    "            ma = i\n",
    "            \n",
    "#     print(ma)\n",
    "#     print(max_dur)\n",
    "    \n",
    "#     wb_fr_pn = pad_arr(wb_fr_pn, max_dur)\n",
    "    \n",
    "    wb_fr_pn = [ar[:min_dur,:] for ar in wb_fr_pn]\n",
    "\n",
    "#     wb_fr_itn = pad_arr(wb_fr_itn, max_dur)\n",
    "    \n",
    "    wb_fr_itn = [ar[:min_dur,:] for ar in wb_fr_itn]\n",
    "    \n",
    "#     wb_aff_pn = pad_arr(wb_aff_pn, max_dur)\n",
    "    \n",
    "    wb_aff_pn = [ar[:min_dur,:] for ar in wb_aff_pn]\n",
    "    \n",
    "#     wb_aff_itn = pad_arr(wb_aff_itn, max_dur)\n",
    "    \n",
    "    wb_aff_itn = [ar[:min_dur,:] for ar in wb_aff_itn]\n",
    "    \n",
    "#     wb_exc_pn = pad_arr(wb_exc_pn, max_dur)\n",
    "    \n",
    "    wb_exc_pn = [ar[:min_dur,:] for ar in wb_exc_pn]\n",
    "    \n",
    "#     wb_inh_pn = pad_arr(wb_inh_pn, max_dur)\n",
    "    \n",
    "    wb_inh_pn = [ar[:min_dur,:] for ar in wb_inh_pn]\n",
    "    \n",
    "#     wb_exc_itn = pad_arr(wb_exc_itn, max_dur)\n",
    "    \n",
    "    wb_exc_itn = [ar[:min_dur,:] for ar in wb_exc_itn]\n",
    "    \n",
    "#     wb_inh_itn = pad_arr(wb_inh_itn, max_dur)\n",
    "    \n",
    "    wb_inh_itn = [ar[:min_dur,:] for ar in wb_inh_itn]\n",
    "\n",
    "#     wb_lfp = pad_arr(wb_lfp, max_dur)\n",
    "        \n",
    "    wb_lfp = [ar[:min_dur,:] for ar in wb_lfp]\n",
    "    \n",
    "    t1 = np.concatenate((pb_fr_pn, wb_fr_pn), axis=1)\n",
    "    t2 = np.concatenate((pb_fr_itn, wb_fr_itn), axis=1)\n",
    "    t3 = np.concatenate((pb_aff_pn, wb_aff_pn), axis=1)\n",
    "    t4 = np.concatenate((pb_aff_itn, wb_aff_itn), axis=1)\n",
    "    t5 = np.concatenate((pb_exc_pn, wb_exc_pn), axis=1)\n",
    "    t6 = np.concatenate((pb_inh_pn, wb_inh_pn), axis=1)\n",
    "    t7 = np.concatenate((pb_exc_itn, wb_exc_itn), axis=1)\n",
    "    t8 = np.concatenate((pb_inh_itn, wb_inh_itn), axis=1)\n",
    "    t9 = np.concatenate((pb_lfp, wb_lfp), axis=1)\n",
    "    \n",
    "#     full_labels = np.concatenate((pb_fr_pn, pb_fr_itn, pb_lfp), axis=2)\n",
    "    \n",
    "#     front_data = np.concatenate((pb_fr_pn, pb_fr_itn, pb_aff_pn, pb_aff_itn,pb_exc_pn, \n",
    "#                                  pb_inh_pn, pb_exc_itn, pb_inh_itn, pb_lfp), axis=2)\n",
    "    \n",
    "#     rear_data = np.concatenate((wb_fr_pn, wb_fr_itn, wb_aff_pn, wb_aff_itn,\n",
    "#                                 wb_exc_pn, wb_inh_pn, wb_exc_itn, wb_inh_itn, wb_lfp), axis=2)\n",
    "    \n",
    "#     full_data = rear_data\n",
    "\n",
    "    full_labels = np.concatenate((t1,t2,t9),axis=2)\n",
    "    full_data = np.concatenate((t1,t2,t3,t4,t5,t6,t7,t8,t9), axis=2)\n",
    "    print(full_data.shape)\n",
    "    \n",
    "#     for j in range(3):\n",
    "#         x = full_labels[:,:,j]\n",
    "#         full_labels[:,:,j] = (x - np.min(x))/(np.max(x)-np.min(x))\n",
    "    \n",
    "#     for i in range(full_data.shape[0]):\n",
    "#         for j in range(input_size):\n",
    "#             x = full_data[i,:,j]\n",
    "#             full_data[i,:,j] = (x - np.min(x))/(np.max(x)-np.min(x))\n",
    "    \n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 5472 #5498 \n",
    "    k = 4352\n",
    "    full = np.arange(data_samples)\n",
    "    training_indices = np.random.choice(full, size=k, replace=False)\n",
    "    validation_indices = np.delete(full,training_indices)\n",
    "    \n",
    "    max_index = min_dur+50\n",
    "    lag = 1\n",
    "    front_offset = 11\n",
    "    \n",
    "    training_data = full_data[:k,0:max_index-lag,:] \n",
    "    validation_data = full_data[k:data_samples,0:max_index-lag,:]\n",
    "    \n",
    "    if output_index is None:\n",
    "        training_labels = full_labels[:k,front_offset:max_index,:] \n",
    "    else:\n",
    "        training_labels = full_labels[:k,front_offset:max_index,output_index]\n",
    "    \n",
    "    if output_index is None:\n",
    "        validation_labels = full_labels[k:data_samples,front_offset:max_index,:]\n",
    "    else:\n",
    "        validation_labels = full_labels[k:data_samples,front_offset:max_index,output_index]\n",
    "    \n",
    "    training_data = np.transpose(training_data, (0,2,1))\n",
    "    training_labels = np.transpose(training_labels, (0,2,1))\n",
    "    validation_data = np.transpose(validation_data, (0,2,1))\n",
    "    validation_labels = np.transpose(validation_labels, (0,2,1))\n",
    "    \n",
    "    td = [] \n",
    "    tl = []\n",
    "    vd = []\n",
    "    vl = []\n",
    "    for i in range(training_data.shape[0]):\n",
    "        for j in range(training_data.shape[2]-10):\n",
    "            t = training_data[i,:,j:j+10]\n",
    "            td.append(t.reshape((1,-1)))\n",
    "            t2 = training_labels[i,:,j]\n",
    "            tl.append(t2.reshape((1,-1)))\n",
    "    td = np.vstack(td)\n",
    "    tl = np.vstack(tl)\n",
    "    print(td.shape)\n",
    "    print(tl.shape)\n",
    "    \n",
    "    \n",
    "    for i in range(validation_data.shape[0]):\n",
    "        for j in range(validation_data.shape[2]-10):\n",
    "            t = training_data[i,:,j:j+10]\n",
    "            vd.append(t.reshape((1,-1)))\n",
    "            t2 = training_labels[i,:,j]\n",
    "            vl.append(t2.reshape((1,-1)))\n",
    "    vd = np.vstack(vd)\n",
    "    print(vd.shape)\n",
    "    vl = np.vstack(vl)\n",
    "    print(vl.shape)\n",
    "    \n",
    "#     print(training_data.shape)\n",
    "#     print(training_labels.shape)\n",
    "#     print(validation_data.shape)\n",
    "#     print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(td), torch.Tensor(tl))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(vd), torch.Tensor(vl))\n",
    "\n",
    "    return training_dataset, validation_dataset\n",
    "# get_data_from_mat(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_mat(file_path, output_index=None, type='pre_pn'):\n",
    "    data = scipy.io.loadmat(file_path)\n",
    "\n",
    "    full_labels = []\n",
    "    full_data = []\n",
    "\n",
    "    for i in range(1, data['info_collect'].shape[0]):\n",
    "        arr = data['info_collect'][i]\n",
    "        \n",
    "#         duration.append(arr[0])\n",
    "#         amp.append(arr[1])\n",
    "        \n",
    "#         fr_pn.append(np.row_stack((arr[2], arr[11])))\n",
    "#         fr_itn.append(np.row_stack((arr[3], arr[12])))\n",
    "#         aff_pn.append(np.row_stack((arr[4], arr[13])))\n",
    "#         aff_itn.append(np.row_stack((arr[5], arr[14])))\n",
    "#         exc_pn.append(np.row_stack((arr[6], arr[15])))\n",
    "#         inh_pn.append(np.row_stack((arr[7], arr[16])))\n",
    "#         exc_itn.append(np.row_stack((arr[8], arr[17])))\n",
    "#         inh_itn.append(np.row_stack((arr[9], arr[18])))\n",
    "#         lfp.append(np.row_stack((arr[10], arr[19])))\n",
    "        \n",
    "        full_labels.append(np.column_stack((np.row_stack((arr[2], arr[11])), \n",
    "                                            np.row_stack((arr[3], arr[12])), \n",
    "                                            np.row_stack((arr[10], arr[19])))))\n",
    "        \n",
    "        full_data.append(np.column_stack((np.row_stack((arr[2], arr[11])), \n",
    "                                          np.row_stack((arr[3], arr[12])), \n",
    "                                          np.row_stack((arr[4], arr[13])), \n",
    "                                          np.row_stack((arr[5], arr[14])), \n",
    "                                          np.row_stack((arr[6], arr[15])), \n",
    "                                          np.row_stack((arr[7], arr[16])), \n",
    "                                          np.row_stack((arr[8], arr[17])), \n",
    "                                          np.row_stack((arr[9], arr[18])), \n",
    "                                          np.row_stack((arr[10], arr[19])))))\n",
    "    \n",
    "    full_data = np.asarray(full_data)\n",
    "    full_labels = np.asarray(full_labels)\n",
    "    \n",
    "    random.seed(10)\n",
    "    data_samples = 5472 #5498 \n",
    "    k = 4352\n",
    "    \n",
    "    lag = 1\n",
    "    front_offset = 11\n",
    "    \n",
    "    training_data = full_data[:k] \n",
    "    validation_data = full_data[k:data_samples]\n",
    "    training_labels = full_labels[:k]\n",
    "    validation_labels = full_data[k:data_samples]\n",
    "    \n",
    "#     if output_index is None:\n",
    "#         training_labels = full_labels[:k,front_offset:max_index,:] \n",
    "#     else:\n",
    "#         training_labels = full_labels[:k,front_offset:max_index,output_index]\n",
    "    \n",
    "#     if output_index is None:\n",
    "#         validation_labels = full_labels[k:data_samples,front_offset:max_index,:]\n",
    "#     else:\n",
    "#         validation_labels = full_labels[k:data_samples,front_offset:max_index,output_index]\n",
    "    \n",
    "    td = [] \n",
    "    tl = []\n",
    "    vd = []\n",
    "    vl = []\n",
    "    for i, sample in enumerate(training_data):\n",
    "#         print(sample.shape)\n",
    "        label = training_labels[i]\n",
    "#         print(label.shape)\n",
    "        for j in range(sample.shape[0]-10):\n",
    "            t = sample[j:j+10,:]\n",
    "#             print(t[-1])\n",
    "#             print(t.shape)\n",
    "            td.append(t.reshape((1,-1)))\n",
    "            t2 = label[j+10,:]\n",
    "#             print(t2)\n",
    "            tl.append(t2.reshape((1,-1)))\n",
    "    td = np.vstack(td)\n",
    "    tl = np.vstack(tl)\n",
    "    print(td.shape)\n",
    "    print(tl.shape)\n",
    "    \n",
    "    \n",
    "    for i, sample in enumerate(validation_data):\n",
    "        label = validation_labels[i]\n",
    "        for j in range(sample.shape[0]-10):\n",
    "            t = sample[j:j+10,:]\n",
    "            vd.append(t.reshape((1,-1)))\n",
    "            t2 = label[j+10,:]\n",
    "            vl.append(t2.reshape((1,-1)))\n",
    "    vd = np.vstack(vd)\n",
    "    print(vd.shape)\n",
    "    vl = np.vstack(vl)\n",
    "    print(vl.shape)\n",
    "    \n",
    "#     print(training_data.shape)\n",
    "#     print(training_labels.shape)\n",
    "#     print(validation_data.shape)\n",
    "#     print(validation_labels.shape)\n",
    "\n",
    "    training_dataset = TensorDataset(torch.Tensor(td), torch.Tensor(tl))\n",
    "    validation_dataset = TensorDataset(torch.Tensor(vd), torch.Tensor(vl))\n",
    "\n",
    "    return training_dataset, validation_dataset\n",
    "# get_data_from_mat(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs):\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = 0.93 #decay the lr each step to 93% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                output = model(x) \n",
    "#                 print(output.size())\n",
    "#                 print(y.size())\n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y)) \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                                      \n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    lf = ('losses/losses_' + str(MODEL) + '_' \n",
    "          + OUTPUT + '_' + str(FRONT_TIME) + '_' \n",
    "          + str(T_END) + '_fullin.csv')\n",
    "    loss_df.to_csv(lf, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_eval(model, testing_dataloader):\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    temp_list = []\n",
    "    for i, (x, y) in enumerate(testing_dataloader):\n",
    "        output = model(x)         \n",
    "        output_list.append(output.detach().cpu().numpy())\n",
    "        labels_list.append(y.detach().cpu().numpy())\n",
    "#     print(\"Output list size: {}\".format(len(output_list)))\n",
    "#     print(output_list[0].shape)\n",
    "    output_list = np.concatenate(output_list, axis=0)\n",
    "    labels_list = np.concatenate(labels_list, axis=0)\n",
    "#     print(output_list.shape)\n",
    "#     print(labels_list.shape)\n",
    "    print(r2_score(labels_list, output_list))\n",
    "    return output_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/venvs/neuro/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757509, 90)\n",
      "(757509, 3)\n",
      "(197107, 90)\n",
      "(197107, 9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fa74513cc14ad397cad16ceb4505d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/venvs/neuro/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f9e2738a0d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mOUTPUT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRONT_TIME\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_END\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_full.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m pnfr_training_loss, pnfr_validation_loss = train_model(model1,p1,training_loader,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                                        validation_loader,epochs)\n",
      "\u001b[0;32m<ipython-input-8-494b1d617cab>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, save_filepath, training_loader, validation_loader, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#                 print(output.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/neuro/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1 = MODEL(input_size,hidden_size,output_size)\n",
    "\n",
    "training_dataset, validation_dataset = get_data_from_mat(DATA_PATH)\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "training_loader = DataLoader(dataset=training_dataset,batch_size=batch_size,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_dataset,batch_size=batch_size)\n",
    "\n",
    "p1 = 'models/' + str(MODEL) + '_' + OUTPUT + '_' + str(FRONT_TIME) + '_' + str(T_END) + '_full.pth'\n",
    "pnfr_training_loss, pnfr_validation_loss = train_model(model1,p1,training_loader,\n",
    "                                                       validation_loader,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load(p1)\n",
    "model1.eval()\n",
    "t_output_list, t_labels_list = r2_score_eval(model1, training_loader)\n",
    "v_output_list, v_labels_list = r2_score_eval(model1, validation_loader)\n",
    "print(t_output_list.shape)\n",
    "print(t_labels_list.shape)\n",
    "# print(t_labels_list[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=2)\n",
    "fig.tight_layout()\n",
    "ax[0,0].plot(range(epochs), pnfr_training_loss)\n",
    "ax[0,0].set_title('Validation Loss')\n",
    "ax[0,0].set_ylabel('Loss')\n",
    "ax[0,0].set_xlabel('Epoch')\n",
    "\n",
    "ax[0,1].plot(range(epochs), pnfr_validation_loss)\n",
    "ax[0,1].set_title('Training Loss')\n",
    "ax[0,1].set_ylabel('Loss')\n",
    "ax[0,1].set_xlabel('Epoch')\n",
    "\n",
    "\n",
    "ax[1,0].plot(np.arange(v_labels_list.shape[0]), v_labels_list[:,0], color='blue')\n",
    "ax[1,0].plot(np.arange(v_labels_list.shape[0]), v_output_list[:,0], color='red')\n",
    "ax[1,0].set_title('Validation PN FR')\n",
    "ax[1,0].set_ylabel('PN FR')\n",
    "ax[1,0].set_xlabel('Time and Sample')\n",
    "\n",
    "ax[1,1].plot(np.arange(t_labels_list.shape[0]), t_labels_list[:,0], color='blue')\n",
    "ax[1,1].plot(np.arange(t_labels_list.shape[0]), t_output_list[:,0], color='red')\n",
    "ax[1,1].set_title('Training PN FR')\n",
    "ax[1,1].set_ylabel('PN FR')\n",
    "ax[1,1].set_xlabel('Time')\n",
    "\n",
    "ax[2,0].plot(np.arange(v_labels_list.shape[0]), v_labels_list[:,1], color='blue')\n",
    "ax[2,0].plot(np.arange(v_labels_list.shape[0]), v_output_list[:,1], color='red')\n",
    "ax[2,0].set_title('Validation ITN FR')\n",
    "ax[2,0].set_ylabel('ITN FR')\n",
    "ax[2,0].set_xlabel('Time')\n",
    "\n",
    "ax[2,1].plot(np.arange(t_labels_list.shape[0]), t_labels_list[:,1], color='blue')\n",
    "ax[2,1].plot(np.arange(t_labels_list.shape[0]), t_output_list[:,1], color='red')\n",
    "ax[2,1].set_title('Training ITN FR')\n",
    "ax[2,1].set_ylabel('ITN FR')\n",
    "ax[2,1].set_xlabel('Time')\n",
    "\n",
    "ax[3,0].plot(np.arange(v_labels_list.shape[0]), v_labels_list[:,2], color='blue')\n",
    "ax[3,0].plot(np.arange(v_labels_list.shape[0]), v_output_list[:,2], color='red')\n",
    "ax[3,0].set_title('Validation LFP')\n",
    "ax[3,0].set_ylabel('LFP')\n",
    "ax[3,0].set_xlabel('Time')\n",
    "\n",
    "ax[3,1].plot(np.arange(t_labels_list.shape[0]), t_labels_list[:,2], color='blue')\n",
    "ax[3,1].plot(np.arange(t_labels_list.shape[0]), t_output_list[:,2], color='red')\n",
    "ax[3,1].set_title('Training LFP')\n",
    "ax[3,1].set_ylabel('LFP')\n",
    "ax[3,1].set_xlabel('Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
