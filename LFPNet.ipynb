{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git\n",
    "    paths.LOSS_FILE = COLAB_PRE + paths.LOSS_FILE\n",
    "    paths.PATH = COLAB_PRE + paths.PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import copy\n",
    "\n",
    "from utils import preprocess, metrics\n",
    "from config import params, paths\n",
    "from models import LFPNet\n",
    "\n",
    "s = 67\n",
    "\n",
    "rs = RandomState(MT19937(SeedSequence(s)))\n",
    "rng = default_rng(seed=s)\n",
    "torch.manual_seed(s)\n",
    "\n",
    "plt.rcParams.update({'font.size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,device):\n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "    \n",
    "#     feedback_arr = torch.zeros(params.BATCH_SIZE, 90)\n",
    "    \n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = .99995 #0.98 #decay the lr each step to 98% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                if params.RECURRENT_NET:\n",
    "                    x = torch.transpose(x, 2, 1)\n",
    "                x = x.to(device)\n",
    "                output = model(x)\n",
    "                y = y.to(device)\n",
    "#                 if i%100000 == 0 and epoch%5 == 0:\n",
    "#                     print(output)\n",
    "#                     print(y)\n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y)) \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "#                     if i%100000 == 0 and epoch%5 == 0:\n",
    "#                         print(model.cn1.weight.grad)\n",
    "#                         print(model.cn2.weight.grad)\n",
    "#                         print(model.fc1.weight.grad)\n",
    "#                         print(model.fc2.weight.grad)\n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if train_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = train_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    \n",
    "    loss_df.to_csv(paths.LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tr, f_va = preprocess.get_inVivo_LFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_tr, f_va, f_data = preprocess.get_filteredLFP()\n",
    "# f_tr, f_va, f_data = get_rawLFP()\n",
    "# f_tr, f_va, t_filt, v_filt = preprocess.get_end1D()#f_data, t_filt, v_filt, f_filt = get_end1D()\n",
    "\n",
    "# f_tr, f_va, f_data = preprocess.get_rawLFP()\n",
    "\n",
    "\n",
    "# noise = get_WN(channels=2)\n",
    "# sin = get_sin()\n",
    "\n",
    "# burst, fburst = preprocess.get_burstLFP()\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "train_loader = DataLoader(dataset=f_tr,batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "# tfilt_loader = DataLoader(dataset=t_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=f_va,batch_size=params.BATCH_SIZE)\n",
    "# vfilt_loader = DataLoader(dataset=v_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "\n",
    "# full_loader = DataLoader(dataset=f_data,batch_size=params.BATCH_SIZE)\n",
    "\n",
    "# ffull_loader = DataLoader(dataset=f_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "# noise_loader = DataLoader(dataset=noise,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "# sine_loader = DataLoader(dataset=sin,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "\n",
    "# burst_loader = DataLoader(dataset=burst,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "# fburst_loader = DataLoader(dataset=fburst,params.BATCH_SIZE=params.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a73892aa054513a7b4eee7ea122f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] train loss: 1960875.117188 val loss: 404791.258789\n",
      "[6,    30] train loss: 1961167.587891 val loss: 404791.626953\n",
      "[11,    30] train loss: 1961300.333984 val loss: 404789.846680\n",
      "[16,    30] train loss: 1960418.160156 val loss: 404792.024414\n",
      "[21,    30] train loss: 1961441.042969 val loss: 404792.246094\n",
      "[26,    30] train loss: 1962055.838867 val loss: 404791.459961\n",
      "[31,    30] train loss: 1962350.049805 val loss: 404793.916992\n",
      "[36,    30] train loss: 1961086.307617 val loss: 404791.398438\n",
      "[41,    30] train loss: 1961228.822266 val loss: 404793.193359\n",
      "[46,    30] train loss: 1961534.157227 val loss: 404793.136719\n",
      "[51,    30] train loss: 1960676.717773 val loss: 404794.196289\n",
      "[56,    30] train loss: 1960974.891602 val loss: 404793.168945\n",
      "[61,    30] train loss: 1960636.902344 val loss: 404792.040039\n",
      "[66,    30] train loss: 1960609.715820 val loss: 404793.818359\n",
      "[71,    30] train loss: 1962852.327148 val loss: 404792.335938\n",
      "[76,    30] train loss: 1961439.412109 val loss: 404793.694336\n",
      "[81,    30] train loss: 1961229.137695 val loss: 404793.864258\n",
      "[86,    30] train loss: 1961411.875000 val loss: 404793.692383\n",
      "[91,    30] train loss: 1961057.750000 val loss: 404794.853516\n",
      "[96,    30] train loss: 1961724.418945 val loss: 404793.050781\n",
      "[101,    30] train loss: 1960229.247070 val loss: 404794.233398\n",
      "[106,    30] train loss: 1961492.690430 val loss: 404792.496094\n",
      "[111,    30] train loss: 1961234.250977 val loss: 404793.234375\n",
      "[116,    30] train loss: 1960429.751953 val loss: 404795.247070\n",
      "[121,    30] train loss: 1960937.300781 val loss: 404793.574219\n",
      "[126,    30] train loss: 1961331.608398 val loss: 404792.884766\n",
      "[131,    30] train loss: 1961787.311523 val loss: 404794.800781\n",
      "[136,    30] train loss: 1960504.392578 val loss: 404795.289062\n",
      "[141,    30] train loss: 1960947.780273 val loss: 404795.156250\n",
      "[146,    30] train loss: 1961247.310547 val loss: 404796.324219\n",
      "[151,    30] train loss: 1961458.744141 val loss: 404797.299805\n",
      "[156,    30] train loss: 1961329.532227 val loss: 404797.754883\n",
      "[161,    30] train loss: 1960827.023438 val loss: 404796.291992\n",
      "[166,    30] train loss: 1960739.336914 val loss: 404796.376953\n",
      "[171,    30] train loss: 1960418.004883 val loss: 404797.343750\n",
      "[176,    30] train loss: 1961079.371094 val loss: 404797.875000\n",
      "[181,    30] train loss: 1962459.490234 val loss: 404794.975586\n",
      "[186,    30] train loss: 1960566.934570 val loss: 404794.541016\n",
      "[191,    30] train loss: 1960402.732422 val loss: 404795.502930\n",
      "[196,    30] train loss: 1960959.811523 val loss: 404795.047852\n",
      "[201,    30] train loss: 1960279.990234 val loss: 404792.860352\n",
      "[206,    30] train loss: 1960939.053711 val loss: 404792.718750\n",
      "[211,    30] train loss: 1960971.967773 val loss: 404792.229492\n",
      "[216,    30] train loss: 1961615.119141 val loss: 404793.485352\n",
      "[221,    30] train loss: 1960557.582031 val loss: 404792.626953\n",
      "[226,    30] train loss: 1961365.828125 val loss: 404792.395508\n",
      "[231,    30] train loss: 1960928.256836 val loss: 404791.627930\n",
      "[236,    30] train loss: 1961257.277344 val loss: 404791.146484\n",
      "[241,    30] train loss: 1960945.594727 val loss: 404792.415039\n",
      "[246,    30] train loss: 1962017.824219 val loss: 404792.400391\n",
      "[251,    30] train loss: 1961033.002930 val loss: 404792.925781\n",
      "[256,    30] train loss: 1961007.514648 val loss: 404792.538086\n",
      "[261,    30] train loss: 1962332.383789 val loss: 404791.700195\n",
      "[266,    30] train loss: 1961838.697266 val loss: 404791.229492\n",
      "[271,    30] train loss: 1961381.349609 val loss: 404791.853516\n",
      "[276,    30] train loss: 1961204.461914 val loss: 404793.472656\n",
      "[281,    30] train loss: 1960857.817383 val loss: 404792.854492\n",
      "[286,    30] train loss: 1960654.707031 val loss: 404792.723633\n",
      "[291,    30] train loss: 1961748.816406 val loss: 404794.182617\n",
      "[296,    30] train loss: 1960772.010742 val loss: 404794.257812\n",
      "[301,    30] train loss: 1960587.308594 val loss: 404794.203125\n",
      "[306,    30] train loss: 1960703.571289 val loss: 404795.068359\n",
      "[311,    30] train loss: 1961145.757812 val loss: 404794.081055\n",
      "[316,    30] train loss: 1961774.825195 val loss: 404793.180664\n",
      "[321,    30] train loss: 1961104.892578 val loss: 404794.129883\n",
      "[326,    30] train loss: 1961150.122070 val loss: 404796.245117\n",
      "[331,    30] train loss: 1960494.361328 val loss: 404795.996094\n",
      "[336,    30] train loss: 1960990.602539 val loss: 404795.010742\n",
      "[341,    30] train loss: 1960821.074219 val loss: 404795.756836\n",
      "[346,    30] train loss: 1960706.253906 val loss: 404794.015625\n",
      "[351,    30] train loss: 1961331.137695 val loss: 404794.216797\n",
      "[356,    30] train loss: 1960360.468750 val loss: 404792.867188\n",
      "[361,    30] train loss: 1961441.912109 val loss: 404793.847656\n",
      "[366,    30] train loss: 1961688.340820 val loss: 404795.631836\n",
      "[371,    30] train loss: 1961750.005859 val loss: 404796.208008\n",
      "[376,    30] train loss: 1961103.400391 val loss: 404794.825195\n",
      "[381,    30] train loss: 1960846.821289 val loss: 404796.096680\n",
      "[386,    30] train loss: 1960702.824219 val loss: 404796.490234\n",
      "[391,    30] train loss: 1961528.168945 val loss: 404796.787109\n",
      "[396,    30] train loss: 1960537.173828 val loss: 404797.430664\n",
      "[401,    30] train loss: 1961356.560547 val loss: 404795.083008\n",
      "[406,    30] train loss: 1962301.101562 val loss: 404795.090820\n",
      "[411,    30] train loss: 1960749.155273 val loss: 404794.843750\n",
      "[416,    30] train loss: 1960891.493164 val loss: 404793.707031\n",
      "[421,    30] train loss: 1960465.666016 val loss: 404791.946289\n",
      "[426,    30] train loss: 1961837.750000 val loss: 404794.870117\n",
      "[431,    30] train loss: 1961892.934570 val loss: 404794.684570\n",
      "[436,    30] train loss: 1960387.882812 val loss: 404795.461914\n",
      "[441,    30] train loss: 1960881.351562 val loss: 404794.110352\n",
      "[446,    30] train loss: 1961544.859375 val loss: 404793.224609\n",
      "[451,    30] train loss: 1960746.906250 val loss: 404794.356445\n",
      "[456,    30] train loss: 1961384.618164 val loss: 404793.242188\n",
      "[461,    30] train loss: 1960774.968750 val loss: 404792.419922\n",
      "[466,    30] train loss: 1961403.341797 val loss: 404793.271484\n",
      "[471,    30] train loss: 1961188.349609 val loss: 404792.931641\n",
      "[476,    30] train loss: 1960363.503906 val loss: 404791.953125\n",
      "[481,    30] train loss: 1961272.206055 val loss: 404792.432617\n",
      "[486,    30] train loss: 1961008.242188 val loss: 404794.760742\n",
      "[491,    30] train loss: 1961187.021484 val loss: 404793.903320\n",
      "[496,    30] train loss: 1960819.041016 val loss: 404794.248047\n"
     ]
    }
   ],
   "source": [
    "model1 = params.MODEL(\n",
    "    in_size=params.INPUT_SIZE,\n",
    "    h_size=params.HIDDEN_SIZE,\n",
    "    out_size=params.OUTPUT_SIZE,\n",
    "    num_layers=params.NUM_LAYERS,\n",
    "    dropout=params.DROPOUT\n",
    ")\n",
    "# model1 = torch.load(paths.PATH)\n",
    "model_initial = copy.deepcopy(model1)\n",
    "model1 = torch.load(paths.PATH)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "# for name, param in model1.named_parameters():\n",
    "#     print(name)\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# model1.rnn.weight_ih_l0.requires_grad = True\n",
    "# model1.rnn.weight_hh_l0.requires_grad = True\n",
    "# model1.rnn.bias_ih_l0.requires_grad = True\n",
    "# model1.rnn.bias_hh_l0.requires_grad = True\n",
    "\n",
    "# model1.convs2k3.weight.requires_grad = True\n",
    "# model1.convs2k3.bias.requires_grad = True\n",
    "# model1.dilation.weight.requires_grad = True\n",
    "# model1.dilation.bias.requires_grad = True\n",
    "# model1.convs1k5.weight.requires_grad = True\n",
    "# model1.convs1k5.bias.requires_grad = True\n",
    "# model1.convs1k3.weight.requires_grad = True\n",
    "# model1.convs1k3.bias.requires_grad = True\n",
    "\n",
    "pnfr_training_loss, pnfr_validation_loss = train_model(model1,paths.PATH,train_loader,\n",
    "                                                       val_loader,params.EPOCHS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps ahead: 0.0025159535728600524\n",
      "1 steps ahead: 0.0010912941644322105\n",
      "2 steps ahead: 0.0011990763131961568\n",
      "3 steps ahead: 0.0017586288783277837\n",
      "4 steps ahead: 0.0017725131306293163\n",
      "5 steps ahead: 0.0014861994126014189\n",
      "6 steps ahead: 0.0010185072413566587\n",
      "7 steps ahead: 0.0010477220551398103\n",
      "8 steps ahead: 0.00028305137332129693\n",
      "9 steps ahead: 0.0013082613535077003\n",
      "10 steps ahead: 0.0013165171525919561\n",
      "11 steps ahead: 0.0009114689330053105\n",
      "12 steps ahead: 0.0013103058048017369\n",
      "13 steps ahead: 0.00036159909583399763\n",
      "14 steps ahead: 0.0007275250151056456\n",
      "15 steps ahead: 0.0005860289055686918\n",
      "16 steps ahead: 0.0007267631512338779\n",
      "17 steps ahead: 0.0010134089396552914\n",
      "18 steps ahead: 0.0011589012570417667\n",
      "19 steps ahead: 0.0011099699020447051\n",
      "20 steps ahead: 0.0010246078502750278\n",
      "21 steps ahead: 0.0011003969158177584\n",
      "22 steps ahead: 0.0006627084685240092\n",
      "23 steps ahead: 0.0009692244438253006\n",
      "24 steps ahead: 0.0007862188389629043\n",
      "25 steps ahead: 0.0008180786810204621\n",
      "26 steps ahead: 0.0007398479623226262\n",
      "27 steps ahead: 0.00026297324285506907\n",
      "28 steps ahead: 0.0009692570690919\n",
      "29 steps ahead: 0.0003694202990137674\n",
      "30 steps ahead: 0.0009183693522959446\n",
      "31 steps ahead: 0.0009140675742931315\n",
      "32 steps ahead: 0.00023722021294170226\n",
      "33 steps ahead: 0.00033942979685264163\n",
      "34 steps ahead: 0.0012488517314838132\n",
      "35 steps ahead: 0.0015123598971811258\n",
      "36 steps ahead: 0.0008477795647866548\n",
      "37 steps ahead: 0.0010501167868184336\n",
      "38 steps ahead: 0.0007391012590476365\n",
      "39 steps ahead: 0.0011981804708495192\n",
      "40 steps ahead: 0.0008084707925954948\n",
      "41 steps ahead: 0.0006181272184824227\n",
      "42 steps ahead: 0.0005422077476707754\n",
      "43 steps ahead: 0.0008376139821912876\n",
      "44 steps ahead: 0.00108082550607147\n",
      "45 steps ahead: 0.0009433264101071925\n",
      "46 steps ahead: 0.0014882660036479312\n",
      "47 steps ahead: 0.0009218695975966584\n",
      "48 steps ahead: 0.0017864514212615479\n",
      "49 steps ahead: 0.00030930694860653585\n",
      "50 steps ahead: 0.0011913822993451229\n",
      "51 steps ahead: 0.00010900253090695422\n",
      "52 steps ahead: 0.001417725995828456\n",
      "53 steps ahead: 0.0002822461253282338\n",
      "54 steps ahead: 0.0006018076746960865\n",
      "55 steps ahead: 1.8117261736727563e-05\n",
      "56 steps ahead: 0.0014790019689315814\n",
      "57 steps ahead: 0.0009930796428029298\n",
      "58 steps ahead: 0.00024155407826254027\n",
      "59 steps ahead: 0.00025489449074667103\n",
      "60 steps ahead: 0.0006914103977779806\n",
      "61 steps ahead: 0.0014029408210199934\n",
      "62 steps ahead: 0.0010776197000744725\n",
      "63 steps ahead: 0.0008218909775945527\n",
      "64 steps ahead: 0.0010492829639461299\n",
      "65 steps ahead: 0.0013708015083200298\n",
      "66 steps ahead: 0.0008516000261964418\n",
      "67 steps ahead: 0.0009263957632887898\n",
      "68 steps ahead: 1.8059425374028493e-06\n",
      "69 steps ahead: 0.000777296871369737\n",
      "70 steps ahead: 0.0015299783460811511\n",
      "71 steps ahead: 0.001379092327640441\n",
      "72 steps ahead: 0.001226367962770869\n",
      "73 steps ahead: 0.001425647359120319\n",
      "74 steps ahead: 0.00018847714250580516\n",
      "75 steps ahead: 0.0008827294200270952\n",
      "76 steps ahead: 0.0005705015489829712\n",
      "77 steps ahead: 0.0014570211663851307\n",
      "78 steps ahead: 0.0015053879102583378\n",
      "79 steps ahead: 0.0011799777047249993\n",
      "80 steps ahead: 0.000769065860896645\n",
      "81 steps ahead: 0.0016760758714449153\n",
      "82 steps ahead: 0.0013772624546631862\n",
      "83 steps ahead: 0.0003453920019552692\n",
      "84 steps ahead: 0.0008398500790127716\n",
      "85 steps ahead: 0.00012063811780493872\n",
      "86 steps ahead: 0.0010755427083372071\n",
      "87 steps ahead: 0.001244282138593067\n",
      "88 steps ahead: 0.0013956776236713209\n",
      "89 steps ahead: 0.0007889806527823229\n",
      "90 steps ahead: 0.0008957118055875624\n",
      "91 steps ahead: 0.0015002965049145667\n",
      "92 steps ahead: 0.0011061888313292911\n",
      "93 steps ahead: 0.0009969910226276246\n",
      "94 steps ahead: 0.0007985519186802792\n",
      "95 steps ahead: 0.0008381006553755954\n",
      "96 steps ahead: 0.0006357031349712994\n",
      "97 steps ahead: 0.0003906508800121111\n",
      "98 steps ahead: 0.0012399442081457712\n",
      "99 steps ahead: 0.00021527787184472125\n",
      "0 steps ahead: 0.002226498654917086\n",
      "1 steps ahead: 0.0010881071814451415\n",
      "2 steps ahead: 0.0004425489201026478\n",
      "3 steps ahead: 0.001612772400359952\n",
      "4 steps ahead: 0.001009254380647362\n",
      "5 steps ahead: 0.0006890001195157813\n",
      "6 steps ahead: 3.898046526096266e-05\n",
      "7 steps ahead: 0.00010241289354473437\n",
      "8 steps ahead: 7.998623615523837e-05\n",
      "9 steps ahead: 0.00045365979830647163\n",
      "10 steps ahead: 0.0006133743426630645\n",
      "11 steps ahead: 0.0005073676233815494\n",
      "12 steps ahead: -3.116140034986614e-05\n",
      "13 steps ahead: -6.595664734998508e-05\n",
      "14 steps ahead: 0.00012366425913845625\n",
      "15 steps ahead: 0.0004099709028952958\n",
      "16 steps ahead: -0.00014882811014049757\n",
      "17 steps ahead: 0.000248113472968603\n",
      "18 steps ahead: 0.0005127494441474756\n",
      "19 steps ahead: 0.00024314843928607832\n",
      "20 steps ahead: 0.0001566487905135805\n",
      "21 steps ahead: -0.0006288680715653694\n",
      "22 steps ahead: 0.00010445263953506867\n",
      "23 steps ahead: -3.294519732421897e-05\n",
      "24 steps ahead: -0.0002838205981519071\n",
      "25 steps ahead: -5.707201057858491e-05\n",
      "26 steps ahead: 5.874669746874783e-05\n",
      "27 steps ahead: 3.100087812923924e-05\n",
      "28 steps ahead: -0.0004938630610562544\n",
      "29 steps ahead: -0.00045451564040921433\n",
      "30 steps ahead: -0.00014905646549379092\n",
      "31 steps ahead: -2.0758634918571772e-05\n",
      "32 steps ahead: -0.00013848049027509468\n",
      "33 steps ahead: -0.00017015148384280643\n",
      "34 steps ahead: 8.326113866563567e-05\n",
      "35 steps ahead: 0.00032067276752167917\n",
      "36 steps ahead: -0.0002999882510625884\n",
      "37 steps ahead: -0.00037602481222664963\n",
      "38 steps ahead: -0.0003050306220135912\n",
      "39 steps ahead: -3.214028243614386e-05\n",
      "40 steps ahead: 8.455794227102409e-05\n",
      "41 steps ahead: 7.135603030306026e-05\n",
      "42 steps ahead: -9.85235294113096e-05\n",
      "43 steps ahead: -0.00043522376324700573\n",
      "44 steps ahead: -0.0007028533489306099\n",
      "45 steps ahead: 6.565882688447378e-05\n",
      "46 steps ahead: -0.00021924044779608742\n",
      "47 steps ahead: 5.0879233515765954e-05\n",
      "48 steps ahead: 0.00030859297820218234\n",
      "49 steps ahead: -0.0002536043469221205\n",
      "50 steps ahead: 6.171512607811014e-05\n",
      "51 steps ahead: -7.410293770293563e-05\n",
      "52 steps ahead: -0.00014792109013828103\n",
      "53 steps ahead: -0.0003936338474848089\n",
      "54 steps ahead: -3.418217920780897e-05\n",
      "55 steps ahead: -8.539124071216264e-05\n",
      "56 steps ahead: -2.9036301420148902e-05\n",
      "57 steps ahead: -0.0002510945340929993\n",
      "58 steps ahead: 2.9957958848303434e-05\n",
      "59 steps ahead: -0.00036744085114781555\n",
      "60 steps ahead: -0.00019456727760891468\n",
      "61 steps ahead: -4.9257584325390624e-05\n",
      "62 steps ahead: -0.0002133058423055001\n",
      "63 steps ahead: 0.00013865273924473787\n",
      "64 steps ahead: -0.00023542360982986565\n",
      "65 steps ahead: -0.00016402708626839235\n",
      "66 steps ahead: -9.194978697446565e-05\n",
      "67 steps ahead: -2.6965034331416504e-05\n",
      "68 steps ahead: -8.10306414331663e-05\n",
      "69 steps ahead: -0.00013321519845099772\n",
      "70 steps ahead: -0.00012771238846331912\n",
      "71 steps ahead: -0.0002983521797290667\n",
      "72 steps ahead: -0.0003731117613166024\n",
      "73 steps ahead: 4.589235728424956e-05\n",
      "74 steps ahead: -3.627515774429213e-05\n",
      "75 steps ahead: -6.112155100623795e-05\n",
      "76 steps ahead: -0.0003117717553480226\n",
      "77 steps ahead: 4.621926832071299e-05\n",
      "78 steps ahead: -0.0004988984422047427\n",
      "79 steps ahead: -0.00047393682373053636\n",
      "80 steps ahead: -6.894078465857767e-05\n",
      "81 steps ahead: -0.00014655853987011191\n",
      "82 steps ahead: -0.00020569284788929387\n",
      "83 steps ahead: -0.00041036150441486896\n",
      "84 steps ahead: -0.00036803647385585947\n",
      "85 steps ahead: -9.752234032833762e-05\n",
      "86 steps ahead: -1.8590111331651116e-05\n",
      "87 steps ahead: -0.00026827604982715414\n",
      "88 steps ahead: -0.00046207035336554014\n",
      "89 steps ahead: -4.7306674892944045e-05\n",
      "90 steps ahead: -0.00027748070531763425\n",
      "91 steps ahead: -5.120061119523456e-05\n",
      "92 steps ahead: 7.087238013625541e-05\n",
      "93 steps ahead: -0.00020250215006889505\n",
      "94 steps ahead: -3.311099538505502e-05\n",
      "95 steps ahead: 2.0176598993004724e-05\n",
      "96 steps ahead: 2.773009919909697e-06\n",
      "97 steps ahead: -9.093451931829755e-05\n",
      "98 steps ahead: -0.00015623526624519712\n",
      "99 steps ahead: 1.763386039632664e-05\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(paths.PATH)\n",
    "model1.eval()\n",
    "\n",
    "start = 10\n",
    "k = 10\n",
    "end= (start + k) if k != None else None\n",
    "\n",
    "model1.to('cpu')\n",
    "\n",
    "t_pred, t_real = metrics.r2_eval(model1, train_loader, k=end)\n",
    "v_pred, v_real = metrics.r2_eval(model1, val_loader, k=end)\n",
    "\n",
    "# f_pred, f_real = r2_eval(model1, full_loader,filt=ffull_loader, k=end)\n",
    "# print(f_real)\n",
    "# t_pred, t_real = r2_eval(model1, train_loader, filt=None ,k=end)\n",
    "# v_pred, v_real = r2_eval(model1, val_loader, filt=None, k=end)\n",
    "# f_pred, f_real = r2_eval(model1, full_loader,filt=None, k=end)\n",
    "# n_pred, n_real = r2_eval(model1, noise_loader, end)\n",
    "# s_pred, s_real = r2_eval(model1, sine_loader, end)\n",
    "\n",
    "# b_pred, b_real = r2_eval(model1, burst_loader, filt=fburst_loader,k=end)\n",
    "\n",
    "# for i in range(len(s_pred)):\n",
    "#     print(\"output: {} label: {}\".format(s_pred[i], s_real[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 13601.695312\n",
      "Val MSE: 13444.363281\n",
      "(10240, 100)\n",
      "(10240, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train MSE: {:f}\".format(mean_squared_error(t_real, t_pred)))\n",
    "print(\"Val MSE: {:f}\".format(mean_squared_error(v_real, v_pred)))\n",
    "# print(\"Full MSE: {:f}\".format(mean_squared_error(f_real, f_pred)))\n",
    "# print(\"Burst MSE: {:f}\".format(mean_squared_error(b_real, b_pred)))\n",
    "print(t_real.shape)\n",
    "print(t_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, vp, tr, vr = t_pred, v_pred, t_real, v_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240,)\n",
      "(10240,)\n"
     ]
    }
   ],
   "source": [
    "# print(next(iter(train_loader))[0][:,2,0])\n",
    "# print(next(iter(burst_loader))[0][:,2,0])\n",
    "t_pred = tp[:,0]\n",
    "v_pred = vp[:,0]\n",
    "t_real = tr[:,0]\n",
    "v_real = vr[:,0]\n",
    "print(t_pred.shape)\n",
    "print(t_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a68f0f62f743fdad6558d08314420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, -5.47777777777778, 'Epoch')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(nrows=1, ncols=2)\n",
    "fig1.tight_layout()\n",
    "ax1[0].plot(range(params.EPOCHS), pnfr_training_loss)\n",
    "ax1[0].set_title('Training Loss')\n",
    "ax1[0].set_ylabel('Loss')\n",
    "ax1[0].set_xlabel('Epoch')\n",
    "\n",
    "ax1[1].plot(range(params.EPOCHS), pnfr_validation_loss)\n",
    "ax1[1].set_title('Validation Loss')\n",
    "ax1[1].set_ylabel('Loss')\n",
    "ax1[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cdbc36eb3b4c3eb72a0885f9d64431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) (20,)\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].plot(np.arange(0,end), v_real[0:end], color='blue',label='Labels')\n",
    "# ax[2,0].plot(np.arange(start-10,end), v_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "print(np.arange(0,end).shape, v_pred[:end].shape)\n",
    "ax[0].scatter(np.arange(0,end), v_pred[:end], color='slateblue',label='Predicted t+1')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[0].plot(np.arange(start-10,end), v_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "\n",
    "ax[0].set_title('Validation LFPNet')\n",
    "ax[0].set_ylabel('LFP')\n",
    "ax[0].set_xlabel('Time')\n",
    "# ax[2,0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(0,end), t_real[:end], color='blue',label='Labels')\n",
    "# a[2,1].plot(np.arange(start-10,end), t_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "ax[1].scatter(np.arange(0,end), t_pred[:end], color='slateblue',label='Training t+1')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[1].plot(np.arange(start-10,end), t_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "ax[1].set_title('Training LFPNet')\n",
    "ax[1].set_ylabel('LFP')\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[0].legend(loc=2, prop={'size': 10})\n",
    "\n",
    "# import plotly.tools as tls\n",
    "# plotly_fig = tls.mpl_to_plotly(fig)\n",
    "# plotly_fig.write_html(\"testfile.html\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9b146f61fe44f281e546581a107a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73420cd843a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training t+10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Full LFP vs Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_real' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.plot(np.arange(start,end), f_real[start:end], color='blue',label='Labels')\n",
    "ax.scatter(np.arange(start,end), f_pred[start:end], color='red',label='Training t+10')\n",
    "ax.set_title('Full LFP vs Time')\n",
    "ax.set_ylabel('Signal')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# ax[1].plot(np.arange(start,end), n_real[start:end], color='blue',label='Labels')\n",
    "# ax[1].scatter(np.arange(start,end), n_pred[start:end], color='red',label='Training t+10')\n",
    "# ax[1].set_title('Noise')\n",
    "# ax[1].set_ylabel('Signal')\n",
    "# ax[1].set_xlabel('Time')z\n",
    "\n",
    "# ax.plot(np.arange(start,end), s_real[start:end], color='blue',label='Labels')\n",
    "# ax.scatter(np.arange(start,end), s_pred[start:end], color='red',label='Training t+10')\n",
    "# ax.set_title('Sine')\n",
    "# ax.set_ylabel('LFP')\n",
    "# ax.set_xlabel('Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "\n",
    "STEPS = 100\n",
    "# model_initial = params.MODEL(params.INPUT_SIZE,params.HIDDEN_SIZE,params.OUTPUT_SIZE)\n",
    "model_final = copy.deepcopy(model1)\n",
    "\n",
    "\n",
    "# data that the evaluator will use when evaluating loss\n",
    "x, y = iter(noise_loader).__next__()\n",
    "metric = loss_landscapes.metrics.Loss(nn.MSELoss(), x, y)\n",
    "\n",
    "\n",
    "loss_data_fin = loss_landscapes.random_plane(model_final, metric, 10000, STEPS, normalization='model', deepcopy_model=True)\n",
    "# plt.contour(loss_data_fin, levels=50)\n",
    "# plt.title('Loss Contours around Trained Model')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "X = np.array([[j for j in range(STEPS)] for i in range(STEPS)])\n",
    "Y = np.array([[i for _ in range(STEPS)] for i in range(STEPS)])\n",
    "ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface Plot of Loss Landscape')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict()['conv_block.0.weight'])\n",
    "print(model1.state_dict()['conv_block.0.bias'])\n",
    "print(model1.state_dict()['conv_block.2.weight'])\n",
    "print(model1.state_dict()['conv_block.2.bias'])\n",
    "print(model1.state_dict()['ck1s1.weight'])\n",
    "print(model1.state_dict()['ck1s1.bias'])\n",
    "print(model1.state_dict()['fc1.weight'])\n",
    "print(model1.state_dict()['fc1.bias'])\n",
    "print(model1.state_dict()['fc2.weight'])\n",
    "print(model1.state_dict()['fc2.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
