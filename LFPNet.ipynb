{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git\n",
    "    paths.LOSS_FILE = COLAB_PRE + paths.LOSS_FILE\n",
    "    paths.PATH = COLAB_PRE + paths.PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import copy\n",
    "\n",
    "from utils import preprocess, metrics\n",
    "from config import params, paths\n",
    "from models import LFPNet\n",
    "\n",
    "s = 67\n",
    "\n",
    "rs = RandomState(MT19937(SeedSequence(s)))\n",
    "rng = default_rng(seed=s)\n",
    "torch.manual_seed(s)\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,device):\n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "    \n",
    "#     feedback_arr = torch.zeros(params.BATCH_SIZE, 90)\n",
    "    \n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = .99995 #0.98 #decay the lr each step to 98% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                if params.RECURRENT_NET:\n",
    "                    x = torch.transpose(x, 2, 1)\n",
    "                x = x.to(device)\n",
    "                output = model(x)\n",
    "                y = y.to(device)\n",
    "#                 if i%100000 == 0 and epoch%5 == 0:\n",
    "#                     print(output)\n",
    "#                     print(y)\n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y)) \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "#                     if i%100000 == 0 and epoch%5 == 0:\n",
    "#                         print(model.cn1.weight.grad)\n",
    "#                         print(model.cn2.weight.grad)\n",
    "#                         print(model.fc1.weight.grad)\n",
    "#                         print(model.fc2.weight.grad)\n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if train_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = train_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    \n",
    "    loss_df.to_csv(paths.LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 1, 100) (15000, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "f_tr, f_va, filtered = preprocess.get_inVivo_LFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn datasets into iterable dataloaders\n",
    "train_loader = DataLoader(dataset=f_tr,batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "# tfilt_loader = DataLoader(dataset=t_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=f_va,batch_size=params.BATCH_SIZE)\n",
    "filt_loader = DataLoader(dataset=filtered, batch_size=params.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LFPNetLSTM(\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (dilation_branch): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv1d(1, 50, kernel_size=(3,), stride=(1,), dilation=(257,))\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(50, 50, kernel_size=(3,), stride=(1,), dilation=(129,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Conv1d(50, 1, kernel_size=(3,), stride=(1,), dilation=(62,))\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (convolution_block1): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv1d(1, 50, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (convolution_block2): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv1d(50, 50, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (convolution_block3): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv1d(50, 50, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv1d(50, 1, kernel_size=(1,), stride=(1,))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fcn_branch): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (fcn_pred_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=96, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=96, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (rnn): LSTM(1, 1, num_layers=2, batch_first=True, dropout=0.5)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = params.MODEL(\n",
    "    in_size=params.INPUT_SIZE,\n",
    "    h_size=params.HIDDEN_SIZE,\n",
    "    out_size=params.OUTPUT_SIZE,\n",
    "    num_layers=params.NUM_LAYERS,\n",
    "    dropout=params.DROPOUT\n",
    ")\n",
    "model_initial = copy.deepcopy(model1)\n",
    "model1 = torch.load(paths.PATH)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "\n",
    "# pnfr_training_loss, pnfr_validation_loss = train_model(model1,paths.PATH,train_loader,\n",
    "#                                                        val_loader,params.EPOCHS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56320, 100) (56320, 100)\n",
      "0 steps ahead: -62.24849380120956\n",
      "1 steps ahead: -0.4201598144620966\n",
      "2 steps ahead: 0.9239162142980463\n",
      "3 steps ahead: 0.9091473128203439\n",
      "4 steps ahead: 0.9262890717800505\n",
      "5 steps ahead: 0.917984649565689\n",
      "6 steps ahead: 0.9117589610989203\n",
      "7 steps ahead: 0.8816378483313783\n",
      "8 steps ahead: 0.7958236768156848\n",
      "9 steps ahead: 0.37205618148021147\n",
      "10 steps ahead: 0.5890361371688875\n",
      "11 steps ahead: 0.8756708818840553\n",
      "12 steps ahead: 0.9103817188781008\n",
      "13 steps ahead: 0.9100619486251283\n",
      "14 steps ahead: 0.8854785726579845\n",
      "15 steps ahead: 0.8406013463700406\n",
      "16 steps ahead: 0.7451695205854243\n",
      "17 steps ahead: 0.4835815034461871\n",
      "18 steps ahead: 0.03609568635301674\n",
      "19 steps ahead: 0.5909746730606497\n",
      "20 steps ahead: 0.8054954435568056\n",
      "21 steps ahead: 0.8525908815121941\n",
      "22 steps ahead: 0.850464551512942\n",
      "23 steps ahead: 0.8212751286094717\n",
      "24 steps ahead: 0.7612967475051676\n",
      "25 steps ahead: 0.6295570008976095\n",
      "26 steps ahead: 0.34600773052983014\n",
      "27 steps ahead: 0.07995544350751949\n",
      "28 steps ahead: 0.37779643005130104\n",
      "29 steps ahead: 0.6326154267625728\n",
      "30 steps ahead: 0.7220508082095896\n",
      "31 steps ahead: 0.7432186585061635\n",
      "32 steps ahead: 0.7222390564505803\n",
      "33 steps ahead: 0.6597088109098487\n",
      "34 steps ahead: 0.5365954444897759\n",
      "35 steps ahead: 0.3267069397321247\n",
      "36 steps ahead: 0.10832233263679614\n",
      "37 steps ahead: 0.1193894322323954\n",
      "38 steps ahead: 0.32116612679531387\n",
      "39 steps ahead: 0.46415741708312697\n",
      "40 steps ahead: 0.5289405760815727\n",
      "41 steps ahead: 0.5390657803035634\n",
      "42 steps ahead: 0.5013629659084154\n",
      "43 steps ahead: 0.41705689572145244\n",
      "44 steps ahead: 0.2916645154107782\n",
      "45 steps ahead: 0.1552031188080758\n",
      "46 steps ahead: 0.06632833793828541\n",
      "47 steps ahead: 0.0475494440204689\n",
      "48 steps ahead: 0.10678615745155495\n",
      "49 steps ahead: 0.16801115505903685\n",
      "50 steps ahead: 0.20970566296727966\n",
      "51 steps ahead: 0.2241137765010086\n",
      "52 steps ahead: 0.213027737088055\n",
      "53 steps ahead: 0.18062689133113718\n",
      "54 steps ahead: 0.13743451194958545\n",
      "55 steps ahead: 0.09298707622816726\n",
      "56 steps ahead: 0.058070215818064375\n",
      "57 steps ahead: 0.042732571693032795\n",
      "58 steps ahead: 0.040272835078560165\n",
      "59 steps ahead: 0.03918734734394125\n",
      "60 steps ahead: 0.034228125840415013\n",
      "61 steps ahead: 0.038762508111437666\n",
      "62 steps ahead: 0.042748810943968985\n",
      "63 steps ahead: 0.04836625007543627\n",
      "64 steps ahead: 0.06238327295616908\n",
      "65 steps ahead: 0.08025200983144187\n",
      "66 steps ahead: 0.09337368942336233\n",
      "67 steps ahead: 0.09666182548021807\n",
      "68 steps ahead: 0.08487255794270354\n",
      "69 steps ahead: 0.06293060666196804\n",
      "70 steps ahead: 0.03490964519698614\n",
      "71 steps ahead: 0.012373330880035449\n",
      "72 steps ahead: 0.001003034004899206\n",
      "73 steps ahead: 0.011685693990432844\n",
      "74 steps ahead: 0.031749261326463496\n",
      "75 steps ahead: 0.05317863494782138\n",
      "76 steps ahead: 0.06851714351804283\n",
      "77 steps ahead: 0.07369654777126666\n",
      "78 steps ahead: 0.06804012281704808\n",
      "79 steps ahead: 0.05534843808959733\n",
      "80 steps ahead: 0.03953276393844696\n",
      "81 steps ahead: 0.024327972171331647\n",
      "82 steps ahead: 0.015273338004732295\n",
      "83 steps ahead: 0.009179807996624989\n",
      "84 steps ahead: 0.00991847499359888\n",
      "85 steps ahead: 0.011736466883346286\n",
      "86 steps ahead: 0.019691219689263573\n",
      "87 steps ahead: 0.024896137181499967\n",
      "88 steps ahead: 0.029462303142820168\n",
      "89 steps ahead: 0.03029078162905019\n",
      "90 steps ahead: 0.02872098674111412\n",
      "91 steps ahead: 0.02445658374861137\n",
      "92 steps ahead: 0.01784395524580562\n",
      "93 steps ahead: 0.01193957519549449\n",
      "94 steps ahead: 0.007096390274577424\n",
      "95 steps ahead: 0.0026291228464930816\n",
      "96 steps ahead: 0.00412895872471053\n",
      "97 steps ahead: 0.003671795802117961\n",
      "98 steps ahead: 0.0052570667452335185\n",
      "99 steps ahead: 0.009137442999999523\n",
      "(15000, 100) (15000, 100)\n",
      "0 steps ahead: 0.9999917857756294, \t-57.56321053917079\n",
      "1 steps ahead: 0.999986973533492, \t-0.40181083749860225\n",
      "2 steps ahead: 0.9999891672443314, \t0.9262168184495426\n",
      "3 steps ahead: 0.999908712872167, \t0.9095571035773332\n",
      "4 steps ahead: 0.9997137702979647, \t0.9265017748373732\n",
      "5 steps ahead: 0.9992065704329035, \t0.917825895148211\n",
      "6 steps ahead: 0.99853310693866, \t0.910857697567821\n",
      "7 steps ahead: 0.997668739098863, \t0.8794977180008263\n",
      "8 steps ahead: 0.9970404627929901, \t0.7909046084464447\n",
      "9 steps ahead: 0.9969696938167617, \t0.35498903707281615\n",
      "10 steps ahead: 0.996767564253613, \t0.589812564229143\n",
      "11 steps ahead: 0.9940682977146957, \t0.8752878758757188\n",
      "12 steps ahead: 0.9879338454231255, \t0.9097324326091627\n",
      "13 steps ahead: 0.977877319471174, \t0.9087099270937536\n",
      "14 steps ahead: 0.9623301555682412, \t0.8828909531209094\n",
      "15 steps ahead: 0.9470703150449851, \t0.836634157062057\n",
      "16 steps ahead: 0.9367671831385158, \t0.737981838089165\n",
      "17 steps ahead: 0.9336341545169069, \t0.4637186505208005\n",
      "18 steps ahead: 0.9325335989916623, \t0.018905363382313856\n",
      "19 steps ahead: 0.9288175574420983, \t0.5910173468609933\n",
      "20 steps ahead: 0.9089676289411847, \t0.80546613696545\n",
      "21 steps ahead: 0.8702123625658208, \t0.8502749959741084\n",
      "22 steps ahead: 0.8136410539416421, \t0.8453433348741186\n",
      "23 steps ahead: 0.7513709143980449, \t0.8124001637583566\n",
      "24 steps ahead: 0.7034859809450513, \t0.7468232257515031\n",
      "25 steps ahead: 0.6746109623722063, \t0.6058540086173732\n",
      "26 steps ahead: 0.6673007474928011, \t0.30568381010766243\n",
      "27 steps ahead: 0.6704763044105565, \t0.03477515808894449\n",
      "28 steps ahead: 0.663773904046812, \t0.36905415261112384\n",
      "29 steps ahead: 0.62733525743027, \t0.6321023858815442\n",
      "30 steps ahead: 0.5515088104225399, \t0.7176168452090679\n",
      "31 steps ahead: 0.46135437265596235, \t0.7321186197556078\n",
      "32 steps ahead: 0.3727887274708672, \t0.7027258847707873\n",
      "33 steps ahead: 0.31062921352966744, \t0.6318492404006668\n",
      "34 steps ahead: 0.2812839251454913, \t0.4982324187363789\n",
      "35 steps ahead: 0.2784983589649439, \t0.27568463150641587\n",
      "36 steps ahead: 0.2860891549559571, \t0.05139249978710292\n",
      "37 steps ahead: 0.2856090018388565, \t0.09877549566787747\n",
      "38 steps ahead: 0.26502548086645805, \t0.3124160296965215\n",
      "39 steps ahead: 0.21812108031251065, \t0.45231502239405075\n",
      "40 steps ahead: 0.16030058884131038, \t0.5065518092433396\n",
      "41 steps ahead: 0.10945497707922147, \t0.5036310814620425\n",
      "42 steps ahead: 0.07492520606756914, \t0.4559390172998422\n",
      "43 steps ahead: 0.05999614113575602, \t0.3662289047670617\n",
      "44 steps ahead: 0.05948603957142318, \t0.2384509482953705\n",
      "45 steps ahead: 0.06620468263772128, \t0.1031373165050552\n",
      "46 steps ahead: 0.07569175298146069, \t0.026671975021702488\n",
      "47 steps ahead: 0.07165775072643288, \t0.02982078669458821\n",
      "48 steps ahead: 0.06476977059062206, \t0.0847731956545188\n",
      "49 steps ahead: 0.054071833811168646, \t0.1371493521319772\n",
      "50 steps ahead: 0.04290395721535911, \t0.1657467667834186\n",
      "51 steps ahead: 0.03848000012036912, \t0.1728875692234092\n",
      "52 steps ahead: 0.0383583001799459, \t0.16034407333199185\n",
      "53 steps ahead: 0.04213707445539461, \t0.13487468031414684\n",
      "54 steps ahead: 0.047173251981622144, \t0.1029381069173484\n",
      "55 steps ahead: 0.051962095165635325, \t0.07194301004395798\n",
      "56 steps ahead: 0.053764913330021535, \t0.04532371888472797\n",
      "57 steps ahead: 0.05307257475721161, \t0.02533240716627716\n",
      "58 steps ahead: 0.04947744386411401, \t0.010031215959491635\n",
      "59 steps ahead: 0.048843122772067815, \t0.00312864993877493\n",
      "60 steps ahead: 0.052355299810689426, \t0.004602292635546901\n",
      "61 steps ahead: 0.052023225138762896, \t0.0065448723300239875\n",
      "62 steps ahead: 0.056469916140588716, \t0.019456957550903486\n",
      "63 steps ahead: 0.05860780885178085, \t0.03640759384661252\n",
      "64 steps ahead: 0.058858922651181866, \t0.05591442318127848\n",
      "65 steps ahead: 0.057361486058463584, \t0.07399802268247635\n",
      "66 steps ahead: 0.052688600717094025, \t0.08441334583681182\n",
      "67 steps ahead: 0.04711591811896465, \t0.08511790185804868\n",
      "68 steps ahead: 0.04071058917453141, \t0.07357530162030601\n",
      "69 steps ahead: 0.03597717847553883, \t0.0527483512053154\n",
      "70 steps ahead: 0.03311209319830233, \t0.027904320932475635\n",
      "71 steps ahead: 0.03180529894004758, \t0.007545628782277269\n",
      "72 steps ahead: 0.031735821810115805, \t-0.00016898742455584426\n",
      "73 steps ahead: 0.03217870800181799, \t0.007369158768633088\n",
      "74 steps ahead: 0.03190826843124328, \t0.02570073331801448\n",
      "75 steps ahead: 0.028710056676947815, \t0.04535429279332748\n",
      "76 steps ahead: 0.024365226624632252, \t0.060397943618784256\n",
      "77 steps ahead: 0.018925838152059393, \t0.06537303151178442\n",
      "78 steps ahead: 0.013500476190306032, \t0.05947725563168249\n",
      "79 steps ahead: 0.00997223087001975, \t0.0463054860566966\n",
      "80 steps ahead: 0.008065419982365651, \t0.029900018388778427\n",
      "81 steps ahead: 0.008101384359518127, \t0.01558908096377909\n",
      "82 steps ahead: 0.007918269518405796, \t0.005269015988572323\n",
      "83 steps ahead: 0.007360192586726155, \t0.0007197900923450185\n",
      "84 steps ahead: 0.006412615151043188, \t0.001484050723790431\n",
      "85 steps ahead: 0.004911696902157647, \t0.005173469836067368\n",
      "86 steps ahead: 0.0051469842565396995, \t0.01134324691174704\n",
      "87 steps ahead: 0.004007730621556571, \t0.014834503150216172\n",
      "88 steps ahead: 0.004323314992353056, \t0.017516959163631274\n",
      "89 steps ahead: 0.0041200872991846715, \t0.0174184532817947\n",
      "90 steps ahead: 0.004840960925404314, \t0.01646232589496388\n",
      "91 steps ahead: 0.005181154691028422, \t0.014049343614059406\n",
      "92 steps ahead: 0.005439482233147941, \t0.011136240982365653\n",
      "93 steps ahead: 0.004644477556441706, \t0.007254327984694164\n",
      "94 steps ahead: 0.001946048807764944, \t0.0018344897399018567\n",
      "95 steps ahead: 0.003346096103440055, \t0.0010140642952191081\n",
      "96 steps ahead: 0.003809040536719155, \t-0.00014637641497228238\n",
      "97 steps ahead: 0.004882283742467908, \t4.345984239639833e-05\n",
      "98 steps ahead: 0.005038226456135275, \t0.0002353774017683774\n",
      "99 steps ahead: 0.005629699955082668, \t0.0019101098641324255\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(paths.PATH)\n",
    "model1.eval()\n",
    "\n",
    "start = 10\n",
    "k = 100\n",
    "end= (start + k) if k != None else None\n",
    "\n",
    "model1.to('cpu')\n",
    "\n",
    "t_pred, t_real = metrics.r2_eval(model1, train_loader, k=end)\n",
    "v_pred, v_real, o2, l2 = metrics.r2_eval(model1, val_loader, filt=filt_loader, k=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 12899.304688\n",
      "Val MSE: 13317.976562\n",
      "(56320, 100)\n",
      "(56320, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train MSE: {:f}\".format(mean_squared_error(t_real, t_pred)))\n",
    "print(\"Val MSE: {:f}\".format(mean_squared_error(v_real, v_pred)))\n",
    "# print(\"Full MSE: {:f}\".format(mean_squared_error(f_real, f_pred)))\n",
    "# print(\"Burst MSE: {:f}\".format(mean_squared_error(b_real, b_pred)))\n",
    "print(t_real.shape)\n",
    "print(t_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, vp, tr, vr = t_pred, v_pred, t_real, v_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(train_loader))[0][:,2,0])\n",
    "# print(next(iter(burst_loader))[0][:,2,0])\n",
    "# t_pred = tp[:,29]\n",
    "# v_pred = vp[:,29]\n",
    "# t_real = tr[:,29]\n",
    "# v_real = vr[:,29]\n",
    "t_pred = tp[1,:]\n",
    "v_pred = vp[1,:]\n",
    "t_real = tr[1,:]\n",
    "v_real = vr[1,:]\n",
    "v_o2 =   o2[1,:]\n",
    "v_l2 =   l2[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d7b23c5f364f6cb936d5ea62cd68b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pnfr_training_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ed11864d6640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpnfr_training_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pnfr_training_loss' is not defined"
     ]
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(nrows=1, ncols=2)\n",
    "fig1.tight_layout()\n",
    "ax1[0].plot(range(params.EPOCHS), pnfr_training_loss)\n",
    "ax1[0].set_title('Training Loss')\n",
    "ax1[0].set_ylabel('Loss')\n",
    "ax1[0].set_xlabel('Epoch')\n",
    "\n",
    "ax1[1].plot(range(params.EPOCHS), pnfr_validation_loss)\n",
    "ax1[1].set_title('Validation Loss')\n",
    "ax1[1].set_ylabel('Loss')\n",
    "ax1[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf14e4e004074a319c59735fc981b112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1)\n",
    "fig.tight_layout()\n",
    "print(v_real.shape)\n",
    "\n",
    "end=99\n",
    "\n",
    "ax[0].plot(np.arange(0,end), t_real[:end], color='blue',label='Labels')\n",
    "ax[0].scatter(np.arange(0,end), t_pred[:end], color='slateblue',label='Training t+30')\n",
    "ax[0].set_title('Training LFPNet')\n",
    "ax[0].set_ylabel('LFP')\n",
    "ax[0].set_xlabel('Time')\n",
    "\n",
    "ax[1].plot(np.arange(0,end), v_real[0:end], color='blue',label='Labels')\n",
    "ax[1].scatter(np.arange(0,end), v_pred[:end], color='slateblue',label='Predicted t+30')\n",
    "ax[1].set_title('Validation LFPNet')\n",
    "ax[1].set_ylabel('LFP')\n",
    "ax[1].set_xlabel('Time')\n",
    "# ax[2,0].legend()\n",
    "\n",
    "ax[2].plot(np.arange(0,end), v_l2[:end], color='blue',label='Labels')\n",
    "ax[2].scatter(np.arange(0,end), v_o2[:end], color='slateblue',label='Training t+30')\n",
    "ax[2].set_title('Neuronal Component LFPNet')\n",
    "ax[2].set_ylabel('LFP')\n",
    "ax[2].set_xlabel('Time')\n",
    "\n",
    "\n",
    "ax[0].legend(loc=2, prop={'size': 10})\n",
    "\n",
    "# import plotly.tools as tls\n",
    "# plotly_fig = tls.mpl_to_plotly(fig)\n",
    "# plotly_fig.write_html(\"testfile.html\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.plot(np.arange(start,end), f_real[start:end], color='blue',label='Labels')\n",
    "ax.scatter(np.arange(start,end), f_pred[start:end], color='red',label='Training t+10')\n",
    "ax.set_title('Full LFP vs Time')\n",
    "ax.set_ylabel('Signal')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# ax[1].plot(np.arange(start,end), n_real[start:end], color='blue',label='Labels')\n",
    "# ax[1].scatter(np.arange(start,end), n_pred[start:end], color='red',label='Training t+10')\n",
    "# ax[1].set_title('Noise')\n",
    "# ax[1].set_ylabel('Signal')\n",
    "# ax[1].set_xlabel('Time')z\n",
    "\n",
    "# ax.plot(np.arange(start,end), s_real[start:end], color='blue',label='Labels')\n",
    "# ax.scatter(np.arange(start,end), s_pred[start:end], color='red',label='Training t+10')\n",
    "# ax.set_title('Sine')\n",
    "# ax.set_ylabel('LFP')\n",
    "# ax.set_xlabel('Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "\n",
    "STEPS = 100\n",
    "# model_initial = params.MODEL(params.INPUT_SIZE,params.HIDDEN_SIZE,params.OUTPUT_SIZE)\n",
    "model_final = copy.deepcopy(model1)\n",
    "\n",
    "\n",
    "# data that the evaluator will use when evaluating loss\n",
    "x, y = iter(noise_loader).__next__()\n",
    "metric = loss_landscapes.metrics.Loss(nn.MSELoss(), x, y)\n",
    "\n",
    "\n",
    "loss_data_fin = loss_landscapes.random_plane(model_final, metric, 10000, STEPS, normalization='model', deepcopy_model=True)\n",
    "# plt.contour(loss_data_fin, levels=50)\n",
    "# plt.title('Loss Contours around Trained Model')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "X = np.array([[j for j in range(STEPS)] for i in range(STEPS)])\n",
    "Y = np.array([[i for _ in range(STEPS)] for i in range(STEPS)])\n",
    "ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface Plot of Loss Landscape')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict()['conv_block.0.weight'])\n",
    "print(model1.state_dict()['conv_block.0.bias'])\n",
    "print(model1.state_dict()['conv_block.2.weight'])\n",
    "print(model1.state_dict()['conv_block.2.bias'])\n",
    "print(model1.state_dict()['ck1s1.weight'])\n",
    "print(model1.state_dict()['ck1s1.bias'])\n",
    "print(model1.state_dict()['fc1.weight'])\n",
    "print(model1.state_dict()['fc1.bias'])\n",
    "print(model1.state_dict()['fc2.weight'])\n",
    "print(model1.state_dict()['fc2.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
