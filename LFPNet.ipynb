{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git\n",
    "    paths.LOSS_FILE = COLAB_PRE + paths.LOSS_FILE\n",
    "    paths.PATH = COLAB_PRE + paths.PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import copy\n",
    "\n",
    "from utils import preprocess, metrics\n",
    "from config import params, paths\n",
    "from models import LFPNet\n",
    "\n",
    "s = 67\n",
    "\n",
    "rs = RandomState(MT19937(SeedSequence(s)))\n",
    "rng = default_rng(seed=s)\n",
    "torch.manual_seed(s)\n",
    "\n",
    "plt.rcParams.update({'font.size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,device):\n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "    \n",
    "#     feedback_arr = torch.zeros(params.BATCH_SIZE, 90)\n",
    "    \n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = .99995 #0.98 #decay the lr each step to 98% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                if params.RECURRENT_NET:\n",
    "                    x = torch.transpose(x, 2, 1)\n",
    "                x = x.to(device)\n",
    "                output = model(x)\n",
    "                y = y.to(device)\n",
    "#                 if i%100000 == 0 and epoch%5 == 0:\n",
    "#                     print(output)\n",
    "#                     print(y)\n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y)) \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "#                     if i%100000 == 0 and epoch%5 == 0:\n",
    "#                         print(model.cn1.weight.grad)\n",
    "#                         print(model.cn2.weight.grad)\n",
    "#                         print(model.fc1.weight.grad)\n",
    "#                         print(model.fc2.weight.grad)\n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if train_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = train_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    \n",
    "    loss_df.to_csv(paths.LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tr, f_va = preprocess.get_inVivo_LFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_tr, f_va, f_data = preprocess.get_filteredLFP()\n",
    "# f_tr, f_va, f_data = get_rawLFP()\n",
    "# f_tr, f_va, t_filt, v_filt = preprocess.get_end1D()#f_data, t_filt, v_filt, f_filt = get_end1D()\n",
    "\n",
    "# f_tr, f_va, f_data = preprocess.get_rawLFP()\n",
    "\n",
    "\n",
    "# noise = get_WN(channels=2)\n",
    "# sin = get_sin()\n",
    "\n",
    "# burst, fburst = preprocess.get_burstLFP()\n",
    "\n",
    "# Turn datasets into iterable dataloaders\n",
    "train_loader = DataLoader(dataset=f_tr,batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "# tfilt_loader = DataLoader(dataset=t_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=f_va,batch_size=params.BATCH_SIZE)\n",
    "# vfilt_loader = DataLoader(dataset=v_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "\n",
    "# full_loader = DataLoader(dataset=f_data,batch_size=params.BATCH_SIZE)\n",
    "\n",
    "# ffull_loader = DataLoader(dataset=f_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "# noise_loader = DataLoader(dataset=noise,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "# sine_loader = DataLoader(dataset=sin,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "\n",
    "# burst_loader = DataLoader(dataset=burst,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "# fburst_loader = DataLoader(dataset=fburst,params.BATCH_SIZE=params.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac699ac5ebd9459a90fd9259a78fd7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] train loss: 1961786.954102 val loss: 404837.899414\n",
      "[6,    30] train loss: 1962099.352539 val loss: 404837.890625\n",
      "[11,    30] train loss: 1962221.646484 val loss: 404837.936523\n",
      "[16,    30] train loss: 1961337.311523 val loss: 404838.006836\n",
      "[21,    30] train loss: 1962359.470703 val loss: 404838.078125\n",
      "[26,    30] train loss: 1962992.703125 val loss: 404838.076172\n",
      "[31,    30] train loss: 1963283.559570 val loss: 404838.029297\n",
      "[36,    30] train loss: 1962002.010742 val loss: 404838.104492\n",
      "[41,    30] train loss: 1962151.739258 val loss: 404838.161133\n",
      "[46,    30] train loss: 1962464.592773 val loss: 404838.223633\n",
      "[51,    30] train loss: 1961605.451172 val loss: 404838.331055\n",
      "[56,    30] train loss: 1961902.558594 val loss: 404838.447266\n",
      "[61,    30] train loss: 1961579.098633 val loss: 404838.558594\n",
      "[66,    30] train loss: 1961532.640625 val loss: 404838.734375\n",
      "[71,    30] train loss: 1963799.975586 val loss: 404838.818359\n",
      "[76,    30] train loss: 1962365.245117 val loss: 404838.979492\n",
      "[81,    30] train loss: 1962160.783203 val loss: 404839.084961\n",
      "[86,    30] train loss: 1962347.153320 val loss: 404839.206055\n",
      "[91,    30] train loss: 1961992.171875 val loss: 404839.351562\n",
      "[96,    30] train loss: 1962639.360352 val loss: 404839.424805\n",
      "[101,    30] train loss: 1961174.660156 val loss: 404839.583008\n",
      "[106,    30] train loss: 1962419.773438 val loss: 404839.687500\n",
      "[111,    30] train loss: 1962153.732422 val loss: 404839.755859\n",
      "[116,    30] train loss: 1961353.020508 val loss: 404839.780273\n",
      "[121,    30] train loss: 1961870.149414 val loss: 404839.830078\n",
      "[126,    30] train loss: 1962261.894531 val loss: 404839.792969\n",
      "[131,    30] train loss: 1962731.090820 val loss: 404839.823242\n",
      "[136,    30] train loss: 1961442.142578 val loss: 404839.805664\n",
      "[141,    30] train loss: 1961903.740234 val loss: 404839.784180\n",
      "[146,    30] train loss: 1962194.268555 val loss: 404839.786133\n",
      "[151,    30] train loss: 1962409.244141 val loss: 404839.797852\n",
      "[156,    30] train loss: 1962273.032227 val loss: 404839.791016\n",
      "[161,    30] train loss: 1961745.534180 val loss: 404839.756836\n",
      "[166,    30] train loss: 1961691.944336 val loss: 404839.842773\n",
      "[171,    30] train loss: 1961337.019531 val loss: 404839.731445\n",
      "[176,    30] train loss: 1962021.310547 val loss: 404839.711914\n",
      "[181,    30] train loss: 1963389.871094 val loss: 404839.792969\n",
      "[186,    30] train loss: 1961488.128906 val loss: 404839.828125\n",
      "[191,    30] train loss: 1961333.553711 val loss: 404839.882812\n",
      "[196,    30] train loss: 1961874.778320 val loss: 404839.909180\n",
      "[201,    30] train loss: 1961219.102539 val loss: 404840.044922\n",
      "[206,    30] train loss: 1961877.824219 val loss: 404839.949219\n",
      "[211,    30] train loss: 1961905.481445 val loss: 404840.026367\n",
      "[216,    30] train loss: 1962562.160156 val loss: 404840.133789\n",
      "[221,    30] train loss: 1961493.536133 val loss: 404840.007812\n",
      "[226,    30] train loss: 1962303.535156 val loss: 404840.068359\n",
      "[231,    30] train loss: 1961880.051758 val loss: 404840.060547\n",
      "[236,    30] train loss: 1962203.716797 val loss: 404840.089844\n",
      "[241,    30] train loss: 1961885.259766 val loss: 404840.147461\n",
      "[246,    30] train loss: 1962963.467773 val loss: 404840.381836\n",
      "[251,    30] train loss: 1961965.120117 val loss: 404841.054688\n",
      "[256,    30] train loss: 1961925.989258 val loss: 404839.807617\n",
      "[261,    30] train loss: 1963252.690430 val loss: 404840.573242\n",
      "[266,    30] train loss: 1962781.832031 val loss: 404840.498047\n",
      "[271,    30] train loss: 1962316.676758 val loss: 404840.752930\n",
      "[276,    30] train loss: 1962141.324219 val loss: 404840.878906\n",
      "[281,    30] train loss: 1961782.934570 val loss: 404841.073242\n",
      "[286,    30] train loss: 1961600.939453 val loss: 404840.836914\n",
      "[291,    30] train loss: 1962696.138672 val loss: 404840.825195\n",
      "[296,    30] train loss: 1961703.097656 val loss: 404841.626953\n",
      "[301,    30] train loss: 1961501.602539 val loss: 404841.887695\n",
      "[306,    30] train loss: 1961635.908203 val loss: 404841.916992\n",
      "[311,    30] train loss: 1962056.086914 val loss: 404841.907227\n",
      "[316,    30] train loss: 1962697.735352 val loss: 404842.375977\n",
      "[321,    30] train loss: 1962025.141602 val loss: 404842.680664\n",
      "[326,    30] train loss: 1962046.680664 val loss: 404842.975586\n",
      "[331,    30] train loss: 1961398.978516 val loss: 404843.311523\n",
      "[336,    30] train loss: 1961903.123047 val loss: 404843.101562\n",
      "[341,    30] train loss: 1961729.184570 val loss: 404844.285156\n",
      "[346,    30] train loss: 1961600.496094 val loss: 404844.409180\n",
      "[351,    30] train loss: 1962237.616211 val loss: 404844.555664\n",
      "[356,    30] train loss: 1961249.720703 val loss: 404844.316406\n",
      "[361,    30] train loss: 1962336.387695 val loss: 404844.311523\n",
      "[366,    30] train loss: 1962587.101562 val loss: 404844.333984\n",
      "[371,    30] train loss: 1962656.616211 val loss: 404846.153320\n",
      "[376,    30] train loss: 1962011.536133 val loss: 404845.394531\n",
      "[381,    30] train loss: 1961739.002930 val loss: 404845.408203\n",
      "[386,    30] train loss: 1961594.366211 val loss: 404845.443359\n",
      "[391,    30] train loss: 1962423.284180 val loss: 404846.335938\n",
      "[396,    30] train loss: 1961416.474609 val loss: 404846.175781\n",
      "[401,    30] train loss: 1962243.848633 val loss: 404847.310547\n",
      "[406,    30] train loss: 1963172.023438 val loss: 404846.675781\n",
      "[411,    30] train loss: 1961651.272461 val loss: 404846.544922\n",
      "[416,    30] train loss: 1961771.050781 val loss: 404847.253906\n",
      "[421,    30] train loss: 1961351.907227 val loss: 404847.198242\n",
      "[426,    30] train loss: 1962721.449219 val loss: 404846.323242\n",
      "[431,    30] train loss: 1962772.737305 val loss: 404846.904297\n",
      "[436,    30] train loss: 1961251.048828 val loss: 404847.616211\n",
      "[441,    30] train loss: 1961770.408203 val loss: 404848.226562\n",
      "[446,    30] train loss: 1962404.474609 val loss: 404846.336914\n",
      "[451,    30] train loss: 1961625.038086 val loss: 404847.310547\n",
      "[456,    30] train loss: 1962249.586914 val loss: 404849.012695\n",
      "[461,    30] train loss: 1961629.199219 val loss: 404847.389648\n",
      "[466,    30] train loss: 1962257.727539 val loss: 404848.549805\n",
      "[471,    30] train loss: 1962035.644531 val loss: 404847.650391\n",
      "[476,    30] train loss: 1961214.662109 val loss: 404848.015625\n",
      "[481,    30] train loss: 1962119.212891 val loss: 404847.611328\n",
      "[486,    30] train loss: 1961869.481445 val loss: 404847.893555\n",
      "[491,    30] train loss: 1962019.166016 val loss: 404847.677734\n",
      "[496,    30] train loss: 1961668.484375 val loss: 404847.597656\n",
      "[501,    30] train loss: 1962078.102539 val loss: 404847.691406\n",
      "[506,    30] train loss: 1961497.482422 val loss: 404846.825195\n",
      "[511,    30] train loss: 1962193.791992 val loss: 404846.958984\n",
      "[516,    30] train loss: 1962448.910156 val loss: 404846.534180\n",
      "[521,    30] train loss: 1961622.133789 val loss: 404844.733398\n",
      "[526,    30] train loss: 1962656.320312 val loss: 404845.505859\n",
      "[531,    30] train loss: 1962340.900391 val loss: 404844.773438\n",
      "[536,    30] train loss: 1961813.633789 val loss: 404843.924805\n",
      "[541,    30] train loss: 1961232.581055 val loss: 404843.291016\n",
      "[546,    30] train loss: 1961817.186523 val loss: 404841.763672\n",
      "[551,    30] train loss: 1962059.727539 val loss: 404842.332031\n",
      "[556,    30] train loss: 1962695.673828 val loss: 404841.979492\n",
      "[561,    30] train loss: 1961230.548828 val loss: 404841.416016\n",
      "[566,    30] train loss: 1960755.206055 val loss: 404840.574219\n",
      "[571,    30] train loss: 1962023.730469 val loss: 404839.913086\n",
      "[576,    30] train loss: 1962262.603516 val loss: 404839.985352\n",
      "[581,    30] train loss: 1961335.921875 val loss: 404839.689453\n",
      "[586,    30] train loss: 1961553.293945 val loss: 404838.785156\n",
      "[591,    30] train loss: 1962360.240234 val loss: 404838.761719\n",
      "[596,    30] train loss: 1961797.958984 val loss: 404837.903320\n",
      "[601,    30] train loss: 1961575.315430 val loss: 404836.892578\n",
      "[606,    30] train loss: 1961820.936523 val loss: 404835.329102\n",
      "[611,    30] train loss: 1961955.081055 val loss: 404833.640625\n",
      "[616,    30] train loss: 1961671.770508 val loss: 404833.337891\n",
      "[621,    30] train loss: 1962464.543945 val loss: 404833.197266\n",
      "[626,    30] train loss: 1961195.691406 val loss: 404832.907227\n",
      "[631,    30] train loss: 1961810.536133 val loss: 404832.060547\n",
      "[636,    30] train loss: 1961658.767578 val loss: 404830.873047\n",
      "[641,    30] train loss: 1961096.018555 val loss: 404828.055664\n",
      "[646,    30] train loss: 1962466.362305 val loss: 404827.950195\n",
      "[651,    30] train loss: 1961875.588867 val loss: 404829.406250\n",
      "[656,    30] train loss: 1961146.781250 val loss: 404827.336914\n",
      "[661,    30] train loss: 1961119.905273 val loss: 404827.547852\n",
      "[666,    30] train loss: 1962057.934570 val loss: 404826.331055\n",
      "[671,    30] train loss: 1962885.840820 val loss: 404824.536133\n",
      "[676,    30] train loss: 1961384.094727 val loss: 404823.991211\n",
      "[681,    30] train loss: 1962132.609375 val loss: 404825.649414\n",
      "[686,    30] train loss: 1961038.767578 val loss: 404824.283203\n",
      "[691,    30] train loss: 1961724.249023 val loss: 404824.004883\n",
      "[696,    30] train loss: 1961610.587891 val loss: 404820.812500\n",
      "[701,    30] train loss: 1962120.391602 val loss: 404823.058594\n",
      "[706,    30] train loss: 1961815.528320 val loss: 404821.735352\n",
      "[711,    30] train loss: 1961228.847656 val loss: 404819.725586\n",
      "[716,    30] train loss: 1961796.471680 val loss: 404819.561523\n",
      "[721,    30] train loss: 1961675.398438 val loss: 404820.647461\n",
      "[726,    30] train loss: 1961667.082031 val loss: 404819.221680\n",
      "[731,    30] train loss: 1962130.073242 val loss: 404818.260742\n",
      "[736,    30] train loss: 1961221.687500 val loss: 404818.322266\n",
      "[741,    30] train loss: 1961251.470703 val loss: 404817.731445\n",
      "[746,    30] train loss: 1960981.983398 val loss: 404817.427734\n",
      "[751,    30] train loss: 1961792.629883 val loss: 404818.639648\n",
      "[756,    30] train loss: 1961876.545898 val loss: 404816.846680\n",
      "[761,    30] train loss: 1963236.725586 val loss: 404815.833008\n",
      "[766,    30] train loss: 1961242.200195 val loss: 404816.484375\n",
      "[771,    30] train loss: 1961427.305664 val loss: 404814.249023\n",
      "[776,    30] train loss: 1962061.078125 val loss: 404814.558594\n",
      "[781,    30] train loss: 1962223.814453 val loss: 404814.767578\n",
      "[786,    30] train loss: 1961919.580078 val loss: 404813.530273\n",
      "[791,    30] train loss: 1960931.466797 val loss: 404814.015625\n",
      "[796,    30] train loss: 1961650.713867 val loss: 404812.556641\n",
      "[801,    30] train loss: 1961999.546875 val loss: 404811.691406\n",
      "[806,    30] train loss: 1961107.503906 val loss: 404813.101562\n",
      "[811,    30] train loss: 1961863.263672 val loss: 404812.003906\n",
      "[816,    30] train loss: 1961124.497070 val loss: 404812.280273\n",
      "[821,    30] train loss: 1962028.407227 val loss: 404811.185547\n",
      "[826,    30] train loss: 1961402.284180 val loss: 404810.804688\n",
      "[831,    30] train loss: 1961915.836914 val loss: 404809.302734\n",
      "[836,    30] train loss: 1961388.785156 val loss: 404809.316406\n",
      "[841,    30] train loss: 1961062.775391 val loss: 404807.765625\n",
      "[846,    30] train loss: 1961474.045898 val loss: 404807.257812\n",
      "[851,    30] train loss: 1961865.425781 val loss: 404807.432617\n",
      "[856,    30] train loss: 1961888.978516 val loss: 404807.186523\n",
      "[861,    30] train loss: 1961767.790039 val loss: 404806.787109\n",
      "[866,    30] train loss: 1962204.445312 val loss: 404806.195312\n",
      "[871,    30] train loss: 1962481.831055 val loss: 404805.428711\n",
      "[876,    30] train loss: 1961506.024414 val loss: 404806.678711\n",
      "[881,    30] train loss: 1961477.274414 val loss: 404805.046875\n",
      "[886,    30] train loss: 1960990.939453 val loss: 404806.147461\n",
      "[891,    30] train loss: 1961649.990234 val loss: 404807.508789\n",
      "[896,    30] train loss: 1961754.374023 val loss: 404805.853516\n",
      "[901,    30] train loss: 1960888.896484 val loss: 404805.027344\n",
      "[906,    30] train loss: 1961578.301758 val loss: 404806.736328\n",
      "[911,    30] train loss: 1961218.939453 val loss: 404806.711914\n",
      "[916,    30] train loss: 1961277.994141 val loss: 404805.563477\n",
      "[921,    30] train loss: 1960933.728516 val loss: 404803.851562\n",
      "[926,    30] train loss: 1961807.313477 val loss: 404803.199219\n",
      "[931,    30] train loss: 1961809.591797 val loss: 404804.191406\n",
      "[936,    30] train loss: 1962369.524414 val loss: 404803.654297\n",
      "[941,    30] train loss: 1961527.937500 val loss: 404802.381836\n",
      "[946,    30] train loss: 1961745.874023 val loss: 404802.346680\n",
      "[951,    30] train loss: 1961752.514648 val loss: 404800.762695\n",
      "[956,    30] train loss: 1961995.563477 val loss: 404801.608398\n",
      "[961,    30] train loss: 1962038.932617 val loss: 404802.699219\n",
      "[966,    30] train loss: 1961365.345703 val loss: 404801.821289\n",
      "[971,    30] train loss: 1961668.427734 val loss: 404801.775391\n",
      "[976,    30] train loss: 1962383.760742 val loss: 404802.363281\n",
      "[981,    30] train loss: 1961392.444336 val loss: 404799.604492\n",
      "[986,    30] train loss: 1962907.536133 val loss: 404800.308594\n",
      "[991,    30] train loss: 1961452.806641 val loss: 404799.884766\n",
      "[996,    30] train loss: 1961137.140625 val loss: 404800.207031\n",
      "[1001,    30] train loss: 1960845.433594 val loss: 404800.055664\n",
      "[1006,    30] train loss: 1961681.931641 val loss: 404799.291016\n",
      "[1011,    30] train loss: 1961082.528320 val loss: 404799.614258\n",
      "[1016,    30] train loss: 1961150.778320 val loss: 404799.211914\n",
      "[1021,    30] train loss: 1962046.675781 val loss: 404797.588867\n",
      "[1026,    30] train loss: 1961671.362305 val loss: 404799.226562\n",
      "[1031,    30] train loss: 1961600.615234 val loss: 404799.260742\n",
      "[1036,    30] train loss: 1960951.507812 val loss: 404801.597656\n",
      "[1041,    30] train loss: 1961987.639648 val loss: 404799.157227\n",
      "[1046,    30] train loss: 1961072.925781 val loss: 404800.381836\n",
      "[1051,    30] train loss: 1961844.305664 val loss: 404800.218750\n",
      "[1056,    30] train loss: 1962145.416992 val loss: 404800.640625\n",
      "[1061,    30] train loss: 1961976.865234 val loss: 404799.962891\n",
      "[1066,    30] train loss: 1961903.015625 val loss: 404800.074219\n",
      "[1071,    30] train loss: 1961385.220703 val loss: 404800.026367\n",
      "[1076,    30] train loss: 1961443.887695 val loss: 404801.113281\n",
      "[1081,    30] train loss: 1962274.750977 val loss: 404797.104492\n",
      "[1086,    30] train loss: 1961223.234375 val loss: 404798.327148\n",
      "[1091,    30] train loss: 1960576.617188 val loss: 404798.734375\n",
      "[1096,    30] train loss: 1961376.517578 val loss: 404798.055664\n",
      "[1101,    30] train loss: 1961668.083008 val loss: 404799.236328\n",
      "[1106,    30] train loss: 1961205.404297 val loss: 404797.410156\n",
      "[1111,    30] train loss: 1961263.810547 val loss: 404797.639648\n",
      "[1116,    30] train loss: 1961482.429688 val loss: 404800.890625\n",
      "[1121,    30] train loss: 1961570.124023 val loss: 404800.154297\n",
      "[1126,    30] train loss: 1961200.963867 val loss: 404799.024414\n",
      "[1131,    30] train loss: 1961803.303711 val loss: 404799.988281\n",
      "[1136,    30] train loss: 1962245.755859 val loss: 404799.602539\n",
      "[1141,    30] train loss: 1961459.266602 val loss: 404799.397461\n",
      "[1146,    30] train loss: 1962818.572266 val loss: 404800.209961\n",
      "[1151,    30] train loss: 1961278.023438 val loss: 404799.002930\n",
      "[1156,    30] train loss: 1961437.540039 val loss: 404800.333984\n",
      "[1161,    30] train loss: 1962048.870117 val loss: 404798.266602\n",
      "[1166,    30] train loss: 1961201.031250 val loss: 404798.088867\n",
      "[1171,    30] train loss: 1962036.108398 val loss: 404798.858398\n",
      "[1176,    30] train loss: 1961021.873047 val loss: 404797.939453\n",
      "[1181,    30] train loss: 1961430.839844 val loss: 404798.919922\n",
      "[1186,    30] train loss: 1960385.165039 val loss: 404798.231445\n",
      "[1191,    30] train loss: 1960681.432617 val loss: 404797.393555\n",
      "[1196,    30] train loss: 1961360.536133 val loss: 404796.896484\n",
      "[1201,    30] train loss: 1960958.509766 val loss: 404796.712891\n",
      "[1206,    30] train loss: 1961652.035156 val loss: 404798.276367\n",
      "[1211,    30] train loss: 1962330.496094 val loss: 404797.945312\n",
      "[1216,    30] train loss: 1961142.717773 val loss: 404797.431641\n",
      "[1221,    30] train loss: 1960504.641602 val loss: 404797.051758\n",
      "[1226,    30] train loss: 1961822.557617 val loss: 404796.420898\n",
      "[1231,    30] train loss: 1961149.781250 val loss: 404795.090820\n",
      "[1236,    30] train loss: 1962474.449219 val loss: 404795.578125\n",
      "[1241,    30] train loss: 1960933.125000 val loss: 404795.252930\n",
      "[1246,    30] train loss: 1960672.413086 val loss: 404795.041992\n",
      "[1251,    30] train loss: 1960875.352539 val loss: 404794.895508\n",
      "[1256,    30] train loss: 1960812.820312 val loss: 404794.042969\n",
      "[1261,    30] train loss: 1961624.772461 val loss: 404795.244141\n",
      "[1266,    30] train loss: 1961199.670898 val loss: 404793.131836\n",
      "[1271,    30] train loss: 1962216.095703 val loss: 404794.940430\n",
      "[1276,    30] train loss: 1961641.001953 val loss: 404795.732422\n",
      "[1281,    30] train loss: 1961070.973633 val loss: 404796.115234\n",
      "[1286,    30] train loss: 1962402.719727 val loss: 404794.962891\n",
      "[1291,    30] train loss: 1961797.291016 val loss: 404797.282227\n",
      "[1296,    30] train loss: 1961867.871094 val loss: 404798.607422\n",
      "[1301,    30] train loss: 1960648.140625 val loss: 404796.512695\n",
      "[1306,    30] train loss: 1960949.851562 val loss: 404797.989258\n",
      "[1311,    30] train loss: 1961032.789062 val loss: 404797.573242\n",
      "[1316,    30] train loss: 1961741.511719 val loss: 404798.126953\n",
      "[1321,    30] train loss: 1961423.739258 val loss: 404795.775391\n",
      "[1326,    30] train loss: 1961783.140625 val loss: 404795.433594\n",
      "[1331,    30] train loss: 1961236.992188 val loss: 404796.873047\n",
      "[1336,    30] train loss: 1961773.233398 val loss: 404797.240234\n",
      "[1341,    30] train loss: 1961199.866211 val loss: 404795.589844\n",
      "[1346,    30] train loss: 1961492.408203 val loss: 404796.000977\n",
      "[1351,    30] train loss: 1961422.743164 val loss: 404797.539062\n",
      "[1356,    30] train loss: 1960690.979492 val loss: 404798.474609\n",
      "[1361,    30] train loss: 1961816.022461 val loss: 404797.188477\n",
      "[1366,    30] train loss: 1961663.512695 val loss: 404798.332031\n",
      "[1371,    30] train loss: 1963314.402344 val loss: 404799.053711\n",
      "[1376,    30] train loss: 1961128.856445 val loss: 404798.861328\n",
      "[1381,    30] train loss: 1961516.259766 val loss: 404797.002930\n",
      "[1386,    30] train loss: 1962156.790039 val loss: 404798.068359\n",
      "[1391,    30] train loss: 1961059.945312 val loss: 404798.161133\n",
      "[1396,    30] train loss: 1960795.864258 val loss: 404798.678711\n",
      "[1401,    30] train loss: 1961256.355469 val loss: 404797.912109\n",
      "[1406,    30] train loss: 1962014.018555 val loss: 404798.010742\n",
      "[1411,    30] train loss: 1961909.247070 val loss: 404796.568359\n",
      "[1416,    30] train loss: 1961440.042969 val loss: 404797.091797\n",
      "[1421,    30] train loss: 1961094.459961 val loss: 404794.543945\n",
      "[1426,    30] train loss: 1961600.702148 val loss: 404796.387695\n",
      "[1431,    30] train loss: 1961617.908203 val loss: 404794.375977\n",
      "[1436,    30] train loss: 1960922.175781 val loss: 404794.114258\n",
      "[1441,    30] train loss: 1961657.776367 val loss: 404793.577148\n",
      "[1446,    30] train loss: 1960989.412109 val loss: 404792.653320\n",
      "[1451,    30] train loss: 1961283.343750 val loss: 404793.779297\n",
      "[1456,    30] train loss: 1960965.796875 val loss: 404794.241211\n",
      "[1461,    30] train loss: 1961515.274414 val loss: 404792.979492\n",
      "[1466,    30] train loss: 1960697.556641 val loss: 404792.861328\n",
      "[1471,    30] train loss: 1961365.627930 val loss: 404792.456055\n",
      "[1476,    30] train loss: 1961461.928711 val loss: 404793.879883\n",
      "[1481,    30] train loss: 1961113.119141 val loss: 404794.696289\n",
      "[1486,    30] train loss: 1961089.457031 val loss: 404793.506836\n",
      "[1491,    30] train loss: 1961420.897461 val loss: 404792.948242\n",
      "[1496,    30] train loss: 1961140.510742 val loss: 404794.015625\n",
      "[1501,    30] train loss: 1960445.592773 val loss: 404795.428711\n",
      "[1506,    30] train loss: 1960927.853516 val loss: 404795.294922\n",
      "[1511,    30] train loss: 1961928.403320 val loss: 404795.383789\n",
      "[1516,    30] train loss: 1961015.497070 val loss: 404794.983398\n",
      "[1521,    30] train loss: 1961120.557617 val loss: 404792.090820\n",
      "[1526,    30] train loss: 1962013.604492 val loss: 404792.768555\n",
      "[1531,    30] train loss: 1960959.070312 val loss: 404792.213867\n",
      "[1536,    30] train loss: 1961404.580078 val loss: 404791.579102\n",
      "[1541,    30] train loss: 1961162.666992 val loss: 404790.169922\n",
      "[1546,    30] train loss: 1961962.207031 val loss: 404790.428711\n",
      "[1551,    30] train loss: 1960922.036133 val loss: 404790.710938\n",
      "[1556,    30] train loss: 1961643.546875 val loss: 404791.793945\n",
      "[1561,    30] train loss: 1961298.526367 val loss: 404792.250000\n",
      "[1566,    30] train loss: 1961700.901367 val loss: 404790.804688\n",
      "[1571,    30] train loss: 1961540.258789 val loss: 404790.878906\n",
      "[1576,    30] train loss: 1961632.147461 val loss: 404790.565430\n",
      "[1581,    30] train loss: 1960760.141602 val loss: 404790.702148\n",
      "[1586,    30] train loss: 1960867.346680 val loss: 404792.110352\n",
      "[1591,    30] train loss: 1962944.502930 val loss: 404790.896484\n",
      "[1596,    30] train loss: 1961212.926758 val loss: 404791.864258\n",
      "[1601,    30] train loss: 1961690.837891 val loss: 404792.677734\n",
      "[1606,    30] train loss: 1961600.766602 val loss: 404791.703125\n",
      "[1611,    30] train loss: 1961489.451172 val loss: 404791.679688\n",
      "[1616,    30] train loss: 1961296.595703 val loss: 404792.278320\n",
      "[1621,    30] train loss: 1962405.761719 val loss: 404790.610352\n",
      "[1626,    30] train loss: 1961351.775391 val loss: 404788.657227\n",
      "[1631,    30] train loss: 1961280.535156 val loss: 404791.621094\n",
      "[1636,    30] train loss: 1961459.357422 val loss: 404790.353516\n",
      "[1641,    30] train loss: 1961654.138672 val loss: 404792.387695\n",
      "[1646,    30] train loss: 1961536.926758 val loss: 404792.104492\n",
      "[1651,    30] train loss: 1960901.402344 val loss: 404793.943359\n",
      "[1656,    30] train loss: 1961595.299805 val loss: 404793.041992\n",
      "[1661,    30] train loss: 1961968.540039 val loss: 404792.896484\n",
      "[1666,    30] train loss: 1961599.456055 val loss: 404793.037109\n",
      "[1671,    30] train loss: 1960386.718750 val loss: 404792.345703\n",
      "[1676,    30] train loss: 1961034.068359 val loss: 404792.952148\n",
      "[1681,    30] train loss: 1961703.293945 val loss: 404793.485352\n",
      "[1686,    30] train loss: 1961860.041016 val loss: 404794.007812\n",
      "[1691,    30] train loss: 1962990.013672 val loss: 404790.505859\n",
      "[1696,    30] train loss: 1962200.641602 val loss: 404791.577148\n",
      "[1701,    30] train loss: 1960944.026367 val loss: 404792.233398\n",
      "[1706,    30] train loss: 1960566.714844 val loss: 404793.010742\n",
      "[1711,    30] train loss: 1961658.134766 val loss: 404791.741211\n",
      "[1716,    30] train loss: 1961233.145508 val loss: 404791.815430\n",
      "[1721,    30] train loss: 1961598.902344 val loss: 404792.872070\n",
      "[1726,    30] train loss: 1961007.729492 val loss: 404792.347656\n",
      "[1731,    30] train loss: 1962720.732422 val loss: 404792.177734\n",
      "[1736,    30] train loss: 1960974.969727 val loss: 404791.618164\n",
      "[1741,    30] train loss: 1961446.934570 val loss: 404792.103516\n",
      "[1746,    30] train loss: 1961383.243164 val loss: 404793.039062\n",
      "[1751,    30] train loss: 1961204.517578 val loss: 404790.755859\n",
      "[1756,    30] train loss: 1961375.343750 val loss: 404791.375977\n",
      "[1761,    30] train loss: 1960876.640625 val loss: 404791.819336\n",
      "[1766,    30] train loss: 1961083.874023 val loss: 404792.709961\n",
      "[1771,    30] train loss: 1961047.474609 val loss: 404793.195312\n",
      "[1776,    30] train loss: 1961368.652344 val loss: 404792.475586\n",
      "[1781,    30] train loss: 1961531.583008 val loss: 404792.934570\n",
      "[1786,    30] train loss: 1960655.835938 val loss: 404792.484375\n",
      "[1791,    30] train loss: 1960899.489258 val loss: 404792.247070\n",
      "[1796,    30] train loss: 1961387.698242 val loss: 404793.212891\n",
      "[1801,    30] train loss: 1960290.788086 val loss: 404795.571289\n",
      "[1806,    30] train loss: 1961509.776367 val loss: 404795.381836\n",
      "[1811,    30] train loss: 1961149.057617 val loss: 404793.999023\n",
      "[1816,    30] train loss: 1961646.289062 val loss: 404791.053711\n",
      "[1821,    30] train loss: 1961449.961914 val loss: 404792.762695\n",
      "[1826,    30] train loss: 1960817.665039 val loss: 404792.365234\n",
      "[1831,    30] train loss: 1961148.635742 val loss: 404794.490234\n",
      "[1836,    30] train loss: 1961680.245117 val loss: 404792.938477\n",
      "[1841,    30] train loss: 1961472.434570 val loss: 404793.225586\n",
      "[1846,    30] train loss: 1960882.323242 val loss: 404794.670898\n",
      "[1851,    30] train loss: 1961245.011719 val loss: 404794.449219\n",
      "[1856,    30] train loss: 1961170.193359 val loss: 404794.726562\n",
      "[1861,    30] train loss: 1961702.228516 val loss: 404795.768555\n",
      "[1866,    30] train loss: 1960613.206055 val loss: 404794.783203\n",
      "[1871,    30] train loss: 1961276.371094 val loss: 404793.993164\n",
      "[1876,    30] train loss: 1961017.842773 val loss: 404793.991211\n",
      "[1881,    30] train loss: 1960384.078125 val loss: 404794.162109\n",
      "[1886,    30] train loss: 1962913.284180 val loss: 404793.015625\n",
      "[1891,    30] train loss: 1960069.325195 val loss: 404792.971680\n",
      "[1896,    30] train loss: 1961970.296875 val loss: 404793.289062\n",
      "[1901,    30] train loss: 1960870.176758 val loss: 404795.241211\n",
      "[1906,    30] train loss: 1962088.213867 val loss: 404794.956055\n",
      "[1911,    30] train loss: 1960639.504883 val loss: 404795.779297\n",
      "[1916,    30] train loss: 1960674.795898 val loss: 404795.500000\n",
      "[1921,    30] train loss: 1961483.171875 val loss: 404794.748047\n",
      "[1926,    30] train loss: 1960902.341797 val loss: 404794.829102\n",
      "[1931,    30] train loss: 1961769.269531 val loss: 404794.593750\n",
      "[1936,    30] train loss: 1960744.872070 val loss: 404795.163086\n",
      "[1941,    30] train loss: 1961749.237305 val loss: 404792.668945\n",
      "[1946,    30] train loss: 1961512.649414 val loss: 404792.481445\n",
      "[1951,    30] train loss: 1960605.506836 val loss: 404793.290039\n",
      "[1956,    30] train loss: 1961412.296875 val loss: 404791.617188\n",
      "[1961,    30] train loss: 1960897.985352 val loss: 404792.683594\n",
      "[1966,    30] train loss: 1961225.134766 val loss: 404793.412109\n",
      "[1971,    30] train loss: 1961552.839844 val loss: 404792.818359\n",
      "[1976,    30] train loss: 1960723.831055 val loss: 404791.493164\n",
      "[1981,    30] train loss: 1961237.307617 val loss: 404792.456055\n",
      "[1986,    30] train loss: 1961334.460938 val loss: 404792.338867\n",
      "[1991,    30] train loss: 1960196.803711 val loss: 404792.639648\n",
      "[1996,    30] train loss: 1961133.308594 val loss: 404791.614258\n"
     ]
    }
   ],
   "source": [
    "model1 = params.MODEL(\n",
    "    in_size=params.INPUT_SIZE,\n",
    "    h_size=params.HIDDEN_SIZE,\n",
    "    out_size=params.OUTPUT_SIZE,\n",
    "    num_layers=params.NUM_LAYERS,\n",
    "    dropout=params.DROPOUT\n",
    ")\n",
    "# model1 = torch.load(paths.PATH)\n",
    "model_initial = copy.deepcopy(model1)\n",
    "model1 = torch.load(paths.PATH)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "# for name, param in model1.named_parameters():\n",
    "#     print(name)\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# model1.rnn.weight_ih_l0.requires_grad = True\n",
    "# model1.rnn.weight_hh_l0.requires_grad = True\n",
    "# model1.rnn.bias_ih_l0.requires_grad = True\n",
    "# model1.rnn.bias_hh_l0.requires_grad = True\n",
    "\n",
    "# model1.convs2k3.weight.requires_grad = True\n",
    "# model1.convs2k3.bias.requires_grad = True\n",
    "# model1.dilation.weight.requires_grad = True\n",
    "# model1.dilation.bias.requires_grad = True\n",
    "# model1.convs1k5.weight.requires_grad = True\n",
    "# model1.convs1k5.bias.requires_grad = True\n",
    "# model1.convs1k3.weight.requires_grad = True\n",
    "# model1.convs1k3.bias.requires_grad = True\n",
    "\n",
    "pnfr_training_loss, pnfr_validation_loss = train_model(model1,paths.PATH,train_loader,\n",
    "                                                       val_loader,params.EPOCHS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps ahead: 0.0027074221034542045\n",
      "1 steps ahead: 0.0014055503028966854\n",
      "2 steps ahead: 0.0012890524325357422\n",
      "3 steps ahead: 0.001508242681651062\n",
      "4 steps ahead: 0.002116624303294734\n",
      "5 steps ahead: 0.0018095790197854011\n",
      "6 steps ahead: 0.0011276594752409608\n",
      "7 steps ahead: 0.0010204731260083921\n",
      "8 steps ahead: 9.640135963517871e-05\n",
      "9 steps ahead: 0.0011591916709925432\n",
      "10 steps ahead: 0.0011572250298236275\n",
      "11 steps ahead: 0.0009871119187754296\n",
      "12 steps ahead: 0.001257286724797635\n",
      "13 steps ahead: 0.0008790048060551836\n",
      "14 steps ahead: 0.0005850272751666274\n",
      "15 steps ahead: 0.0008562927556621247\n",
      "16 steps ahead: 0.000520222012232896\n",
      "17 steps ahead: 0.0007878110056485088\n",
      "18 steps ahead: 0.0008433171023330965\n",
      "19 steps ahead: 0.001037725971833936\n",
      "20 steps ahead: 0.0014705693064736014\n",
      "21 steps ahead: 0.0008067826532381916\n",
      "22 steps ahead: 0.0009920125380163958\n",
      "23 steps ahead: 0.0006408570856925033\n",
      "24 steps ahead: 0.0011101875271781747\n",
      "25 steps ahead: 8.671609777144429e-05\n",
      "26 steps ahead: 0.000958167495143214\n",
      "27 steps ahead: 0.0001818071420166545\n",
      "28 steps ahead: 0.0009313569609792349\n",
      "29 steps ahead: 0.0001010753107697493\n",
      "30 steps ahead: 0.00028418051472067773\n",
      "31 steps ahead: 0.0008359939052188237\n",
      "32 steps ahead: 0.000697021010583665\n",
      "33 steps ahead: 0.000581445303511452\n",
      "34 steps ahead: 0.0009653507916476123\n",
      "35 steps ahead: 0.0011097482907515\n",
      "36 steps ahead: 0.0007489903757719185\n",
      "37 steps ahead: 0.000967758708280031\n",
      "38 steps ahead: 0.0010136971980075904\n",
      "39 steps ahead: 0.0012789261994240464\n",
      "40 steps ahead: 0.0007623658295421443\n",
      "41 steps ahead: -9.222914170425867e-05\n",
      "42 steps ahead: 0.00016115814084105562\n",
      "43 steps ahead: 0.00047812290310156325\n",
      "44 steps ahead: 0.0010057176537212609\n",
      "45 steps ahead: 0.0008703555725332324\n",
      "46 steps ahead: 0.0010030095171228437\n",
      "47 steps ahead: 0.0009271005384170472\n",
      "48 steps ahead: 0.0012805566000099189\n",
      "49 steps ahead: 0.00022447537380521076\n",
      "50 steps ahead: 0.0009341778720260985\n",
      "51 steps ahead: 5.29748851630929e-05\n",
      "52 steps ahead: 0.0007255341448134667\n",
      "53 steps ahead: 0.00014159296056515114\n",
      "54 steps ahead: 0.0009215017951932358\n",
      "55 steps ahead: 2.9709767635832662e-05\n",
      "56 steps ahead: 0.0010969136293021986\n",
      "57 steps ahead: 0.0012289502222900817\n",
      "58 steps ahead: 6.734302109723256e-05\n",
      "59 steps ahead: 3.812232350230982e-05\n",
      "60 steps ahead: 0.0006541537357622751\n",
      "61 steps ahead: 0.0015803254747305262\n",
      "62 steps ahead: 0.0009431004030797663\n",
      "63 steps ahead: 0.000729507024687992\n",
      "64 steps ahead: 0.0012291977871911053\n",
      "65 steps ahead: 0.0012865995418771536\n",
      "66 steps ahead: 0.00048268932345452153\n",
      "67 steps ahead: 0.0005895570781265791\n",
      "68 steps ahead: -4.028971143643645e-05\n",
      "69 steps ahead: 0.0008511592342836938\n",
      "70 steps ahead: 0.001381002098973294\n",
      "71 steps ahead: 0.0012120362559729925\n",
      "72 steps ahead: 0.0006407391675898877\n",
      "73 steps ahead: 0.0012183662752525137\n",
      "74 steps ahead: 0.00023333448319839967\n",
      "75 steps ahead: 0.0004669538821278074\n",
      "76 steps ahead: 0.0007800536679828385\n",
      "77 steps ahead: 0.0011810024009922815\n",
      "78 steps ahead: 0.0014015321401041891\n",
      "79 steps ahead: 0.0009917012826834437\n",
      "80 steps ahead: 0.0003454115393083379\n",
      "81 steps ahead: 0.0014560939987137145\n",
      "82 steps ahead: 0.001241909262076013\n",
      "83 steps ahead: -4.34719640887149e-05\n",
      "84 steps ahead: 0.00047831933728159637\n",
      "85 steps ahead: 0.000219218565021162\n",
      "86 steps ahead: 0.001027578536250795\n",
      "87 steps ahead: 0.0014686153090051057\n",
      "88 steps ahead: 0.0015213337249161407\n",
      "89 steps ahead: 0.0006924067850233584\n",
      "90 steps ahead: 0.0007608462986925524\n",
      "91 steps ahead: 0.0014682795627893874\n",
      "92 steps ahead: 0.0012049148558117562\n",
      "93 steps ahead: 0.0002983409197507836\n",
      "94 steps ahead: 0.00043633469921222634\n",
      "95 steps ahead: 0.00044317761338097394\n",
      "96 steps ahead: 0.0005699524530922151\n",
      "97 steps ahead: 0.0009355982436402011\n",
      "98 steps ahead: 0.0007718916990478997\n",
      "99 steps ahead: 0.0001668464549392512\n",
      "0 steps ahead: 0.002233922749163675\n",
      "1 steps ahead: 0.0009935709169749396\n",
      "2 steps ahead: 0.00047591693575543825\n",
      "3 steps ahead: 0.0014979735473691358\n",
      "4 steps ahead: 0.0009283521557583718\n",
      "5 steps ahead: 0.0007228288027845142\n",
      "6 steps ahead: 0.00014810448166269197\n",
      "7 steps ahead: 0.00041680147729572514\n",
      "8 steps ahead: 7.543467004178517e-05\n",
      "9 steps ahead: 0.0003664933298169748\n",
      "10 steps ahead: 0.0006409550978236922\n",
      "11 steps ahead: 0.0006007173046594927\n",
      "12 steps ahead: -1.523769205480363e-06\n",
      "13 steps ahead: 1.3151854309723099e-05\n",
      "14 steps ahead: 0.00012715139897567784\n",
      "15 steps ahead: 0.0004050264977867313\n",
      "16 steps ahead: -0.00020199839511580997\n",
      "17 steps ahead: 0.00021006538487444715\n",
      "18 steps ahead: 0.0006138141726753643\n",
      "19 steps ahead: 0.00030040425136856896\n",
      "20 steps ahead: 0.00014835535954915713\n",
      "21 steps ahead: -0.0005240676686657508\n",
      "22 steps ahead: 0.00011239404216345505\n",
      "23 steps ahead: -7.070796468955365e-05\n",
      "24 steps ahead: -0.00023445334488525482\n",
      "25 steps ahead: -8.288912500953316e-05\n",
      "26 steps ahead: -4.286248972440454e-05\n",
      "27 steps ahead: 2.3098848746960776e-05\n",
      "28 steps ahead: -0.00037433542580478374\n",
      "29 steps ahead: -0.0003843838392327825\n",
      "30 steps ahead: -0.0001270500831125343\n",
      "31 steps ahead: 0.00010693057865929223\n",
      "32 steps ahead: -0.00013468169515484796\n",
      "33 steps ahead: -0.00018304554828318942\n",
      "34 steps ahead: 0.00010837142728636895\n",
      "35 steps ahead: 0.0003410883185425151\n",
      "36 steps ahead: -0.00023200394347799858\n",
      "37 steps ahead: -0.0001739416511914449\n",
      "38 steps ahead: -0.00029719142001516374\n",
      "39 steps ahead: -0.00018151515720155054\n",
      "40 steps ahead: -2.6321373172821794e-05\n",
      "41 steps ahead: 0.00017940674640337662\n",
      "42 steps ahead: -7.55024934413484e-05\n",
      "43 steps ahead: -0.0004456461588417948\n",
      "44 steps ahead: -0.0007776908509660174\n",
      "45 steps ahead: 5.7537948962127494e-05\n",
      "46 steps ahead: -0.00028419280710245687\n",
      "47 steps ahead: 0.00025133644155739265\n",
      "48 steps ahead: 0.0002713514510672699\n",
      "49 steps ahead: -0.0002824988640652837\n",
      "50 steps ahead: 8.80401355609628e-05\n",
      "51 steps ahead: -9.869888315860997e-05\n",
      "52 steps ahead: -0.0001780441182952508\n",
      "53 steps ahead: -0.000411685630024472\n",
      "54 steps ahead: -1.533291712618734e-05\n",
      "55 steps ahead: -0.00010544093621711603\n",
      "56 steps ahead: 5.243172535696061e-06\n",
      "57 steps ahead: -0.00034850505111738705\n",
      "58 steps ahead: -2.4425528490912285e-06\n",
      "59 steps ahead: -0.0002908151726348507\n",
      "60 steps ahead: -0.00021872033757008857\n",
      "61 steps ahead: -3.01119326997501e-05\n",
      "62 steps ahead: -9.667163189686079e-05\n",
      "63 steps ahead: 5.768206945366838e-05\n",
      "64 steps ahead: -0.00016469784055384018\n",
      "65 steps ahead: -0.00014152415995738465\n",
      "66 steps ahead: 8.782234154836033e-05\n",
      "67 steps ahead: 4.670725440214518e-05\n",
      "68 steps ahead: -5.444756999439804e-05\n",
      "69 steps ahead: -8.70619056150268e-05\n",
      "70 steps ahead: -8.379676594749697e-05\n",
      "71 steps ahead: -0.0003764928288569891\n",
      "72 steps ahead: -0.0002772630331862036\n",
      "73 steps ahead: -2.505102707495155e-07\n",
      "74 steps ahead: -4.461962039115086e-05\n",
      "75 steps ahead: -0.00013103861000418604\n",
      "76 steps ahead: -0.0003006231781828017\n",
      "77 steps ahead: -0.00021772307362977195\n",
      "78 steps ahead: -0.000508867210717856\n",
      "79 steps ahead: -0.0003430083714510701\n",
      "80 steps ahead: -0.00017816269195392032\n",
      "81 steps ahead: -0.000144298551358224\n",
      "82 steps ahead: 0.0001025349960902533\n",
      "83 steps ahead: -0.0004170491216815009\n",
      "84 steps ahead: -0.00044082013836632683\n",
      "85 steps ahead: -8.841680462601254e-05\n",
      "86 steps ahead: 1.3032705133642075e-05\n",
      "87 steps ahead: -0.00019892471755778018\n",
      "88 steps ahead: -0.00035660702723472326\n",
      "89 steps ahead: -2.4071357220822875e-06\n",
      "90 steps ahead: -0.00032202169496620137\n",
      "91 steps ahead: 1.4444910292232827e-05\n",
      "92 steps ahead: 0.00018665395401806784\n",
      "93 steps ahead: -2.3246177759173037e-05\n",
      "94 steps ahead: -5.0109507635953676e-05\n",
      "95 steps ahead: 2.4123729327696353e-05\n",
      "96 steps ahead: 7.620667728336361e-05\n",
      "97 steps ahead: -9.022465921049161e-05\n",
      "98 steps ahead: -0.0001715127538015615\n",
      "99 steps ahead: 2.020362042887136e-06\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(paths.PATH)\n",
    "model1.eval()\n",
    "\n",
    "start = 10\n",
    "k = 10\n",
    "end= (start + k) if k != None else None\n",
    "\n",
    "model1.to('cpu')\n",
    "\n",
    "t_pred, t_real = metrics.r2_eval(model1, train_loader, k=end)\n",
    "v_pred, v_real = metrics.r2_eval(model1, val_loader, k=end)\n",
    "\n",
    "# f_pred, f_real = r2_eval(model1, full_loader,filt=ffull_loader, k=end)\n",
    "# print(f_real)\n",
    "# t_pred, t_real = r2_eval(model1, train_loader, filt=None ,k=end)\n",
    "# v_pred, v_real = r2_eval(model1, val_loader, filt=None, k=end)\n",
    "# f_pred, f_real = r2_eval(model1, full_loader,filt=None, k=end)\n",
    "# n_pred, n_real = r2_eval(model1, noise_loader, end)\n",
    "# s_pred, s_real = r2_eval(model1, sine_loader, end)\n",
    "\n",
    "# b_pred, b_real = r2_eval(model1, burst_loader, filt=fburst_loader,k=end)\n",
    "\n",
    "# for i in range(len(s_pred)):\n",
    "#     print(\"output: {} label: {}\".format(s_pred[i], s_real[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 13259.136719\n",
      "Val MSE: 13444.141602\n",
      "(10240, 100)\n",
      "(10240, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train MSE: {:f}\".format(mean_squared_error(t_real, t_pred)))\n",
    "print(\"Val MSE: {:f}\".format(mean_squared_error(v_real, v_pred)))\n",
    "# print(\"Full MSE: {:f}\".format(mean_squared_error(f_real, f_pred)))\n",
    "# print(\"Burst MSE: {:f}\".format(mean_squared_error(b_real, b_pred)))\n",
    "print(t_real.shape)\n",
    "print(t_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, vp, tr, vr = t_pred, v_pred, t_real, v_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240,)\n",
      "(10240,)\n"
     ]
    }
   ],
   "source": [
    "# print(next(iter(train_loader))[0][:,2,0])\n",
    "# print(next(iter(burst_loader))[0][:,2,0])\n",
    "t_pred = tp[:,0]\n",
    "v_pred = vp[:,0]\n",
    "t_real = tr[:,0]\n",
    "v_real = vr[:,0]\n",
    "print(t_pred.shape)\n",
    "print(t_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37589ba2036b4c6a8cf59597e21c4d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, -5.47777777777778, 'Epoch')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(nrows=1, ncols=2)\n",
    "fig1.tight_layout()\n",
    "ax1[0].plot(range(params.EPOCHS), pnfr_training_loss)\n",
    "ax1[0].set_title('Training Loss')\n",
    "ax1[0].set_ylabel('Loss')\n",
    "ax1[0].set_xlabel('Epoch')\n",
    "\n",
    "ax1[1].plot(range(params.EPOCHS), pnfr_validation_loss)\n",
    "ax1[1].set_title('Validation Loss')\n",
    "ax1[1].set_ylabel('Loss')\n",
    "ax1[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb7b34297d74e169c959a70d7aaefc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) (20,)\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].plot(np.arange(0,end), v_real[0:end], color='blue',label='Labels')\n",
    "# ax[2,0].plot(np.arange(start-10,end), v_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "print(np.arange(0,end).shape, v_pred[:end].shape)\n",
    "ax[0].scatter(np.arange(0,end), v_pred[:end], color='slateblue',label='Predicted t+1')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[0].plot(np.arange(start-10,end), v_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "\n",
    "ax[0].set_title('Validation LFPNet')\n",
    "ax[0].set_ylabel('LFP')\n",
    "ax[0].set_xlabel('Time')\n",
    "# ax[2,0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(0,end), t_real[:end], color='blue',label='Labels')\n",
    "# a[2,1].plot(np.arange(start-10,end), t_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "ax[1].scatter(np.arange(0,end), t_pred[:end], color='slateblue',label='Training t+1')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[1].plot(np.arange(start-10,end), t_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "ax[1].set_title('Training LFPNet')\n",
    "ax[1].set_ylabel('LFP')\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[0].legend(loc=2, prop={'size': 10})\n",
    "\n",
    "# import plotly.tools as tls\n",
    "# plotly_fig = tls.mpl_to_plotly(fig)\n",
    "# plotly_fig.write_html(\"testfile.html\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af1ba5d704d4f20aadaddef3d7ae854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73420cd843a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training t+10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Full LFP vs Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_real' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.plot(np.arange(start,end), f_real[start:end], color='blue',label='Labels')\n",
    "ax.scatter(np.arange(start,end), f_pred[start:end], color='red',label='Training t+10')\n",
    "ax.set_title('Full LFP vs Time')\n",
    "ax.set_ylabel('Signal')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# ax[1].plot(np.arange(start,end), n_real[start:end], color='blue',label='Labels')\n",
    "# ax[1].scatter(np.arange(start,end), n_pred[start:end], color='red',label='Training t+10')\n",
    "# ax[1].set_title('Noise')\n",
    "# ax[1].set_ylabel('Signal')\n",
    "# ax[1].set_xlabel('Time')z\n",
    "\n",
    "# ax.plot(np.arange(start,end), s_real[start:end], color='blue',label='Labels')\n",
    "# ax.scatter(np.arange(start,end), s_pred[start:end], color='red',label='Training t+10')\n",
    "# ax.set_title('Sine')\n",
    "# ax.set_ylabel('LFP')\n",
    "# ax.set_xlabel('Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "\n",
    "STEPS = 100\n",
    "# model_initial = params.MODEL(params.INPUT_SIZE,params.HIDDEN_SIZE,params.OUTPUT_SIZE)\n",
    "model_final = copy.deepcopy(model1)\n",
    "\n",
    "\n",
    "# data that the evaluator will use when evaluating loss\n",
    "x, y = iter(noise_loader).__next__()\n",
    "metric = loss_landscapes.metrics.Loss(nn.MSELoss(), x, y)\n",
    "\n",
    "\n",
    "loss_data_fin = loss_landscapes.random_plane(model_final, metric, 10000, STEPS, normalization='model', deepcopy_model=True)\n",
    "# plt.contour(loss_data_fin, levels=50)\n",
    "# plt.title('Loss Contours around Trained Model')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "X = np.array([[j for j in range(STEPS)] for i in range(STEPS)])\n",
    "Y = np.array([[i for _ in range(STEPS)] for i in range(STEPS)])\n",
    "ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface Plot of Loss Landscape')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict()['conv_block.0.weight'])\n",
    "print(model1.state_dict()['conv_block.0.bias'])\n",
    "print(model1.state_dict()['conv_block.2.weight'])\n",
    "print(model1.state_dict()['conv_block.2.bias'])\n",
    "print(model1.state_dict()['ck1s1.weight'])\n",
    "print(model1.state_dict()['ck1s1.bias'])\n",
    "print(model1.state_dict()['fc1.weight'])\n",
    "print(model1.state_dict()['fc1.bias'])\n",
    "print(model1.state_dict()['fc2.weight'])\n",
    "print(model1.state_dict()['fc2.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
