{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "COLAB_PRE = 'Neuron_Burst_Analysis/'\n",
    "if RunningInCOLAB:\n",
    "    !git clone https://github.com/MJC598/Neuron_Burst_Analysis.git\n",
    "    paths.LOSS_FILE = COLAB_PRE + paths.LOSS_FILE\n",
    "    paths.PATH = COLAB_PRE + paths.PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import random\n",
    "import time\n",
    "import pandas as pds\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import copy\n",
    "\n",
    "from utils import preprocess, metrics\n",
    "from config import params, paths\n",
    "from models import LFPNet\n",
    "\n",
    "s = 67\n",
    "\n",
    "rs = RandomState(MT19937(SeedSequence(s)))\n",
    "rng = default_rng(seed=s)\n",
    "torch.manual_seed(s)\n",
    "\n",
    "plt.rcParams.update({'font.size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,device):\n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "    \n",
    "#     feedback_arr = torch.zeros(params.BATCH_SIZE, 90)\n",
    "    \n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    decay_rate = .99995 #0.98 #decay the lr each step to 98% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                if params.RECURRENT_NET:\n",
    "                    x = torch.transpose(x, 2, 1)\n",
    "                x = x.to(device)\n",
    "                output = model(x)\n",
    "                y = y.to(device)\n",
    "#                 if i%100000 == 0 and epoch%5 == 0:\n",
    "#                     print(output)\n",
    "#                     print(y)\n",
    "                loss = loss_func(torch.squeeze(output), torch.squeeze(y)) \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "#                     if i%100000 == 0 and epoch%5 == 0:\n",
    "#                         print(model.cn1.weight.grad)\n",
    "#                         print(model.cn2.weight.grad)\n",
    "#                         print(model.fc1.weight.grad)\n",
    "#                         print(model.fc2.weight.grad)\n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            print('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if train_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = train_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pds.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    \n",
    "    loss_df.to_csv(paths.LOSS_FILE, index=None)\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tr, f_va, filtered = preprocess.get_inVivo_LFP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn datasets into iterable dataloaders\n",
    "train_loader = DataLoader(dataset=f_tr,batch_size=params.BATCH_SIZE, shuffle=True)\n",
    "# tfilt_loader = DataLoader(dataset=t_filt,params.BATCH_SIZE=params.BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=f_va,batch_size=params.BATCH_SIZE)\n",
    "filt_loader = DataLoader(dataset=filtered, batch_size=params.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d976961b094699b60a64662eacae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] train loss: 2409195.541016 val loss: 469812.209961\n",
      "[6,    30] train loss: 2071278.071289 val loss: 414640.997070\n",
      "[11,    30] train loss: 2062351.522461 val loss: 415685.850586\n",
      "[16,    30] train loss: 2054151.336914 val loss: 413472.032227\n",
      "[21,    30] train loss: 2052158.391602 val loss: 413073.768555\n",
      "[26,    30] train loss: 2042396.523438 val loss: 414102.326172\n",
      "[31,    30] train loss: 2025385.005859 val loss: 409776.846680\n",
      "[36,    30] train loss: 2010501.010742 val loss: 407965.871094\n",
      "[41,    30] train loss: 1994369.701172 val loss: 407093.268555\n",
      "[46,    30] train loss: 1986315.608398 val loss: 406459.955078\n",
      "[51,    30] train loss: 1981250.332031 val loss: 407526.114258\n",
      "[56,    30] train loss: 1978135.411133 val loss: 407875.371094\n",
      "[61,    30] train loss: 1977082.537109 val loss: 405636.801758\n",
      "[66,    30] train loss: 1975985.982422 val loss: 407028.873047\n",
      "[71,    30] train loss: 1971209.405273 val loss: 406159.418945\n",
      "[76,    30] train loss: 1968563.804688 val loss: 404518.509766\n",
      "[81,    30] train loss: 1967339.746094 val loss: 405352.875000\n",
      "[86,    30] train loss: 1964760.737305 val loss: 403875.155273\n",
      "[91,    30] train loss: 1964730.604492 val loss: 403829.209961\n",
      "[96,    30] train loss: 1963997.690430 val loss: 403484.764648\n",
      "[101,    30] train loss: 1962376.929688 val loss: 402972.037109\n",
      "[106,    30] train loss: 1961411.481445 val loss: 404552.058594\n",
      "[111,    30] train loss: 1959166.524414 val loss: 403274.591797\n",
      "[116,    30] train loss: 1958268.696289 val loss: 403123.671875\n",
      "[121,    30] train loss: 1958524.967773 val loss: 402970.751953\n",
      "[126,    30] train loss: 1954986.766602 val loss: 402806.603516\n",
      "[131,    30] train loss: 1955883.728516 val loss: 403634.798828\n",
      "[136,    30] train loss: 1954564.877930 val loss: 403747.844727\n",
      "[141,    30] train loss: 1953009.446289 val loss: 402986.503906\n",
      "[146,    30] train loss: 1952995.148438 val loss: 404413.358398\n",
      "[151,    30] train loss: 1950647.473633 val loss: 402145.264648\n",
      "[156,    30] train loss: 1950040.211914 val loss: 403528.107422\n",
      "[161,    30] train loss: 1948541.475586 val loss: 401673.244141\n",
      "[166,    30] train loss: 1948924.403320 val loss: 401931.320312\n",
      "[171,    30] train loss: 1948733.127930 val loss: 402035.576172\n",
      "[176,    30] train loss: 1948543.687500 val loss: 403200.000977\n",
      "[181,    30] train loss: 1947896.075195 val loss: 402275.789062\n",
      "[186,    30] train loss: 1946992.570312 val loss: 403137.434570\n",
      "[191,    30] train loss: 1946496.999023 val loss: 403580.256836\n",
      "[196,    30] train loss: 1942538.898438 val loss: 402651.009766\n",
      "[201,    30] train loss: 1943117.355469 val loss: 403578.125000\n",
      "[206,    30] train loss: 1944055.565430 val loss: 402248.163086\n",
      "[211,    30] train loss: 1941622.850586 val loss: 402436.464844\n",
      "[216,    30] train loss: 1941164.909180 val loss: 403283.943359\n",
      "[221,    30] train loss: 1940401.009766 val loss: 402769.704102\n",
      "[226,    30] train loss: 1939784.978516 val loss: 404029.212891\n",
      "[231,    30] train loss: 1940041.886719 val loss: 403370.533203\n",
      "[236,    30] train loss: 1939675.215820 val loss: 404355.358398\n",
      "[241,    30] train loss: 1939157.142578 val loss: 401963.711914\n",
      "[246,    30] train loss: 1937696.096680 val loss: 403372.447266\n"
     ]
    }
   ],
   "source": [
    "model1 = params.MODEL(\n",
    "    in_size=params.INPUT_SIZE,\n",
    "    h_size=params.HIDDEN_SIZE,\n",
    "    out_size=params.OUTPUT_SIZE,\n",
    "    num_layers=params.NUM_LAYERS,\n",
    "    dropout=params.DROPOUT\n",
    ")\n",
    "model_initial = copy.deepcopy(model1)\n",
    "# model1 = torch.load(paths.PATH)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "\n",
    "pnfr_training_loss, pnfr_validation_loss = train_model(model1,paths.PATH,train_loader,\n",
    "                                                       val_loader,params.EPOCHS,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps ahead: -21.290414760241546\n",
      "1 steps ahead: 0.07825226448324585\n",
      "2 steps ahead: 0.9414190355312032\n",
      "3 steps ahead: 0.9293185388007486\n",
      "4 steps ahead: 0.9131125212568969\n",
      "5 steps ahead: 0.8963735998559247\n",
      "6 steps ahead: 0.8860337736795757\n",
      "7 steps ahead: 0.8617083777467502\n",
      "8 steps ahead: 0.7894534077776698\n",
      "9 steps ahead: 0.3773210459778099\n",
      "10 steps ahead: 0.5941471853096889\n",
      "11 steps ahead: 0.8785433965087686\n",
      "12 steps ahead: 0.9141018698622433\n",
      "13 steps ahead: 0.899785605855702\n",
      "14 steps ahead: 0.8792602212424274\n",
      "15 steps ahead: 0.8368749010920318\n",
      "16 steps ahead: 0.7444446749185414\n",
      "17 steps ahead: 0.4793595946638094\n",
      "18 steps ahead: 0.0672573075934324\n",
      "19 steps ahead: 0.5972312403607167\n",
      "20 steps ahead: 0.8045670634943436\n",
      "21 steps ahead: 0.8478833324926603\n",
      "22 steps ahead: 0.8443646165392438\n",
      "23 steps ahead: 0.8143628443458854\n",
      "24 steps ahead: 0.7515258109343257\n",
      "25 steps ahead: 0.6189876583141213\n",
      "26 steps ahead: 0.342292205848441\n",
      "27 steps ahead: 0.07769148730213082\n",
      "28 steps ahead: 0.3713162337502236\n",
      "29 steps ahead: 0.6325868942725075\n",
      "30 steps ahead: 0.7204624841587417\n",
      "31 steps ahead: 0.7364529193815927\n",
      "32 steps ahead: 0.7119546469112459\n",
      "33 steps ahead: 0.6504856729068899\n",
      "34 steps ahead: 0.5276748640052153\n",
      "35 steps ahead: 0.3203432834350072\n",
      "36 steps ahead: 0.10552630840559984\n",
      "37 steps ahead: 0.12397286072849678\n",
      "38 steps ahead: 0.3199739744034459\n",
      "39 steps ahead: 0.4576849140407139\n",
      "40 steps ahead: 0.5187586652202447\n",
      "41 steps ahead: 0.5220390000475328\n",
      "42 steps ahead: 0.48804123017256795\n",
      "43 steps ahead: 0.40510522365280976\n",
      "44 steps ahead: 0.28442357545947483\n",
      "45 steps ahead: 0.15021145385559675\n",
      "46 steps ahead: 0.06664427971306175\n",
      "47 steps ahead: 0.056445156444020816\n",
      "48 steps ahead: 0.1100706558064396\n",
      "49 steps ahead: 0.17531481978367747\n",
      "50 steps ahead: 0.21461616381610815\n",
      "51 steps ahead: 0.22481338107137183\n",
      "52 steps ahead: 0.20919123119835015\n",
      "53 steps ahead: 0.17263712614740856\n",
      "54 steps ahead: 0.12788160971897033\n",
      "55 steps ahead: 0.08802979299932512\n",
      "56 steps ahead: 0.06194078990832641\n",
      "57 steps ahead: 0.046698264073095075\n",
      "58 steps ahead: 0.04149413941242275\n",
      "59 steps ahead: 0.039223901347668844\n",
      "60 steps ahead: 0.035599134149239986\n",
      "61 steps ahead: 0.030451365818230824\n",
      "62 steps ahead: 0.03254514095011507\n",
      "63 steps ahead: 0.0428627107906544\n",
      "64 steps ahead: 0.05933617341622888\n",
      "65 steps ahead: 0.07887052392208027\n",
      "66 steps ahead: 0.09492778824642323\n",
      "67 steps ahead: 0.09690248679965818\n",
      "68 steps ahead: 0.08442913449015854\n",
      "69 steps ahead: 0.060304226167028574\n",
      "70 steps ahead: 0.032257048661580634\n",
      "71 steps ahead: 0.00925914340976186\n",
      "72 steps ahead: 0.0009132992854802291\n",
      "73 steps ahead: 0.00809886674573912\n",
      "74 steps ahead: 0.028642980869565737\n",
      "75 steps ahead: 0.04788762439044625\n",
      "76 steps ahead: 0.0639050961439771\n",
      "77 steps ahead: 0.06905616185467744\n",
      "78 steps ahead: 0.06653603134273511\n",
      "79 steps ahead: 0.05695077329197484\n",
      "80 steps ahead: 0.04179899266088416\n",
      "81 steps ahead: 0.025654177381184917\n",
      "82 steps ahead: 0.011605033158772593\n",
      "83 steps ahead: 0.005731477498522652\n",
      "84 steps ahead: 0.003708147679391427\n",
      "85 steps ahead: 0.009922079598841282\n",
      "86 steps ahead: 0.017890805363221185\n",
      "87 steps ahead: 0.025731367278359563\n",
      "88 steps ahead: 0.029013249813761632\n",
      "89 steps ahead: 0.03418221425580814\n",
      "90 steps ahead: 0.02944051051906149\n",
      "91 steps ahead: 0.024045445704612756\n",
      "92 steps ahead: 0.015789567330449783\n",
      "93 steps ahead: 0.008214453353923346\n",
      "94 steps ahead: 0.002586318664992926\n",
      "95 steps ahead: 0.0031069397120616626\n",
      "96 steps ahead: 0.003884885016486317\n",
      "97 steps ahead: 0.0043607051519135576\n",
      "98 steps ahead: 0.004457279655814883\n",
      "99 steps ahead: 0.004464449698956097\n",
      "0 steps ahead: -20.550612123012982\n",
      "1 steps ahead: 0.07742341808119635\n",
      "2 steps ahead: 0.9409644758780003\n",
      "3 steps ahead: 0.9288827083345788\n",
      "4 steps ahead: 0.9117280851036942\n",
      "5 steps ahead: 0.8933620124293007\n",
      "6 steps ahead: 0.8817643816475949\n",
      "7 steps ahead: 0.8554165826928405\n",
      "8 steps ahead: 0.7801407064995483\n",
      "9 steps ahead: 0.3574355174313072\n",
      "10 steps ahead: 0.5801184638096077\n",
      "11 steps ahead: 0.8793422923794025\n",
      "12 steps ahead: 0.913599325096278\n",
      "13 steps ahead: 0.8977963305677555\n",
      "14 steps ahead: 0.8753992882653587\n",
      "15 steps ahead: 0.82989127686623\n",
      "16 steps ahead: 0.7333265394875711\n",
      "17 steps ahead: 0.46475311713634615\n",
      "18 steps ahead: 0.033148480707775985\n",
      "19 steps ahead: 0.5867598416370388\n",
      "20 steps ahead: 0.8038442982111799\n",
      "21 steps ahead: 0.8450265003875108\n",
      "22 steps ahead: 0.8380560185244558\n",
      "23 steps ahead: 0.8042361116438037\n",
      "24 steps ahead: 0.7362849899881193\n",
      "25 steps ahead: 0.5958999652832407\n",
      "26 steps ahead: 0.3109585429706292\n",
      "27 steps ahead: 0.042160318500813254\n",
      "28 steps ahead: 0.3538786303824688\n",
      "29 steps ahead: 0.6262675754425286\n",
      "30 steps ahead: 0.7148686579436934\n",
      "31 steps ahead: 0.7256608445270749\n",
      "32 steps ahead: 0.693514266152185\n",
      "33 steps ahead: 0.6231981096637081\n",
      "34 steps ahead: 0.4905723179455539\n",
      "35 steps ahead: 0.2756986544073645\n",
      "36 steps ahead: 0.06030945481995309\n",
      "37 steps ahead: 0.09511521249406818\n",
      "38 steps ahead: 0.2983421143223357\n",
      "39 steps ahead: 0.43670919069955705\n",
      "40 steps ahead: 0.4921288968163847\n",
      "41 steps ahead: 0.48546456874557187\n",
      "42 steps ahead: 0.44060434727217324\n",
      "43 steps ahead: 0.3522946157568173\n",
      "44 steps ahead: 0.23041383881899247\n",
      "45 steps ahead: 0.10745528823833672\n",
      "46 steps ahead: 0.03238422093082238\n",
      "47 steps ahead: 0.02792336656024974\n",
      "48 steps ahead: 0.0757519833074225\n",
      "49 steps ahead: 0.12756254255644728\n",
      "50 steps ahead: 0.15590418881255008\n",
      "51 steps ahead: 0.16227600533501318\n",
      "52 steps ahead: 0.15089840402614318\n",
      "53 steps ahead: 0.12794476533215882\n",
      "54 steps ahead: 0.09940291803623569\n",
      "55 steps ahead: 0.0726860558598944\n",
      "56 steps ahead: 0.050652071337801585\n",
      "57 steps ahead: 0.031837184105019234\n",
      "58 steps ahead: 0.01612238066963756\n",
      "59 steps ahead: 0.008247289412454006\n",
      "60 steps ahead: 0.004200850902458697\n",
      "61 steps ahead: 0.0036061876392843306\n",
      "62 steps ahead: 0.015570932311072094\n",
      "63 steps ahead: 0.0336270204432324\n",
      "64 steps ahead: 0.056768054507541965\n",
      "65 steps ahead: 0.07804583194576264\n",
      "66 steps ahead: 0.0910444692656599\n",
      "67 steps ahead: 0.09051577230052776\n",
      "68 steps ahead: 0.07715009422203256\n",
      "69 steps ahead: 0.05263086663461425\n",
      "70 steps ahead: 0.025476642038317343\n",
      "71 steps ahead: 0.005387236646313243\n",
      "72 steps ahead: 0.00039459908495842466\n",
      "73 steps ahead: 0.0063430859378476034\n",
      "74 steps ahead: 0.025713937140170673\n",
      "75 steps ahead: 0.0435111035285809\n",
      "76 steps ahead: 0.05684688955461348\n",
      "77 steps ahead: 0.05898647427801518\n",
      "78 steps ahead: 0.05393975123302741\n",
      "79 steps ahead: 0.04296022048774262\n",
      "80 steps ahead: 0.028759761366292658\n",
      "81 steps ahead: 0.015847269039589218\n",
      "82 steps ahead: 0.003732198046832891\n",
      "83 steps ahead: 0.0004168609442429805\n",
      "84 steps ahead: 0.0002870369825207497\n",
      "85 steps ahead: 0.003607698596760467\n",
      "86 steps ahead: 0.007670465230820933\n",
      "87 steps ahead: 0.01124333763952301\n",
      "88 steps ahead: 0.01300348228583803\n",
      "89 steps ahead: 0.013755184075868065\n",
      "90 steps ahead: 0.013797442377756042\n",
      "91 steps ahead: 0.011788793145187881\n",
      "92 steps ahead: 0.009754956653452496\n",
      "93 steps ahead: 0.006142098010555319\n",
      "94 steps ahead: 0.0018694909451187902\n",
      "95 steps ahead: 0.00011199738054723518\n",
      "96 steps ahead: -0.00025910397791428785\n",
      "97 steps ahead: -0.0006263212010613195\n",
      "98 steps ahead: -0.0001915915419079539\n",
      "99 steps ahead: 0.0005810686870074688\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(paths.PATH)\n",
    "model1.eval()\n",
    "\n",
    "start = 10\n",
    "k = 100\n",
    "end= (start + k) if k != None else None\n",
    "\n",
    "model1.to('cpu')\n",
    "\n",
    "t_pred, t_real = metrics.r2_eval(model1, train_loader, k=end)\n",
    "v_pred, v_real = metrics.r2_eval(model1, val_loader, filt=filt_loader, k=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 12993.468750\n",
      "Val MSE: 13388.003906\n",
      "(56320, 100)\n",
      "(56320, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train MSE: {:f}\".format(mean_squared_error(t_real, t_pred)))\n",
    "print(\"Val MSE: {:f}\".format(mean_squared_error(v_real, v_pred)))\n",
    "# print(\"Full MSE: {:f}\".format(mean_squared_error(f_real, f_pred)))\n",
    "# print(\"Burst MSE: {:f}\".format(mean_squared_error(b_real, b_pred)))\n",
    "print(t_real.shape)\n",
    "print(t_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, vp, tr, vr = t_pred, v_pred, t_real, v_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56320,)\n",
      "(56320,)\n"
     ]
    }
   ],
   "source": [
    "# print(next(iter(train_loader))[0][:,2,0])\n",
    "# print(next(iter(burst_loader))[0][:,2,0])\n",
    "t_pred = tp[:,29]\n",
    "v_pred = vp[:,29]\n",
    "t_real = tr[:,29]\n",
    "v_real = vr[:,29]\n",
    "print(t_pred.shape)\n",
    "print(t_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c8f6a8cf6f448da8e1c6a96a9db98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, -5.47777777777778, 'Epoch')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(nrows=1, ncols=2)\n",
    "fig1.tight_layout()\n",
    "ax1[0].plot(range(params.EPOCHS), pnfr_training_loss)\n",
    "ax1[0].set_title('Training Loss')\n",
    "ax1[0].set_ylabel('Loss')\n",
    "ax1[0].set_xlabel('Epoch')\n",
    "\n",
    "ax1[1].plot(range(params.EPOCHS), pnfr_validation_loss)\n",
    "ax1[1].set_title('Validation Loss')\n",
    "ax1[1].set_ylabel('Loss')\n",
    "ax1[1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d137ca3a74ff497c86e1057d99b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110,) (110,)\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].plot(np.arange(0,end), v_real[0:end], color='blue',label='Labels')\n",
    "# ax[2,0].plot(np.arange(start-10,end), v_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "print(np.arange(0,end).shape, v_pred[:end].shape)\n",
    "ax[0].scatter(np.arange(0,end), v_pred[:end], color='slateblue',label='Predicted t+30')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[0].scatter(np.arange(start-params.OUTPUT_SIZE,end), v_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[0].scatter(np.arange(start-10,end), v_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[0].plot(np.arange(start-10,end), v_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "\n",
    "ax[0].set_title('Validation LFPNet')\n",
    "ax[0].set_ylabel('LFP')\n",
    "ax[0].set_xlabel('Time')\n",
    "# ax[2,0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(0,end), t_real[:end], color='blue',label='Labels')\n",
    "# a[2,1].plot(np.arange(start-10,end), t_output_list[start-10:end,2], color='red',label='Internal Loop')\n",
    "ax[1].scatter(np.arange(0,end), t_pred[:end], color='slateblue',label='Training t+30')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-2:end+8,1], color='lightsteelblue',label='Training t+2')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-3:end+7,2], color='gray',label='Training t+3')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-4:end+6,3], color='sienna',label='Training t+4')\n",
    "# ax[1].scatter(np.arange(start-params.OUTPUT_SIZE,end), t_pred[start-5:end+5,4], color='magenta',label='Training t+5')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-6:end+4,5], color='aquamarine',label='Training t+6')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-7:end+3,6], color='darkorange',label='Training t+7')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-8:end+2,7], color='brown',label='Training t+8')\n",
    "# ax[1].scatter(np.arange(start-10,end), t_pred[start-9:end+1,8], color='purple',label='Training t+9')\n",
    "# ax[1].plot(np.arange(start-10,end), t_pred[start-10:end], color='green',label='Training t+10')\n",
    "\n",
    "ax[1].set_title('Training LFPNet')\n",
    "ax[1].set_ylabel('LFP')\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[0].legend(loc=2, prop={'size': 10})\n",
    "\n",
    "# import plotly.tools as tls\n",
    "# plotly_fig = tls.mpl_to_plotly(fig)\n",
    "# plotly_fig.write_html(\"testfile.html\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cad91cedf34da2981271b0ab60b747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73420cd843a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training t+10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Full LFP vs Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_real' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax.plot(np.arange(start,end), f_real[start:end], color='blue',label='Labels')\n",
    "ax.scatter(np.arange(start,end), f_pred[start:end], color='red',label='Training t+10')\n",
    "ax.set_title('Full LFP vs Time')\n",
    "ax.set_ylabel('Signal')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# ax[1].plot(np.arange(start,end), n_real[start:end], color='blue',label='Labels')\n",
    "# ax[1].scatter(np.arange(start,end), n_pred[start:end], color='red',label='Training t+10')\n",
    "# ax[1].set_title('Noise')\n",
    "# ax[1].set_ylabel('Signal')\n",
    "# ax[1].set_xlabel('Time')z\n",
    "\n",
    "# ax.plot(np.arange(start,end), s_real[start:end], color='blue',label='Labels')\n",
    "# ax.scatter(np.arange(start,end), s_pred[start:end], color='red',label='Training t+10')\n",
    "# ax.set_title('Sine')\n",
    "# ax.set_ylabel('LFP')\n",
    "# ax.set_xlabel('Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import loss_landscapes.metrics\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "\n",
    "STEPS = 100\n",
    "# model_initial = params.MODEL(params.INPUT_SIZE,params.HIDDEN_SIZE,params.OUTPUT_SIZE)\n",
    "model_final = copy.deepcopy(model1)\n",
    "\n",
    "\n",
    "# data that the evaluator will use when evaluating loss\n",
    "x, y = iter(noise_loader).__next__()\n",
    "metric = loss_landscapes.metrics.Loss(nn.MSELoss(), x, y)\n",
    "\n",
    "\n",
    "loss_data_fin = loss_landscapes.random_plane(model_final, metric, 10000, STEPS, normalization='model', deepcopy_model=True)\n",
    "# plt.contour(loss_data_fin, levels=50)\n",
    "# plt.title('Loss Contours around Trained Model')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "X = np.array([[j for j in range(STEPS)] for i in range(STEPS)])\n",
    "Y = np.array([[i for _ in range(STEPS)] for i in range(STEPS)])\n",
    "ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface Plot of Loss Landscape')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict()['conv_block.0.weight'])\n",
    "print(model1.state_dict()['conv_block.0.bias'])\n",
    "print(model1.state_dict()['conv_block.2.weight'])\n",
    "print(model1.state_dict()['conv_block.2.bias'])\n",
    "print(model1.state_dict()['ck1s1.weight'])\n",
    "print(model1.state_dict()['ck1s1.bias'])\n",
    "print(model1.state_dict()['fc1.weight'])\n",
    "print(model1.state_dict()['fc1.bias'])\n",
    "print(model1.state_dict()['fc2.weight'])\n",
    "print(model1.state_dict()['fc2.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
